---
title: "Dataiku Interview Project"
author:
  - name: "Joffrey Joumaa"
date: "December 5, 2017"
format:
  html:
    toc: true
    toc-location: left
    number-sections: true
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-link: true
    df-print: paged
    fig-align: "center"
execute: 
  echo: true
  cache: true
  warning: false
knitr:
  opts_chunk:
    message: false
    tidy: styler
---

```{r setup}
#| include: false
#| eval: false

knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  comment = NA, 
  cache = TRUE,
  fig.align = "center"
)
```

```{=html}
<style>
body {
text-align: justify}
</style>
```

Please find below my report about the dataset provided by Dataiku. This analysis has been realized using R (version 3.3.3) with all packages updated on `r format(Sys.Date(), "%d/%m/%Y")`, under a UNIX/LINUX environment (Debian 9.2). The provided script (`code_jjoumaa.R`) runs in about an hour with an Intel i7 6600U based computer equipped with 32 Gb RAM (DDR4).

# Prepare Problem

## Load libraries

Most of the data manipulation was done using `data.table` package due to its ability to handle large datasets. Data visualization was mostly done using `ggplot2` package. For the modelling part, I've used `caret` package, which provides a lot of useful tools for data science.

```{r load_libraries}
# data visualization
library(ggplot2)
library(corrplot)
library(gridExtra)
# data manipulation
library(data.table)
library(stringr)
library(magrittr)
# data modeling
library(caret)
library(caretEnsemble)
library(RANN)
# markdown table
library(knitr)
library(DT)
library(pander)
# (optional) multithreading
library(doMC)
registerDoMC(cores = 4)
```

## Load dataset

As we'll see in the next steps, both datasets (learning and validation one) present a lot of missing values in the form of `?` in raw files, converted then in `NA` values in R.

```{r load_dataset}
# learning dataset
dataset = fread("./dataiku_data/census_income_learn.csv", 
                na.strings = "?")
# validation dataset
validation = fread("./dataiku_data/census_income_test.csv", 
                   na.strings = "?")
```

Since neither dataset has column names, I've loaded the description file `census_income_metadata.txt`, and extracted rows mentioning column names.

```{r load_colnames}
# selection of the right rows containing colnames
datasetNames = fread("./dataiku_data/census_income_metadata.txt", 
                     nrows = (68-22), 
                     skip = 22, 
                     drop = "V1")
# extraction of word in capital
colInter = datasetNames[, unlist(str_extract_all(V2, '\\b[A-Z]+[A-Z0-9]\\b'))]
colInter
```

The problem was that 45 columns were mentioned in the description files (`colInter`), whereas both datasets only have 42 columns. To solve this, I've matched each column with their respective names based on the number of unique "value" by columns and the one exposed in the description file. Using this method, I've found that AGI, FEDTAX, PEARNVAL, PTOTVAL and TAXINC attributes could be removed from the initial column names, and that the attributes YEARS and Y (the attributes to model, *i.e* the income level) should be added.

```{r tbl-right_colnames}
#| tbl-cap: "Preview of the dataset"

# number of instances for each attributes
dataset[, sapply(.SD, function(x){length(unique(x))})]

# remove unexpected column names
colInter = colInter[-c(which(colInter == "AGI"))]
colInter = colInter[-c(which(colInter == "FEDTAX"))]
colInter = colInter[-c(which(colInter == "PEARNVAL"))]
colInter = colInter[-c(which(colInter == "PTOTVAL"))]
colInter = colInter[-c(which(colInter == "TAXINC"))]

# add two more column names
colInter = c(colInter, c("YEAR", "Y"))
colnames(dataset) = colInter[1:42]
colnames(validation) = colInter[1:42]

# dataset preview
datatable(head(validation, 5),  
          extensions = 'FixedColumns', 
          options = list(
            dom = "t", 
            scrollX = TRUE, 
            fixedColumns = FALSE))
```

# Summarize Data

## Descriptive statistics

It is difficult to present all attributes, but a quick summary provides a good overview of the structure.

```{r tbl-descriptive_stat}
#| tbl-cap: "Structure of the dataset"

# index of column with characters
ncolCha = which(sapply(dataset, class) == "character")

# conversion of characters in factors
dataset[, (ncolCha):=lapply(.SD, as.factor), .SDcols = ncolCha]

# structure of the dataset
kable(data.frame(variable = names(dataset), 
                 classe = sapply(dataset, class), 
                 first_values = sapply(dataset, function(x) {
                   paste0(head(x), collapse = ", ")}), 
                 row.names = NULL))
```

```{r tbl-summary-data}
#| tbl-cap: "Summary of the dataset"

# summary of the dataset
pander(summary(dataset))
```


As mentioned above, a lot of missing values occurred in this dataset, especially for the attributes MIGMTR1, MIGMTR3, MIGMTR4 and MIGSUN.

## Data visualizations

Another way to have an idea of the occurrence of missing values is to used data visualization.

```{r fig-missingValue}
#| fig.cap="Representation of missing values within the learning dataset"

# build dataPlot
dataPlot = setDT(reshape2::melt(is.na(dataset)))

# subsample
dataPlot = dataPlot[,.SD[sample(nrow(dataset), 1000)], by=Var2] %>% 
  .[, `:=`(id_row, c(1:.N)), by = c("Var2")]

# missing values
ggplot(dataPlot, aes(x = Var2, y = id_row))+
  geom_tile(aes(fill = value))+
  labs(x = "Attributes", y = "Rows")+
  scale_fill_manual(values = c("white", "black"), 
                    labels = c("Real", "Missing")) +
  theme(legend.position = "top", 
        axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.key = element_rect(colour = "black"))
```

It also helps for the representation of numerical attributes.

```{r fig-densityHistogram}
#| fig.cap: "Histogram and density for each of the numerical attributes."

# numerical attributes index
ncolInt = which(sapply(dataset, class) == "integer")

# plot for each numerical attributes
ggplot(reshape2::melt(dataset[, .SD, .SDcols = c(ncolInt, ncol(dataset))], id.vars = "Y"), 
       aes(x = value, col = Y, fill = Y))+ 
  geom_histogram(aes(y = ..density..), 
                 bins = 10, 
                 fill = "white")+
  geom_density(alpha = .2)+
  theme(legend.position = "top")+
  facet_wrap(~variable, scales = "free")
```

Nothing particularly clear here, except that some attributes do not seem to contain a lot of information (AHRSPAY, CAPGAIN, CAPLOSS, DIVVAL, SEOTR, VETYN, WKSWORK or YEAR). We can also note the attribute "AAGE" has a Gaussian-like distribution a bit shifted, especially for `Y = "- 50000.`, which may require to be transformed. Let's have a look at another representation of this data.

```{r fig-boxplot}
#| fig.cap: "Boxplot for each of the numerical attributes."

ggplot(reshape2::melt(dataset[, .SD, .SDcols = c(ncolInt, ncol(dataset))], id.vars = "Y"), 
       aes(x = "", y = value, col = Y, fill = Y)) + 
  geom_boxplot(alpha = .2)+
  theme(legend.position = "top", 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        axis.ticks.x = element_blank())+
  facet_wrap(~variable, scales = "free")
```

```{r}
#| echo: true
#| eval: false

# nominal attributes names
ncolFac = names(which(sapply(dataset, class) == "factor"))

# plot for each nominal attributes
dataPlot=lapply(ncolFac[-c(length(ncolFac))], function(x){
  A = dataset[, levels(get(x)), by = Y]
  B = dataset[, table(get(x)), by = Y] %>% 
    .[, `:=`(V1 = as.integer(V1), 
             levels = as.factor(A$V1))]
  ggplot(B, aes(x = as.factor(levels), y = V1, fill = Y))+
    geom_bar(stat="identity", 
             position=position_dodge())+
    labs(y = "Count", x = "", title = x)+
    theme(legend.justification = c(0, 1), 
          legend.position = c(0, 1), 
          axis.text.x = element_text(angle = 45, hjust = 1))
})

# print of the two first plot
grid.arrange(grobs=list(dataPlot[[1]], dataPlot[[2]]), ncol=2)
```

```{r fig-nominalAttributes}
#| echo: false
#| fig.cap: "Barplot for each of the nominal attributes."

dataset[, (ncolCha):=lapply(.SD, as.factor), .SDcols=ncolCha]

# nominal attributes names
ncolFac = names(which(sapply(dataset, class) == "factor"))

# plot for each attributes
dataPlot=lapply(ncolFac[-c(length(ncolFac))], function(x){
  A = dataset[, levels(get(x)), by=Y]
  B = dataset[, table(get(x)), by=Y] %>% 
    .[, `:=`(V1 = as.integer(V1), 
             levels = as.factor(A$V1))]
  ggplot(B, aes(x = as.factor(levels), y = V1, fill=Y))+
    geom_bar(stat="identity", 
             position=position_dodge())+
    labs(y = "Count", x = "", title = x)+
    theme(legend.justification = c(0, 1), 
          legend.position = c(0, 1), 
          axis.text.x = element_text(angle = 45, hjust = 1))
})

# print of the two first plot
grid.arrange(grobs=list(dataPlot[[1]], dataPlot[[2]]), ncol=2)
```

```{r just_comment}
# another way to it, which appears not working using Rmarkdown document
# grid.arrange(grobs=dataPlot, ncol=2)
```

# Prepare Data
## Data Cleaning {#dataCleaning}

When looking at @fig-missingValue, it appears that a lot of missing values occurred within the learning dataset. Here, I've taken the liberty to "simply" removed these values. Another way to deal with missing values would have been to use an imputation method, such as the [K-nearest neighbors method](https://topepo.github.io/caret/pre-processing.html#imputation) [^1] provided by the package `caret` (code commented below). Because this method requires high computational costs, I've preferred to stick to the first method, even if, as we'll see in the next steps, it means the final model won't be able to predict `Y` in some cases.

```{r removal_NA}
# removal of missing values
datasetNa = na.omit(dataset)

## or we could have done something like this:
# preProcValues = preProcess(dataset, method = c("knnImpute"))
# dataset.imp = predict(preProcValues, dataset)
```

[^1]: Another method is implemented based on bagged trees but at higher computational costs.

## Feature Selection

As a first step to feature selection, I've decided to remove any attributes with a zero or near zero variance. For many models, this may cause the model to crash or the fit to be unstable.

```{r nzv}
# identification of near zero variance attributes
nzv = nearZeroVar(datasetNa, saveMetrics = TRUE)
datatable(nzv[nzv$nzv, ],
          caption = "Information relative to zero and near zero variance predictors.",
          option = list(dom = "t"))

# removal of these predictors
datasetNaNzv = datasetNa[, .SD, .SDcols = !nzv$nzv]
```

The second step was to remove any attributes with a correlation higher than 0.75. This is a basic method, which could be improved by combining correlogram with significance test.

```{r fig-correlation}
#| fig.cap: "Correlogram of dataset."

# matrix correlation
ncolInt = which(sapply(datasetNaNzv, class) == "integer")
descrCor = cor(datasetNaNzv[, .SD, .SDcols = ncolInt])

# which attributes will be removed based on matrix correlation
corrplot(descrCor, method = "pie")
highlyCorDescr = findCorrelation(descrCor, cutoff = .75)
names(ncolInt[highlyCorDescr])

# attribute "WKSWORK" removed
datasetNaNzv[, (names(ncolInt[highlyCorDescr])):= NULL]
```

Here, I've only removed the attribute "WKSWORK". At the end of the "Feature selection" step, the current dataset includes `r dim(datasetNaNzv)[2]` attributes instead of `r dim(dataset)[2]` in the original one.

## Data Transforms

### Yeo-Johnson transform

I've used the Yeo-Johnson transform, since the attributes "AAGE", *i.e* the age of the people in this dataset, has a Gaussian-like distribution with a skew (@fig-densityHistogram). To make it "more Gaussian", I've simply performed a `YeoJohnson` transform; I would have normally performed a `BoxCox` transform, but it does not support raw values that are equal to zero.

```{r yeojohnson}
# calculate the pre-process parameters from the dataset
preprocessParams = preProcess(datasetNaNzv[, "AAGE"], method=c("YeoJohnson"))

# transform the dataset using the parameters
datasetNaNzv$AAGE=predict(preprocessParams, datasetNaNzv[, "AAGE"])
```
### One-Hot encoding

As we'll see in [the next steps](#CompareAlgorithm), one-hot encoding was not really necessary for both algorithms tested hereafter. However, it may be a good way to improve computational costs for many algorithms that do not perform well, when dealing with nominal attributes.

```{r hot_encoding}
# one-hot encoding
dummies = dummyVars(Y~., data = datasetNaNzv)
matDummies = predict(dummies, newdata = datasetNaNzv)
```

As a consequence, some columns might appear to be a linear combination of others. To ensure the non-redundancy of information, I've removed these rows.

```{r linear_combination}
# remove linear combination
comboInfo = findLinearCombos(matDummies)
datasetNaNzvQr = as.data.table(matDummies[, -comboInfo$remove])
datasetNaNzvQr[, Y:=datasetNaNzv[, Y]]
```

## Split-out validation dataset

I know you provided a "test file", but the learning dataset is large enough to be split in a learning dataset and a validation dataset by itself. Moreover, my computer is not powerful enough to run models on the whole learning dataset, so I've split the learning dataset with 10\% of the data for the training process and the other 90\% for testing (with a powerful enough computers, I would split the dataset using 80\% of the data for the learning step and the other 20\% for the evaluation). In the [last step](#prediction), I'll evaluate the chosen model on the provided "test file".

```{r split_out}
# identification of indexes for splitting
set.seed(7)
trainIndex = createDataPartition(datasetNaNzvQr$Y,
                                 p = .1,
                                 list = FALSE,
                                 times = 1)

# split
datasetNaNzvQr[, Y:=as.factor(make.names(Y))]
dataTrain = datasetNaNzvQr[trainIndex, ]
dataTest = datasetNaNzvQr[-trainIndex, ]
```

# Evaluate Algorithms
## Test options and evaluation metric

Here I've chosen to cross-validate the model using 5 folds repeated twice. As explained before, with a powerful enough computer I would encourage increasing these values. Because the problem to solve is a classification problem, the metric used to select the optimal model is the "Accuracy", except for the ["Stacking"](#stacking) part, where the area under the ROC curve was used.

```{r options}
# cross-validation
set.seed(7)
trainControl = trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 2,
                            savePredictions = "final", # required for stacking
                            classProbs = TRUE)         # required for stacking

# metric evaluation
metric = "Accuracy"
```

## Spot Check Algorithms

Because the dataset is strongly imbalance...

```{r tbl-propImbalance}
#| tbl-cap: "Proportion of each levels of the explaining variable."

# proportion of each levels of Y
datatable(data.table(
  freq = table(datasetNaNzvQr$Y),
  percentage = round(prop.table(table(datasetNaNzvQr$Y))*100, 2)),
  option = list(dom = "t"))
```

... it can have significant negative impact on model fitting (by the way, it seems consistent that a large proportion of these people earn less than 50 000$ pear annum). Here I've explored two different ways to deal with it:

* *down-sampling*: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class.
* *up-sampling*: randomly sample (with replacement) the minority class to be the same size as the majority class.

In addition, I've tested two different algorithms, the **Logistic Regression**, and **k-Nearest Neighbors**.

```{r modelDownUp}
# down-sampling
trainControl$sampling = "down"

# logistic regression
set.seed(7)
fitLogDown = train(Y~., data = dataTrain,
                   method = "glm",
                   family = "binomial",
                   metric = metric,
                   trControl = trainControl)

# KNN
set.seed(7)
fitKnnDown = train(Y~., data = dataTrain,
                   method = "knn",
                   metric = metric,
                   trControl = trainControl)

# up-sampling
trainControl$sampling = "up"

# logistic regression
set.seed(7)
fitLogUp = train(Y~., data = dataTrain,
                 method = "glm",
                 family = "binomial",
                 metric = metric,
                 trControl = trainControl)

# KNN
set.seed(7)
fitKnnUp = train(Y~., data = dataTrain,
                 method = "knn",
                 metric = metric,
                 trControl = trainControl)
```

## Compare Algorithms {#CompareAlgorithm}

```{r compare1_models}
# resample
results = resamples(list(KNN_UP = fitKnnUp,
                         LOG_UP = fitLogUp,
                         KNN_DOWN = fitKnnDown,
                         LOG_DOWN = fitLogDown))

# summary
summary(results)
```

It appears the *up-sampling* method performs better than the *down-sampling*. In addition, the **k-Nearest Neighbors** algorithm provides better results than the **Logistic Regression** one. Let's now compare both algorithms using the *up-sampling* method with a dataset including nominal attributes (*i.e.* not using one-hot encoding).

```{r model_var}
# new data split based on the dataset without hot encoding
datasetNaNzv[, Y:=as.factor(make.names(Y))]
dataTrainVar = datasetNaNzv[trainIndex, ]
dataTestVar = datasetNaNzv[-trainIndex, ]

# logistic regression
set.seed(7)
fitLogUpVar = train(Y~., data = dataTrainVar,
                    method = "glm",
                    family = "binomial",
                    metric = metric,
                    trControl = trainControl)

# KNN
set.seed(7)
fitKnnUpVar = train(Y~., data = dataTrainVar,
                    method = "knn",
                    metric = metric,
                    trControl = trainControl)

# resample
results = resamples(list(KNN_UP = fitKnnUp,
                         LOG_UP = fitLogUp,
                         KNN_DOWN = fitKnnDown,
                         LOG_DOWN = fitLogDown,
                         KNN_UP_VAR = fitKnnUpVar,
                         LOG_UP_VAR = fitLogUpVar))

# summary
summary(results)
```

```{r fig-dotplot}
#| fig.cap: "Models comparison."

dotplot(results)
```

Using direct nominal attributes instead of one-hot encoding provides better results (*i.e.* higher accuracy) for the **Logistic Regression**, which is not the case for **k-Nearest Neighbors** algorithm, which provides the same accuracy.

# Improve Accuracy
## Algorithm Tuning

Depending on the algorithm to chose, we can improve the accuracy by choosing the appropriate set of parameters. Below, you'll find a recipe to find the optimal number of neighbors to set when using a **k-Nearest Neighbors** algorithm. Due to high computational cost, I did not run this code.

```{r model_tuning}
#| echo: true
#| eval: false

# search for the optimal number of neighbors K (did not run)
set.seed(7)
grid = expand.grid(.k = seq(1, 20, by = 1))
fitKnnUpVar = train(Y~., data=dataTrainVar,
                    method="knn",
                    metric=metric,
                    tuneGrid=grid,
                    trControl=trainControl)

# tuning kNN parameter
plot(fitKnnUpVar)
```

## Ensembles
### Stacking {#stacking}

Another way to improve the accuracy is to combine the predictions of several models into ensemble predictions. To do this, we first have to make sure predictions from sub-models have a low correlation.

```{r fig-correlogramPrediction}
#| fig.cap: "Correlogram of the predictions by models."

# correlation of models pairs of predictions
corrplot(modelCor(results), method="pie")
```
Here, that seems to be the case, since no one has a high correlation (*i.e* > 0.75). I've decided to use both algorithms using nominal attributes and the *up-sampling* method to build a *meta*-model (`multiFit`).

```{r model_stack}
# cross-validation
stackControl = trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 2,
                            savePredictions = "final",
                            classProbs = TRUE)

# list of models
set.seed(7)
multiModels = caretList(Y~., data = dataTrainVar,
                        methodList = c("glm", "knn"),
                        tuneList = list(glm = caretModelSpec(method='glm',
                                                             family='binomial'),
                                        knn = caretModelSpec(method='knn')),
                        trControl = trainControl)

# stacking
set.seed(7)
multiFits = caretEnsemble(multiModels,
                          trControl = stackControl)
print(multiFits)
```

The accuracy seems to be better than both algorithms alone (`Log`: `r round(fitLogUpVar$results$Accuracy, 2)` and `kNN`: `r round(fitKnnUpVar$results$Accuracy, 2)`, whereas `stacking`: `r round(multiFits$error$Accuracy, 2)`). However, the Kappa value of 0 is symptomatic of a problem that we're going to clarify when looking at the [predictions](#prediction).

### Boosting

Another way to used ensemble predictions is to use an algorithm based on boosting method. The idea here is to build multiple models where each of which learns to fix the prediction errors of a prior model in the chain. Let's test one of the most popular boosting machine learning algorithms, the **C5.0** classification based on a set of rules.

```{r boosting}
# C5.0 classification
set.seed(7)
fitC5UpVar = train(Y~., data = dataTrainVar,
                   method = "C5.0Rules",
                   metric = metric,
                   trControl = trainControl)
print(fitC5UpVar)
```

# Finalize Model
## Predictions on validation dataset {#prediction}

Here comes the part where I've tested the two ensemble models (stacking and boosting) predictions on your validation dataset. But first, I have to pre-process this dataset to make sure both models will correctly run.

```{r validation_test}
# remove missing value
validationNa = na.omit(validation)

# keep the same attributes used in the learning dataset
validationNa = validationNa[, .SD, .SDcols = colnames(dataTrainVar)]

# suitable name for R (i.e. the same as the learning dataset)
validationNa[, Y:=make.names(Y)]

# Yeo-Johnson transform of the attribute AAGE
validationNa$AAGE = predict(preprocessParams, validationNa[, "AAGE"])

# convert characters to factors
ncolCha = names(which(sapply(validationNa, class) == "character"))
validationNa[, (ncolCha) := lapply(.SD, as.factor), .SDcols=ncolCha]
```

Because I removed a lot of rows during the [data cleaning](#dataCleaning) step, it removed some levels of nominal attributes in the learning dataset. This implies that neither model can make prediction for this levels. To avoid errors when predicting on the validation dataset, I've removed any rows that do not match levels of the training dataset.

```{r levels_removed}
# only keep levels of factors in the validation datatset that match those in the learning one
for (i in which(dataTrainVar[, lapply(.SD, class), .SDcols=-"Y"] == "factor")){
  x = validationNa[, lapply(.SD, unique), .SDcols = i]
  x = as.character(x[[1]])
  y = dataTrainVar[, lapply(.SD, unique), .SDcols = i]
  y = as.character(y[[1]])
  if (length(setdiff(x, y))>0){
    delNrow = as.numeric()
    for (j in 1:length(setdiff(x, y))){
      delNrow = c(delNrow, which(validationNa[, i, with=F] == setdiff(x, y)[j]))
    }
    validationNa = validationNa[-delNrow, ]
  }
}
```

Now that we have a validation dataset that matches the same column names and levels of nominal attributes from the learning one, we can make predictions and compare them to the real values.

```{r prediction, eval=FALSE}
# make prediction
predictionsStack = predict(multiFits,
                           validationNa[, .SD, .SDcols = -"Y"],
                           type="raw")

predictionsC5 = predict(fitC5UpVar,
                        validationNa[, .SD, .SDcols = -"Y"],
                        type="raw")

# confusion matrix calculation
confStack = confusionMatrix(predictionsStack, validationNa[, Y])
confC5 = confusionMatrix(predictionsC5, validationNa[, Y])
confStack
confC5
```

We can see that the stacking model (*i.e* `Log` + `kNN`) has, indeed, an accuracy of `r round(multiFits$error$Accuracy, 2)` which is higher than the boosting model (*i.e* `C5.0`) with "only" `r round(fitC5UpVar$results$Accuracy, 2)`. However, the confusion matrix shows the stacking model simply predicts the same value (the one from the prevalent class), which results in a misleading accuracy (*i.e* accuracy = proportion of the majority class).

```{r accuracy, eval=FALSE}
# accuracy & proportion of the majority class
accuracyStack = round(confStack$overall[1], 2)                     # accuracy of stack model
propMajorClass = round(max(prop.table(table(validationNa$Y))), 2)  # prop of prevalent class

# accuracy = proportion of the majority class?
accuracyStack == propMajorClass
```

## Conclusion
### Some insights

Based on the calculation of variable importance in the `fitC5UpVar` model, the three variables that, in order of importance, are the most important when estimating if the income level will be more than 50 000$ per annum seems to be:

* the tax filer status (levels: Nonfiler)
* the age
* the number of persons that worked for employer

```{r fig-varImportance}
#| fig.cap: "Distribution and barplot of the three most important variables when predicting the income level of the person represented by the record."

# variable importance for the fitC5UpVar model
varImp(fitC5UpVar, scale = 100)

# some plot to illustrate
grid.arrange(
  ggplot(dataset, aes(x = FILESTAT, fill=Y))+
    geom_bar(position = position_dodge())+
    theme(legend.position = "top"),
  ggplot(dataset, aes(x = AAGE, fill = Y))+
    geom_bar()+
    theme(legend.position = "none"),
  ggplot(dataset, aes(x = NOEMP, fill = Y))+
    geom_bar(position = position_dodge())+
    theme(legend.position="none"),
  nrow = 3, heights = c(2, 1.5, 1.5))
```

### Go further

Here are few ideas to improve this analysis:

* Data Preparation
    + Data cleaning
        - Try to impute missing value using k nearest neighbors or bagged tree algorithm. It should allow the selected model to predict the income level for all people, regardless of the different levels of a factor in any nominal attributes.
        - Look closer for outliers and maybe try some methods like PCA to remove noises.
    + Data transform
        - Try normalized numerical variables.
        - Consider Box-Cox transform on other numerical attributes.
    + Data split
        - Simply use the whole learning dataset and not 10 \% of it.
* Evaluate algorithms
    + Spot Check Algorithm: test more linear and non-linear algorithms.
* Improve accuracy
    + Ensembles
        - For boosting, it's the same, let's take a look at several boosting machine learning algorithms which seems to be very promising.
        - For stacking, same as above, test different combinations of models, and then combine predictions using several different algorithms (*i.e* not just a simple linear model, like I did here).
