<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2017-12-05" />

<title>Dataiku Interview Project</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.19/datatables.js"></script>
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="site_libs/dt-ext-fixedcolumns-1.10.20/css/fixedColumns.dataTables.min.css" rel="stylesheet" />
<script src="site_libs/dt-ext-fixedcolumns-1.10.20/js/dataTables.fixedColumns.min.js"></script>
<link href="site_libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Portfolio - Joffrey JOUMAA</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Notebooks
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="analysis_jjoumaa.html">Dataiku Interview</a>
    </li>
    <li>
      <a href="presentation_jjoumaa.html">Saegus Interview</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Webapps
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="https://agile-wildwood-23168.herokuapp.com/">Dash (Python)</a>
    </li>
    <li>
      <a href="https://joffreyjoumaa.shinyapps.io/shiny_risk/">Shiny (R)</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="https://jjoumaa.ddns.net/weanlingNES/">weanlingNES</a>
    </li>
  </ul>
</li>
<li>
  <a href="resume_jjoumaa.html">Resume</a>
</li>
<li>
  <a href="https://github.com/SESjo">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="about.html">About me</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Dataiku Interview Project</h1>
<h4 class="author">Joffrey JOUMAA</h4>
<h4 class="date">December 5, 2017</h4>

</div>


<style>
body {
text-align: justify}
</style>
<p>Please find below my report about the dataset provided by Dataiku. This analysis has been realized using R (version 3.3.3) with all packages updated on 10/01/2022, under a UNIX/LINUX environment (Debian 9.2). The provided script (<code>code_jjoumaa.R</code>) runs in about an hour with an Intel i7 6600U based computer equipped with 32 Gb RAM (DDR4).</p>
<div id="prepare-problem" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Prepare Problem</h1>
<div id="load-libraries" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Load libraries</h2>
<p>Most of the data manipulation was done using <code>data.table</code> package due to its ability to handle large datasets. Data visualization was mostly done using <code>ggplot2</code> package. For the modelling part, I’ve used <code>caret</code> package, which provides a lot of useful tools for data science.</p>
<pre class="r"><code># data visualization
library(ggplot2)
library(corrplot)
library(gridExtra)
# data manipulation
library(data.table)
library(stringr)
# data modeling
library(caret)
library(caretEnsemble)
library(RANN)
# markdown table
library(knitr)
library(DT)
# (optional) multithreading
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="load-dataset" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Load dataset</h2>
<p>As we’ll see in the next steps, both datasets (learning and validation one) present a lot of missing values in the form of <code>?</code> in raw files, converted then in <code>NA</code> values in R.</p>
<pre class="r"><code># learning dataset
dataset = fread(&quot;census_income_learn.csv&quot;, 
                na.strings = &quot;?&quot;)
# validation dataset
validation = fread(&quot;census_income_test.csv&quot;, 
                   na.strings = &quot;?&quot;)</code></pre>
<p>Since neither dataset has column names, I’ve loaded the description file <code>census_income_metadata.txt</code>, and extracted rows mentioning column names.</p>
<pre class="r"><code># selection of the right rows containing colnames
datasetNames = fread(&quot;census_income_metadata.txt&quot;, 
                     nrows = (68-22), 
                     skip = 22, 
                     drop = &quot;V1&quot;)
# extraction of word in capital
colInter = datasetNames[, unlist(str_extract_all(V2, &#39;\\b[A-Z]+[A-Z0-9]\\b&#39;))]
colInter</code></pre>
<pre><code> [1] &quot;AAGE&quot;     &quot;ACLSWKR&quot;  &quot;ADTIND&quot;   &quot;ADTOCC&quot;   &quot;AGI&quot;      &quot;AHGA&quot;     &quot;AHRSPAY&quot; 
 [8] &quot;AHSCOL&quot;   &quot;AMARITL&quot;  &quot;AMJIND&quot;   &quot;AMJOCC&quot;   &quot;ARACE&quot;    &quot;AREORGN&quot;  &quot;ASEX&quot;    
[15] &quot;AUNMEM&quot;   &quot;AUNTYPE&quot;  &quot;AWKSTAT&quot;  &quot;CAPGAIN&quot;  &quot;CAPLOSS&quot;  &quot;DIVVAL&quot;   &quot;FEDTAX&quot;  
[22] &quot;FILESTAT&quot; &quot;GRINREG&quot;  &quot;GRINST&quot;   &quot;HHDFMX&quot;   &quot;HHDREL&quot;   &quot;MARSUPWT&quot; &quot;MIGMTR1&quot; 
[29] &quot;MIGMTR3&quot;  &quot;MIGMTR4&quot;  &quot;MIGSAME&quot;  &quot;MIGSUN&quot;   &quot;NOEMP&quot;    &quot;PARENT&quot;   &quot;PEARNVAL&quot;
[36] &quot;PEFNTVTY&quot; &quot;PEMNTVTY&quot; &quot;PENATVTY&quot; &quot;PRCITSHP&quot; &quot;PTOTVAL&quot;  &quot;SEOTR&quot;    &quot;TAXINC&quot;  
[43] &quot;VETQVA&quot;   &quot;VETYN&quot;    &quot;WKSWORK&quot; </code></pre>
<p>The problem was that 45 columns were mentioned in the description files (<code>colInter</code>), whereas both datasets only have 42 columns. To solve this, I’ve matched each column with their respective names based on the number of unique “value” by columns and the one exposed in the description file. Using this method, I’ve found that AGI, FEDTAX, PEARNVAL, PTOTVAL and TAXINC attributes could be removed from the initial column names, and that the attributes YEARS and Y (the attributes to model, <em>i.e</em> the income level) should be added.</p>
<pre class="r"><code># number of instances for each attributes
dataset[, sapply(.SD, function(x){length(unique(x))})]</code></pre>
<pre><code>   V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12   V13   V14   V15 
   91     9    52    47    17  1240     3     7    24    15     5    10     2     3     6 
  V16   V17   V18   V19   V20   V21   V22   V23   V24   V25   V26   V27   V28   V29   V30 
    8   132   113  1478     6     6    51    38     8 99800    10     9    10     3     4 
  V31   V32   V33   V34   V35   V36   V37   V38   V39   V40   V41   V42 
    7     5    43    43    43     5     3     3     3    53     2     2 </code></pre>
<pre class="r"><code># remove unexpected column names
colInter = colInter[-c(which(colInter == &quot;AGI&quot;))]
colInter = colInter[-c(which(colInter == &quot;FEDTAX&quot;))]
colInter = colInter[-c(which(colInter == &quot;PEARNVAL&quot;))]
colInter = colInter[-c(which(colInter == &quot;PTOTVAL&quot;))]
colInter = colInter[-c(which(colInter == &quot;TAXINC&quot;))]

# add two more column names
colInter = c(colInter, c(&quot;YEAR&quot;, &quot;Y&quot;))
colnames(dataset) = colInter[1:42]
colnames(validation) = colInter[1:42]

# dataset preview
datatable(head(validation, 5), 
          caption = &quot;Preview of the dataset&quot;, 
          extensions = &#39;FixedColumns&#39;, 
          options = list(
            dom = &quot;t&quot;, 
            scrollX = TRUE, 
            fixedColumns = FALSE))</code></pre>
<div id="htmlwidget-ec53886b876e86c3cdfe" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ec53886b876e86c3cdfe">{"x":{"filter":"none","vertical":false,"extensions":["FixedColumns"],"caption":"<caption>Preview of the dataset<\/caption>","data":[["1","2","3","4","5"],[38,44,2,35,49],["Private","Self-employed-not incorporated","Not in universe","Private","Private"],[6,37,0,29,4],[36,12,0,3,34],["1st 2nd 3rd or 4th grade","Associates degree-occup /vocational","Children","High school graduate","High school graduate"],[0,0,0,0,0],["Not in universe","Not in universe","Not in universe","Not in universe","Not in universe"],["Married-civilian spouse present","Married-civilian spouse present","Never married","Divorced","Divorced"],["Manufacturing-durable goods","Business and repair services","Not in universe or children","Transportation","Construction"],["Machine operators assmblrs &amp; inspctrs","Professional specialty","Not in universe","Executive admin and managerial","Precision production craft &amp; repair"],["White","White","White","White","White"],["Mexican (Mexicano)","All other","Mexican-American","All other","All other"],["Female","Female","Male","Female","Male"],["Not in universe","Not in universe","Not in universe","Not in universe","Not in universe"],["Not in universe","Not in universe","Not in universe","Not in universe","Not in universe"],["Full-time schedules","PT for econ reasons usually PT","Children or Armed Forces","Children or Armed Forces","Full-time schedules"],[0,0,0,0,0],[0,0,0,0,0],[0,2500,0,0,0],["Joint one under 65 &amp; one 65+","Joint both under 65","Nonfiler","Head of household","Single"],["Not in universe","Not in universe","Not in universe","Not in universe","Not in universe"],["Not in universe","Not in universe","Not in universe","Not in universe","Not in universe"],["Spouse of householder","Spouse of householder","Child &lt;18 never marr not in subfamily","Householder","Secondary individual"],["Spouse of householder","Spouse of householder","Child under 18 never married","Householder","Nonrelative of householder"],[1032.38,1462.33,1601.75,1866.88,1394.54],[null,null,null,"Nonmover",null],[null,null,null,"Nonmover",null],[null,null,null,"Nonmover",null],["Not in universe under 1 year old","Not in universe under 1 year old","Not in universe under 1 year old","Yes","Not in universe under 1 year old"],[null,null,null,"Not in universe",null],[4,1,0,5,4],["Not in universe","Not in universe","Both parents present","Not in universe","Not in universe"],["Mexico","United-States","United-States","United-States","United-States"],["Mexico","United-States","United-States","United-States","United-States"],["Mexico","United-States","United-States","United-States","United-States"],["Foreign born- Not a citizen of U S","Native- Born in the United States","Native- Born in the United States","Native- Born in the United States","Native- Born in the United States"],[0,0,0,2,0],["Not in universe","Not in universe","Not in universe","Not in universe","Not in universe"],[2,2,0,2,2],[12,26,0,52,50],[95,95,95,94,95],["- 50000.","- 50000.","- 50000.","- 50000.","- 50000."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>AAGE<\/th>\n      <th>ACLSWKR<\/th>\n      <th>ADTIND<\/th>\n      <th>ADTOCC<\/th>\n      <th>AHGA<\/th>\n      <th>AHRSPAY<\/th>\n      <th>AHSCOL<\/th>\n      <th>AMARITL<\/th>\n      <th>AMJIND<\/th>\n      <th>AMJOCC<\/th>\n      <th>ARACE<\/th>\n      <th>AREORGN<\/th>\n      <th>ASEX<\/th>\n      <th>AUNMEM<\/th>\n      <th>AUNTYPE<\/th>\n      <th>AWKSTAT<\/th>\n      <th>CAPGAIN<\/th>\n      <th>CAPLOSS<\/th>\n      <th>DIVVAL<\/th>\n      <th>FILESTAT<\/th>\n      <th>GRINREG<\/th>\n      <th>GRINST<\/th>\n      <th>HHDFMX<\/th>\n      <th>HHDREL<\/th>\n      <th>MARSUPWT<\/th>\n      <th>MIGMTR1<\/th>\n      <th>MIGMTR3<\/th>\n      <th>MIGMTR4<\/th>\n      <th>MIGSAME<\/th>\n      <th>MIGSUN<\/th>\n      <th>NOEMP<\/th>\n      <th>PARENT<\/th>\n      <th>PEFNTVTY<\/th>\n      <th>PEMNTVTY<\/th>\n      <th>PENATVTY<\/th>\n      <th>PRCITSHP<\/th>\n      <th>SEOTR<\/th>\n      <th>VETQVA<\/th>\n      <th>VETYN<\/th>\n      <th>WKSWORK<\/th>\n      <th>YEAR<\/th>\n      <th>Y<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","scrollX":true,"fixedColumns":false,"columnDefs":[{"className":"dt-right","targets":[1,3,4,6,17,18,19,25,31,37,39,40,41]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="summarize-data" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Summarize Data</h1>
<div id="descriptive-statistics" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Descriptive statistics</h2>
<p>It is difficult to present all attributes, but a quick summary provides a good overview of the structure.</p>
<pre class="r"><code># index of column with characters
ncolCha = which(sapply(dataset, class) == &quot;character&quot;)

# conversion of characters in factors
dataset[, (ncolCha):=lapply(.SD, as.factor), .SDcols = ncolCha]

# structure of the dataset
kable(data.frame(variable = names(dataset), 
                 classe = sapply(dataset, class), 
                 first_values = sapply(dataset, function(x) {
                   paste0(head(x), collapse = &quot;, &quot;)}), 
                 row.names = NULL))</code></pre>
<table>
<colgroup>
<col width="3%" />
<col width="3%" />
<col width="92%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="left">classe</th>
<th align="left">first_values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">AAGE</td>
<td align="left">integer</td>
<td align="left">73, 58, 18, 9, 10, 48</td>
</tr>
<tr class="even">
<td align="left">ACLSWKR</td>
<td align="left">factor</td>
<td align="left">Not in universe, Self-employed-not incorporated, Not in universe, Not in universe, Not in universe, Private</td>
</tr>
<tr class="odd">
<td align="left">ADTIND</td>
<td align="left">integer</td>
<td align="left">0, 4, 0, 0, 0, 40</td>
</tr>
<tr class="even">
<td align="left">ADTOCC</td>
<td align="left">integer</td>
<td align="left">0, 34, 0, 0, 0, 10</td>
</tr>
<tr class="odd">
<td align="left">AHGA</td>
<td align="left">factor</td>
<td align="left">High school graduate, Some college but no degree, 10th grade, Children, Children, Some college but no degree</td>
</tr>
<tr class="even">
<td align="left">AHRSPAY</td>
<td align="left">integer</td>
<td align="left">0, 0, 0, 0, 0, 1200</td>
</tr>
<tr class="odd">
<td align="left">AHSCOL</td>
<td align="left">factor</td>
<td align="left">Not in universe, Not in universe, High school, Not in universe, Not in universe, Not in universe</td>
</tr>
<tr class="even">
<td align="left">AMARITL</td>
<td align="left">factor</td>
<td align="left">Widowed, Divorced, Never married, Never married, Never married, Married-civilian spouse present</td>
</tr>
<tr class="odd">
<td align="left">AMJIND</td>
<td align="left">factor</td>
<td align="left">Not in universe or children, Construction, Not in universe or children, Not in universe or children, Not in universe or children, Entertainment</td>
</tr>
<tr class="even">
<td align="left">AMJOCC</td>
<td align="left">factor</td>
<td align="left">Not in universe, Precision production craft &amp; repair, Not in universe, Not in universe, Not in universe, Professional specialty</td>
</tr>
<tr class="odd">
<td align="left">ARACE</td>
<td align="left">factor</td>
<td align="left">White, White, Asian or Pacific Islander, White, White, Amer Indian Aleut or Eskimo</td>
</tr>
<tr class="even">
<td align="left">AREORGN</td>
<td align="left">factor</td>
<td align="left">All other, All other, All other, All other, All other, All other</td>
</tr>
<tr class="odd">
<td align="left">ASEX</td>
<td align="left">factor</td>
<td align="left">Female, Male, Female, Female, Female, Female</td>
</tr>
<tr class="even">
<td align="left">AUNMEM</td>
<td align="left">factor</td>
<td align="left">Not in universe, Not in universe, Not in universe, Not in universe, Not in universe, No</td>
</tr>
<tr class="odd">
<td align="left">AUNTYPE</td>
<td align="left">factor</td>
<td align="left">Not in universe, Not in universe, Not in universe, Not in universe, Not in universe, Not in universe</td>
</tr>
<tr class="even">
<td align="left">AWKSTAT</td>
<td align="left">factor</td>
<td align="left">Not in labor force, Children or Armed Forces, Not in labor force, Children or Armed Forces, Children or Armed Forces, Full-time schedules</td>
</tr>
<tr class="odd">
<td align="left">CAPGAIN</td>
<td align="left">integer</td>
<td align="left">0, 0, 0, 0, 0, 0</td>
</tr>
<tr class="even">
<td align="left">CAPLOSS</td>
<td align="left">integer</td>
<td align="left">0, 0, 0, 0, 0, 0</td>
</tr>
<tr class="odd">
<td align="left">DIVVAL</td>
<td align="left">integer</td>
<td align="left">0, 0, 0, 0, 0, 0</td>
</tr>
<tr class="even">
<td align="left">FILESTAT</td>
<td align="left">factor</td>
<td align="left">Nonfiler, Head of household, Nonfiler, Nonfiler, Nonfiler, Joint both under 65</td>
</tr>
<tr class="odd">
<td align="left">GRINREG</td>
<td align="left">factor</td>
<td align="left">Not in universe, South, Not in universe, Not in universe, Not in universe, Not in universe</td>
</tr>
<tr class="even">
<td align="left">GRINST</td>
<td align="left">factor</td>
<td align="left">Not in universe, Arkansas, Not in universe, Not in universe, Not in universe, Not in universe</td>
</tr>
<tr class="odd">
<td align="left">HHDFMX</td>
<td align="left">factor</td>
<td align="left">Other Rel 18+ ever marr not in subfamily, Householder, Child 18+ never marr Not in a subfamily, Child &lt;18 never marr not in subfamily, Child &lt;18 never marr not in subfamily, Spouse of householder</td>
</tr>
<tr class="even">
<td align="left">HHDREL</td>
<td align="left">factor</td>
<td align="left">Other relative of householder, Householder, Child 18 or older, Child under 18 never married, Child under 18 never married, Spouse of householder</td>
</tr>
<tr class="odd">
<td align="left">MARSUPWT</td>
<td align="left">numeric</td>
<td align="left">1700.09, 1053.55, 991.95, 1758.14, 1069.16, 162.61</td>
</tr>
<tr class="even">
<td align="left">MIGMTR1</td>
<td align="left">factor</td>
<td align="left">NA, MSA to MSA, NA, Nonmover, Nonmover, NA</td>
</tr>
<tr class="odd">
<td align="left">MIGMTR3</td>
<td align="left">factor</td>
<td align="left">NA, Same county, NA, Nonmover, Nonmover, NA</td>
</tr>
<tr class="even">
<td align="left">MIGMTR4</td>
<td align="left">factor</td>
<td align="left">NA, Same county, NA, Nonmover, Nonmover, NA</td>
</tr>
<tr class="odd">
<td align="left">MIGSAME</td>
<td align="left">factor</td>
<td align="left">Not in universe under 1 year old, No, Not in universe under 1 year old, Yes, Yes, Not in universe under 1 year old</td>
</tr>
<tr class="even">
<td align="left">MIGSUN</td>
<td align="left">factor</td>
<td align="left">NA, Yes, NA, Not in universe, Not in universe, NA</td>
</tr>
<tr class="odd">
<td align="left">NOEMP</td>
<td align="left">integer</td>
<td align="left">0, 1, 0, 0, 0, 1</td>
</tr>
<tr class="even">
<td align="left">PARENT</td>
<td align="left">factor</td>
<td align="left">Not in universe, Not in universe, Not in universe, Both parents present, Both parents present, Not in universe</td>
</tr>
<tr class="odd">
<td align="left">PEFNTVTY</td>
<td align="left">factor</td>
<td align="left">United-States, United-States, Vietnam, United-States, United-States, Philippines</td>
</tr>
<tr class="even">
<td align="left">PEMNTVTY</td>
<td align="left">factor</td>
<td align="left">United-States, United-States, Vietnam, United-States, United-States, United-States</td>
</tr>
<tr class="odd">
<td align="left">PENATVTY</td>
<td align="left">factor</td>
<td align="left">United-States, United-States, Vietnam, United-States, United-States, United-States</td>
</tr>
<tr class="even">
<td align="left">PRCITSHP</td>
<td align="left">factor</td>
<td align="left">Native- Born in the United States, Native- Born in the United States, Foreign born- Not a citizen of U S, Native- Born in the United States, Native- Born in the United States, Native- Born in the United States</td>
</tr>
<tr class="odd">
<td align="left">SEOTR</td>
<td align="left">integer</td>
<td align="left">0, 0, 0, 0, 0, 2</td>
</tr>
<tr class="even">
<td align="left">VETQVA</td>
<td align="left">factor</td>
<td align="left">Not in universe, Not in universe, Not in universe, Not in universe, Not in universe, Not in universe</td>
</tr>
<tr class="odd">
<td align="left">VETYN</td>
<td align="left">integer</td>
<td align="left">2, 2, 2, 0, 0, 2</td>
</tr>
<tr class="even">
<td align="left">WKSWORK</td>
<td align="left">integer</td>
<td align="left">0, 52, 0, 0, 0, 52</td>
</tr>
<tr class="odd">
<td align="left">YEAR</td>
<td align="left">integer</td>
<td align="left">95, 94, 95, 94, 94, 95</td>
</tr>
<tr class="even">
<td align="left">Y</td>
<td align="left">factor</td>
<td align="left">- 50000., - 50000., - 50000., - 50000., - 50000., - 50000.</td>
</tr>
</tbody>
</table>
<pre class="r"><code># summary of the dataset
summary(dataset)</code></pre>
<pre><code>      AAGE                                 ACLSWKR           ADTIND          ADTOCC     
 Min.   : 0.00   Not in universe               :100245   Min.   : 0.00   Min.   : 0.00  
 1st Qu.:15.00   Private                       : 72028   1st Qu.: 0.00   1st Qu.: 0.00  
 Median :33.00   Self-employed-not incorporated:  8445   Median : 0.00   Median : 0.00  
 Mean   :34.49   Local government              :  7784   Mean   :15.35   Mean   :11.31  
 3rd Qu.:50.00   State government              :  4227   3rd Qu.:33.00   3rd Qu.:26.00  
 Max.   :90.00   Self-employed-incorporated    :  3265   Max.   :51.00   Max.   :46.00  
                 (Other)                       :  3529                                  
                         AHGA          AHRSPAY                          AHSCOL      
 High school graduate      :48407   Min.   :   0.00   College or university:  5688  
 Children                  :47422   1st Qu.:   0.00   High school          :  6892  
 Some college but no degree:27820   Median :   0.00   Not in universe      :186943  
 Bachelors degree(BA AB BS):19865   Mean   :  55.43                                 
 7th and 8th grade         : 8007   3rd Qu.:   0.00                                 
 10th grade                : 7557   Max.   :9999.00                                 
 (Other)                   :40445                                                   
                            AMARITL                                    AMJIND      
 Divorced                       :12710   Not in universe or children      :100684  
 Married-A F spouse present     :  665   Retail trade                     : 17070  
 Married-civilian spouse present:84222   Manufacturing-durable goods      :  9015  
 Married-spouse absent          : 1518   Education                        :  8283  
 Never married                  :86485   Manufacturing-nondurable goods   :  6897  
 Separated                      : 3460   Finance insurance and real estate:  6145  
 Widowed                        :10463   (Other)                          : 51429  
                            AMJOCC                               ARACE       
 Not in universe               :100684   Amer Indian Aleut or Eskimo:  2251  
 Adm support including clerical: 14837   Asian or Pacific Islander  :  5835  
 Professional specialty        : 13940   Black                      : 20415  
 Executive admin and managerial: 12495   Other                      :  3657  
 Other service                 : 12099   White                      :167365  
 Sales                         : 11783                                       
 (Other)                       : 33685                                       
                      AREORGN           ASEX                    AUNMEM      
 All other                :171907   Female:103984   No             : 16034  
 Mexican-American         :  8079   Male  : 95539   Not in universe:180459  
 Mexican (Mexicano)       :  7234                   Yes            :  3030  
 Central or South American:  3895                                           
 Puerto Rican             :  3313                                           
 Other Spanish            :  2485                                           
 (Other)                  :  2610                                           
                  AUNTYPE                                     AWKSTAT      
 Job leaver           :   598   Children or Armed Forces          :123769  
 Job loser - on layoff:   976   Full-time schedules               : 40736  
 New entrant          :   439   Not in labor force                : 26808  
 Not in universe      :193453   PT for non-econ reasons usually FT:  3322  
 Other job loser      :  2038   Unemployed full-time              :  2311  
 Re-entrant           :  2019   PT for econ reasons usually PT    :  1209  
                                (Other)                           :  1368  
    CAPGAIN           CAPLOSS            DIVVAL       
 Min.   :    0.0   Min.   :   0.00   Min.   :    0.0  
 1st Qu.:    0.0   1st Qu.:   0.00   1st Qu.:    0.0  
 Median :    0.0   Median :   0.00   Median :    0.0  
 Mean   :  434.7   Mean   :  37.31   Mean   :  197.5  
 3rd Qu.:    0.0   3rd Qu.:   0.00   3rd Qu.:    0.0  
 Max.   :99999.0   Max.   :4608.00   Max.   :99999.0  
                                                      
                         FILESTAT                GRINREG                   GRINST      
 Head of household           : 7426   Abroad         :   530   Not in universe:183750  
 Joint both 65+              : 8332   Midwest        :  3575   California     :  1714  
 Joint both under 65         :67383   Northeast      :  2705   Utah           :  1063  
 Joint one under 65 &amp; one 65+: 3867   Not in universe:183750   Florida        :   849  
 Nonfiler                    :75094   South          :  4889   North Carolina :   812  
 Single                      :37421   West           :  4074   (Other)        : 10627  
                                                               NA&#39;s           :   708  
                                     HHDFMX                                HHDREL     
 Householder                            :53248   Householder                  :75475  
 Child &lt;18 never marr not in subfamily  :50326   Child under 18 never married :50426  
 Spouse of householder                  :41695   Spouse of householder        :41709  
 Nonfamily householder                  :22213   Child 18 or older            :14430  
 Child 18+ never marr Not in a subfamily:12030   Other relative of householder: 9703  
 Secondary individual                   : 6122   Nonrelative of householder   : 7601  
 (Other)                                :13889   (Other)                      :  179  
    MARSUPWT                    MIGMTR1                             MIGMTR3     
 Min.   :   37.87   Nonmover        :82538   Nonmover                   :82538  
 1st Qu.: 1061.62   MSA to MSA      :10601   Same county                : 9812  
 Median : 1618.31   NonMSA to nonMSA: 2811   Different county same state: 2797  
 Mean   : 1740.38   Not in universe : 1516   Not in universe            : 1516  
 3rd Qu.: 2188.61   MSA to nonMSA   :  790   Different region           : 1178  
 Max.   :18656.30   (Other)         : 1571   (Other)                    : 1986  
                    NA&#39;s            :99696   NA&#39;s                       :99696  
                        MIGMTR4                                  MIGSAME      
 Nonmover                   :82538   No                              : 15773  
 Same county                : 9812   Not in universe under 1 year old:101212  
 Different county same state: 2797   Yes                             : 82538  
 Not in universe            : 1516                                            
 Different state in South   :  973                                            
 (Other)                    : 2191                                            
 NA&#39;s                       :99696                                            
             MIGSUN          NOEMP                          PARENT      
 No             : 9987   Min.   :0.000   Both parents present  : 38983  
 Not in universe:84054   1st Qu.:0.000   Father only present   :  1883  
 Yes            : 5786   Median :1.000   Mother only present   : 12772  
 NA&#39;s           :99696   Mean   :1.956   Neither parent present:  1653  
                         3rd Qu.:4.000   Not in universe       :144232  
                         Max.   :6.000                                  
                                                                        
          PEFNTVTY               PEMNTVTY               PENATVTY     
 United-States:159163   United-States:160479   United-States:176989  
 Mexico       : 10008   Mexico       :  9781   Mexico       :  5767  
 Puerto-Rico  :  2680   Puerto-Rico  :  2473   Puerto-Rico  :  1400  
 Italy        :  2212   Italy        :  1844   Germany      :   851  
 Canada       :  1380   Canada       :  1451   Philippines  :   845  
 (Other)      : 17367   (Other)      : 17376   (Other)      : 10278  
 NA&#39;s         :  6713   NA&#39;s         :  6119   NA&#39;s         :  3393  
                                        PRCITSHP          SEOTR       
 Foreign born- Not a citizen of U S         : 13401   Min.   :0.0000  
 Foreign born- U S citizen by naturalization:  5855   1st Qu.:0.0000  
 Native- Born abroad of American Parent(s)  :  1756   Median :0.0000  
 Native- Born in Puerto Rico or U S Outlying:  1519   Mean   :0.1754  
 Native- Born in the United States          :176992   3rd Qu.:0.0000  
                                                      Max.   :2.0000  
                                                                      
             VETQVA           VETYN          WKSWORK           YEAR     
 No             :  1593   Min.   :0.000   Min.   : 0.00   Min.   :94.0  
 Not in universe:197539   1st Qu.:2.000   1st Qu.: 0.00   1st Qu.:94.0  
 Yes            :   391   Median :2.000   Median : 8.00   Median :94.0  
                          Mean   :1.515   Mean   :23.17   Mean   :94.5  
                          3rd Qu.:2.000   3rd Qu.:52.00   3rd Qu.:95.0  
                          Max.   :2.000   Max.   :52.00   Max.   :95.0  
                                                                        
        Y         
 - 50000.:187141  
 50000+. : 12382  
                  
                  
                  
                  
                  </code></pre>
<p>As mentioned above, a lot of missing values occurred in this dataset, especially for the attributes MIGMTR1, MIGMTR3, MIGMTR4 and MIGSUN.</p>
</div>
<div id="data-visualizations" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Data visualizations</h2>
<p>Another way to have an idea of the occurrence of missing values is to used data visualization.</p>
<pre class="r"><code># build dataPlot
dataPlot = setDT(melt(is.na(dataset)))

# subsample
dataPlot = dataPlot[,.SD[sample(nrow(dataset), 1000)], by=Var2
                    ][, `:=`(id_row, c(1:.N)), by = c(&quot;Var2&quot;)]

# missing values
ggplot(dataPlot, aes(x = Var2, y = id_row))+
  geom_tile(aes(fill = value))+
  labs(x = &quot;Attributes&quot;, y = &quot;Rows&quot;)+
  scale_fill_manual(values = c(&quot;white&quot;, &quot;black&quot;), 
                    labels = c(&quot;Real&quot;, &quot;Missing&quot;)) +
  theme(legend.position = &quot;top&quot;, 
        axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.key = element_rect(colour = &quot;black&quot;))</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:missingValue"></span>
<img src="analysis_jjoumaa_files/figure-html/missingValue-1.png" alt="Representation of missing values within the learning dataset" width="672" />
<p class="caption">
Figure 2.1: Representation of missing values within the learning dataset
</p>
</div>
<p>It also helps for the representation of numerical attributes.</p>
<pre class="r"><code># numerical attributes index
ncolInt = which(sapply(dataset, class) == &quot;integer&quot;)

# plot for each numerical attributes
ggplot(melt(dataset[, .SD, .SDcols = c(ncolInt, ncol(dataset))], id.vars = &quot;Y&quot;), 
       aes(x = value, col = Y, fill = Y))+ 
  geom_histogram(aes(y = ..density..), 
                 bins = 10, 
                 fill = &quot;white&quot;)+
  geom_density(alpha = .2)+
  theme(legend.position = &quot;top&quot;)+
  facet_wrap(~variable, scales = &quot;free&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densityHistogram"></span>
<img src="analysis_jjoumaa_files/figure-html/densityHistogram-1.png" alt="Histogram and density for each of the numerical attributes." width="672" />
<p class="caption">
Figure 2.2: Histogram and density for each of the numerical attributes.
</p>
</div>
<p>Nothing particularly clear here, except that some attributes do not seem to contain a lot of information (AHRSPAY, CAPGAIN, CAPLOSS, DIVVAL, SEOTR, VETYN, WKSWORK or YEAR). We can also note the attribute “AAGE” has a Gaussian-like distribution a bit shifted, especially for <code>Y = "- 50000.</code>, which may require to be transformed. Let’s have a look at another representation of this data.</p>
<pre class="r"><code>ggplot(melt(dataset[, .SD, .SDcols = c(ncolInt, ncol(dataset))], id.vars = &quot;Y&quot;), 
       aes(x = &quot;&quot;, y = value, col = Y, fill = Y)) + 
  geom_boxplot(alpha = .2)+
  theme(legend.position = &quot;top&quot;, 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        axis.ticks.x = element_blank())+
  facet_wrap(~variable, scales = &quot;free&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:boxplot"></span>
<img src="analysis_jjoumaa_files/figure-html/boxplot-1.png" alt="Boxplot for each of the numerical attributes." width="672" />
<p class="caption">
Figure 2.3: Boxplot for each of the numerical attributes.
</p>
</div>
<pre class="r"><code># nominal attributes names
ncolFac = names(which(sapply(dataset, class) == &quot;factor&quot;))

# plot for each nominal attributes
dataPlot=lapply(ncolFac[-c(length(ncolFac))], function(x){
  A = dataset[, levels(get(x)), by = Y]
  B = dataset[, table(get(x)), by = Y][, `:=`(V1 = as.integer(V1), 
                                         levels = as.factor(A$V1))]
  ggplot(B, aes(x = as.factor(levels), y = V1, fill = Y))+
    geom_bar(stat=&quot;identity&quot;, 
             position=position_dodge())+
    labs(y = &quot;Count&quot;, x = &quot;&quot;, title = x)+
    theme(legend.justification = c(0, 1), 
          legend.position = c(0, 1), 
          axis.text.x = element_text(angle = 45, hjust = 1))
})

# print of the two first plot
grid.arrange(grobs=list(dataPlot[[1]], dataPlot[[2]]), ncol=2)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nominalAttributes"></span>
<img src="analysis_jjoumaa_files/figure-html/nominalAttributes-1.png" alt="Barplot for each of the nominal attributes." width="672" />
<p class="caption">
Figure 2.4: Barplot for each of the nominal attributes.
</p>
</div>
<pre class="r"><code># another way to it, which appears not working using Rmarkdown document
# grid.arrange(grobs=dataPlot, ncol=2)</code></pre>
</div>
</div>
<div id="prepare-data" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Prepare Data</h1>
<div id="dataCleaning" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Data Cleaning</h2>
<p>When looking at Figure <a href="#fig:missingValue">2.1</a>, it appears that a lot of missing values occurred within the learning dataset. Here, I’ve taken the liberty to “simply” removed these values. Another way to deal with missing values would have been to use an imputation method, such as the <a href="https://topepo.github.io/caret/pre-processing.html#imputation">K-nearest neighbors method</a> <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> provided by the package <code>caret</code> (code commented below). Because this method requires high computational costs, I’ve preferred to stick to the first method, even if, as we’ll see in the next steps, it means the final model won’t be able to predict <code>Y</code> in some cases.</p>
<pre class="r"><code># removal of missing values
datasetNa = na.omit(dataset)

## or we could have done something like this:
# preProcValues = preProcess(dataset, method = c(&quot;knnImpute&quot;))
# dataset.imp = predict(preProcValues, dataset)</code></pre>
</div>
<div id="feature-selection" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Feature Selection</h2>
<p>As a first step to feature selection, I’ve decided to remove any attributes with a zero or near zero variance. For many models, this may cause the model to crash or the fit to be unstable.</p>
<pre class="r"><code># identification of near zero variance attributes
nzv = nearZeroVar(datasetNa, saveMetrics = TRUE)
datatable(nzv[nzv$nzv, ], 
          caption = &quot;Information relative to zero and near zero variance predictors.&quot;, 
          option = list(dom = &quot;t&quot;))</code></pre>
<div id="htmlwidget-c87996ef7305b544e38f" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-c87996ef7305b544e38f">{"x":{"filter":"none","vertical":false,"caption":"<caption>Information relative to zero and near zero variance predictors.<\/caption>","data":[["AHRSPAY","AHSCOL","AREORGN","AUNTYPE","AWKSTAT","CAPGAIN","CAPLOSS","DIVVAL","GRINST","PENATVTY","VETQVA","YEAR"],[230.485861182519,27.1447288238879,20.1018404907975,83.8842297174111,0,263.784482758621,464.039800995025,157.752310536044,49.7672839506173,31.0068443804035,125.929144385027,0],[0.918742773047409,0.00315357931251971,0.0105119310417324,0.00630715862503942,0.00105119310417324,0.133501524230001,0.111426469042363,0.979711973089457,0.0525596552086618,0.0420477241669295,0.00315357931251971,0.00105119310417324],[false,false,false,false,true,false,false,false,false,false,false,true],[true,true,true,true,true,true,true,true,true,true,true,true]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>freqRatio<\/th>\n      <th>percentUnique<\/th>\n      <th>zeroVar<\/th>\n      <th>nzv<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code># removal of these predictors
datasetNaNzv = datasetNa[, .SD, .SDcols = !nzv$nzv]</code></pre>
<p>The second step was to remove any attributes with a correlation higher than 0.75. This is a basic method, which could be improved by combining correlogram with significance test.</p>
<pre class="r"><code># matrix correlation
ncolInt = which(sapply(datasetNaNzv, class) == &quot;integer&quot;)
descrCor = cor(datasetNaNzv[, .SD, .SDcols = ncolInt])

# which attributes will be removed based on matrix correlation
corrplot(descrCor, method = &quot;pie&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:correlation"></span>
<img src="analysis_jjoumaa_files/figure-html/correlation-1.png" alt="Correlogram of dataset." width="672" />
<p class="caption">
Figure 3.1: Correlogram of dataset.
</p>
</div>
<pre class="r"><code>highlyCorDescr = findCorrelation(descrCor, cutoff = .75)
names(ncolInt[highlyCorDescr])</code></pre>
<pre><code>[1] &quot;WKSWORK&quot;</code></pre>
<pre class="r"><code># attribute &quot;WKSWORK&quot; removed
datasetNaNzv[, (names(ncolInt[highlyCorDescr])):= NULL]</code></pre>
<p>Here, I’ve only removed the attribute “WKSWORK”. At the end of the “Feature selection” step, the current dataset includes 30 attributes instead of 42 in the original one.</p>
</div>
<div id="data-transforms" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Data Transforms</h2>
<div id="yeo-johnson-transform" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Yeo-Johnson transform</h3>
<p>I’ve used the Yeo-Johnson transform, since the attributes “AAGE”, <em>i.e</em> the age of the people in this dataset, has a Gaussian-like distribution with a skew (Figure <a href="#fig:densityHistogram">2.2</a>. To make it “more Gaussian”, I’ve simply performed a <code>YeoJohnson</code> transform; I would have normally performed a <code>BoxCox</code> transform, but it does not support raw values that are equal to zero.</p>
<pre class="r"><code># calculate the pre-process parameters from the dataset
preprocessParams = preProcess(datasetNaNzv[, &quot;AAGE&quot;], method=c(&quot;YeoJohnson&quot;))

# transform the dataset using the parameters
datasetNaNzv$AAGE=predict(preprocessParams, datasetNaNzv[, &quot;AAGE&quot;])</code></pre>
</div>
<div id="one-hot-encoding" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> One-Hot encoding</h3>
<p>As we’ll see in <a href="#CompareAlgorithm">the next steps</a>, one-hot encoding was not really necessary for both algorithms tested hereafter. However, it may be a good way to improve computational costs for many algorithms that do not perform well, when dealing with nominal attributes.</p>
<pre class="r"><code># one-hot encoding
dummies = dummyVars(Y~., data = datasetNaNzv)
matDummies = predict(dummies, newdata = datasetNaNzv)</code></pre>
<p>As a consequence, some columns might appear to be a linear combination of others. To ensure the non-redundancy of information, I’ve removed these rows.</p>
<pre class="r"><code># remove linear combination
comboInfo = findLinearCombos(matDummies)
datasetNaNzvQr = as.data.table(matDummies[, -comboInfo$remove])
datasetNaNzvQr[, Y:=datasetNaNzv[, Y]]</code></pre>
</div>
</div>
<div id="split-out-validation-dataset" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Split-out validation dataset</h2>
<p>I know you provided a “test file”, but the learning dataset is large enough to be split in a learning dataset and a validation dataset by itself. Moreover, my computer is not powerful enough to run models on the whole learning dataset, so I’ve split the learning dataset with 10% of the data for the training process and the other 90% for testing (with a powerful enough computers, I would split the dataset using 80% of the data for the learning step and the other 20% for the evaluation). In the <a href="#prediction">last step</a>, I’ll evaluate the chosen model on the provided “test file”.</p>
<pre class="r"><code># identification of indexes for splitting
set.seed(7)
trainIndex = createDataPartition(datasetNaNzvQr$Y, 
                                 p = .1, 
                                 list = FALSE, 
                                 times = 1)

# split
datasetNaNzvQr[, Y:=as.factor(make.names(Y))]
dataTrain = datasetNaNzvQr[trainIndex, ]
dataTest = datasetNaNzvQr[-trainIndex, ]</code></pre>
</div>
</div>
<div id="evaluate-algorithms" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Evaluate Algorithms</h1>
<div id="test-options-and-evaluation-metric" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Test options and evaluation metric</h2>
<p>Here I’ve chosen to cross-validate the model using 5 folds repeated twice. As explained before, with a powerful enough computer I would encourage increasing these values. Because the problem to solve is a classification problem, the metric used to select the optimal model is the “Accuracy”, except for the <a href="#stacking">“Stacking”</a> part, where the area under the ROC curve was used.</p>
<pre class="r"><code># cross-validation
set.seed(7)
trainControl = trainControl(method = &quot;repeatedcv&quot;, 
                            number = 5, 
                            repeats = 2, 
                            savePredictions = &quot;final&quot;, # required for stacking
                            classProbs = TRUE)         # required for stacking

# metric evaluation
metric = &quot;Accuracy&quot;</code></pre>
</div>
<div id="spot-check-algorithms" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Spot Check Algorithms</h2>
<p>Because the dataset is strongly imbalance…</p>
<pre class="r"><code># proportion of each levels of Y
datatable(data.table(
  freq = table(datasetNaNzvQr$Y), 
  percentage = round(prop.table(table(datasetNaNzvQr$Y))*100, 2)), 
  caption = &quot;Proportion of each levels of the explaining variable.&quot;, 
  option = list(dom = &quot;t&quot;))</code></pre>
<div id="htmlwidget-445cbedba014cf6b242f" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-445cbedba014cf6b242f">{"x":{"filter":"none","vertical":false,"caption":"<caption>Proportion of each levels of the explaining variable.<\/caption>","data":[["1","2"],["X..50000.","X50000.."],[89651,5479],["X..50000.","X50000.."],[94.24,5.76]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>freq.V1<\/th>\n      <th>freq.N<\/th>\n      <th>percentage.V1<\/th>\n      <th>percentage.N<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-right","targets":[2,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>… it can have significant negative impact on model fitting (by the way, it seems consistent that a large proportion of these people earn less than 50 000$ pear annum). Here I’ve explored two different ways to deal with it:</p>
<ul>
<li><em>down-sampling</em>: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class.</li>
<li><em>up-sampling</em>: randomly sample (with replacement) the minority class to be the same size as the majority class.</li>
</ul>
<p>In addition, I’ve tested two different algorithms, the <strong>Logistic Regression</strong>, and <strong>k-Nearest Neighbors</strong>.</p>
<pre class="r"><code># down-sampling
trainControl$sampling = &quot;down&quot;

# logistic regression
set.seed(7)
fitLogDown = train(Y~., data = dataTrain, 
                   method = &quot;glm&quot;, 
                   family = &quot;binomial&quot;, 
                   metric = metric, 
                   trControl = trainControl)

# KNN
set.seed(7)
fitKnnDown = train(Y~., data = dataTrain, 
                   method = &quot;knn&quot;, 
                   metric = metric, 
                   trControl = trainControl)

# up-sampling
trainControl$sampling = &quot;up&quot;

# logistic regression
set.seed(7)
fitLogUp = train(Y~., data = dataTrain, 
                 method = &quot;glm&quot;, 
                 family = &quot;binomial&quot;, 
                 metric = metric, 
                 trControl = trainControl)

# KNN
set.seed(7)
fitKnnUp = train(Y~., data = dataTrain, 
                 method = &quot;knn&quot;, 
                 metric = metric, 
                 trControl = trainControl)</code></pre>
</div>
<div id="CompareAlgorithm" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Compare Algorithms</h2>
<pre class="r"><code># resample
results = resamples(list(KNN_UP = fitKnnUp, 
                         LOG_UP = fitLogUp, 
                         KNN_DOWN = fitKnnDown, 
                         LOG_DOWN = fitLogDown))

# summary
summary(results)</code></pre>
<pre><code>
Call:
summary.resamples(object = results)

Models: KNN_UP, LOG_UP, KNN_DOWN, LOG_DOWN 
Number of resamples: 10 

Accuracy 
              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
KNN_UP   0.7861272 0.7933030 0.8021026 0.7996639 0.8063584 0.8113505    0
LOG_UP   0.3783500 0.8217768 0.8276406 0.7607319 0.8509673 0.8680336    0
KNN_DOWN 0.6006306 0.6115127 0.6203388 0.6213483 0.6330793 0.6405675    0
LOG_DOWN 0.7519706 0.7656703 0.7871215 0.7820564 0.7952426 0.8072479    0

Kappa 
                Min.    1st Qu.     Median       Mean   3rd Qu.       Max. NA&#39;s
KNN_UP    0.12070882 0.13707161 0.14558179 0.14449985 0.1528234 0.16022824    0
LOG_UP   -0.03647692 0.27638025 0.28574482 0.23501992 0.3020309 0.34433398    0
KNN_DOWN  0.06257470 0.06846597 0.07426959 0.07606744 0.0841786 0.09147037    0
LOG_DOWN  0.19528444 0.20958260 0.22604420 0.22825945 0.2491513 0.25819232    0</code></pre>
<p>It appears the <em>up-sampling</em> method performs better than the <em>down-sampling</em>. In addition, the <strong>k-Nearest Neighbors</strong> algorithm provides better results than the <strong>Logistic Regression</strong> one. Let’s now compare both algorithms using the <em>up-sampling</em> method with a dataset including nominal attributes (<em>i.e.</em> not using one-hot encoding).</p>
<pre class="r"><code># new data split based on the dataset without hot encoding
datasetNaNzv[, Y:=as.factor(make.names(Y))]
dataTrainVar = datasetNaNzv[trainIndex, ]
dataTestVar = datasetNaNzv[-trainIndex, ]

# logistic regression
set.seed(7)
fitLogUpVar = train(Y~., data = dataTrainVar, 
                    method = &quot;glm&quot;, 
                    family = &quot;binomial&quot;, 
                    metric = metric, 
                    trControl = trainControl)

# KNN
set.seed(7)
fitKnnUpVar = train(Y~., data = dataTrainVar, 
                    method = &quot;knn&quot;, 
                    metric = metric, 
                    trControl = trainControl)

# resample
results = resamples(list(KNN_UP = fitKnnUp, 
                         LOG_UP = fitLogUp, 
                         KNN_DOWN = fitKnnDown, 
                         LOG_DOWN = fitLogDown, 
                         KNN_UP_VAR = fitKnnUpVar, 
                         LOG_UP_VAR = fitLogUpVar))

# summary
summary(results)</code></pre>
<pre><code>
Call:
summary.resamples(object = results)

Models: KNN_UP, LOG_UP, KNN_DOWN, LOG_DOWN, KNN_UP_VAR, LOG_UP_VAR 
Number of resamples: 10 

Accuracy 
                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
KNN_UP     0.7861272 0.7933030 0.8021026 0.7996639 0.8063584 0.8113505    0
LOG_UP     0.3783500 0.8217768 0.8276406 0.7607319 0.8509673 0.8680336    0
KNN_DOWN   0.6006306 0.6115127 0.6203388 0.6213483 0.6330793 0.6405675    0
LOG_DOWN   0.7519706 0.7656703 0.7871215 0.7820564 0.7952426 0.8072479    0
KNN_UP_VAR 0.7854890 0.7935655 0.8015769 0.7989281 0.8043878 0.8108250    0
LOG_UP_VAR 0.6337362 0.7690824 0.8334648 0.8061751 0.8622686 0.8686285    0

Kappa 
                  Min.    1st Qu.     Median       Mean   3rd Qu.       Max. NA&#39;s
KNN_UP      0.12070882 0.13707161 0.14558179 0.14449985 0.1528234 0.16022824    0
LOG_UP     -0.03647692 0.27638025 0.28574482 0.23501992 0.3020309 0.34433398    0
KNN_DOWN    0.06257470 0.06846597 0.07426959 0.07606744 0.0841786 0.09147037    0
LOG_DOWN    0.19528444 0.20958260 0.22604420 0.22825945 0.2491513 0.25819232    0
KNN_UP_VAR  0.12022934 0.13740228 0.14592863 0.14439255 0.1535522 0.15962758    0
LOG_UP_VAR -0.08948562 0.18339045 0.25932619 0.21196467 0.3022115 0.32559972    0</code></pre>
<pre class="r"><code>dotplot(results)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dotplot"></span>
<img src="analysis_jjoumaa_files/figure-html/dotplot-1.png" alt="Models comparison" width="672" />
<p class="caption">
Figure 4.1: Models comparison
</p>
</div>
<p>Using direct nominal attributes instead of one-hot encoding provides better results (<em>i.e.</em> higher accuracy) for the <strong>Logistic Regression</strong>, which is not the case for <strong>k-Nearest Neighbors</strong> algorithm, which provides the same accuracy.</p>
</div>
</div>
<div id="improve-accuracy" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Improve Accuracy</h1>
<div id="algorithm-tuning" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Algorithm Tuning</h2>
<p>Depending on the algorithm to chose, we can improve the accuracy by choosing the appropriate set of parameters. Below, you’ll find a recipe to find the optimal number of neighbors to set when using a <strong>k-Nearest Neighbors</strong> algorithm. Due to high computational cost, I did not run this code.</p>
<pre class="r"><code># search for the optimal number of neighbors K (did not run)
set.seed(7)
grid = expand.grid(.k = seq(1, 20, by = 1))
fitKnnUpVar = train(Y~., data=dataTrainVar, 
                    method=&quot;knn&quot;, 
                    metric=metric, 
                    tuneGrid=grid, 
                    trControl=trainControl)

# tuning kNN parameter
plot(fitKnnUpVar)</code></pre>
</div>
<div id="ensembles" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Ensembles</h2>
<div id="stacking" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Stacking</h3>
<p>Another way to improve the accuracy is to combine the predictions of several models into ensemble predictions. To do this, we first have to make sure predictions from sub-models have a low correlation.</p>
<pre class="r"><code># correlation of models pairs of predictions 
corrplot(modelCor(results), method=&quot;pie&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:correlogramPrediction"></span>
<img src="analysis_jjoumaa_files/figure-html/correlogramPrediction-1.png" alt="Correlogram of the predictions by models." width="672" />
<p class="caption">
Figure 5.1: Correlogram of the predictions by models.
</p>
</div>
<p>Here, that seems to be the case, since no one has a high correlation (<em>i.e</em> &gt; 0.75). I’ve decided to use both algorithms using nominal attributes and the <em>up-sampling</em> method to build a <em>meta</em>-model (<code>multiFit</code>).</p>
<pre class="r"><code># cross-validation
stackControl = trainControl(method = &quot;repeatedcv&quot;, 
                            number = 5, 
                            repeats = 2, 
                            savePredictions = &quot;final&quot;, 
                            classProbs = TRUE)

# list of models
set.seed(7)
multiModels = caretList(Y~., data = dataTrainVar, 
                        methodList = c(&quot;glm&quot;, &quot;knn&quot;), 
                        tuneList = list(glm = caretModelSpec(method=&#39;glm&#39;, 
                                                             family=&#39;binomial&#39;), 
                                        knn = caretModelSpec(method=&#39;knn&#39;)), 
                        trControl = trainControl)

# stacking
set.seed(7)
multiFits = caretEnsemble(multiModels, 
                          trControl = stackControl)
print(multiFits)</code></pre>
<pre><code>A glm ensemble of 4 base models: glm, knn, glm.1, knn.1

Ensemble results:
Generalized Linear Model 

19028 samples
    4 predictor
    2 classes: &#39;X..50000.&#39;, &#39;X50000..&#39; 

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 2 times) 
Summary of sample sizes: 15223, 15221, 15223, 15223, 15222, 15223, ... 
Resampling results:

  Accuracy   Kappa
  0.9424007  0    </code></pre>
<p>The accuracy seems to be better than both algorithms alone (<code>Log</code>: 0.81 and <code>kNN</code>: 0.8, 0.75, 0.71, whereas <code>stacking</code>: 0.94). However, the Kappa value of 0 is symptomatic of a problem that we’re going to clarify when looking at the <a href="#prediction">predictions</a>.</p>
</div>
<div id="boosting" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Boosting</h3>
<p>Another way to used ensemble predictions is to use an algorithm based on boosting method. The idea here is to build multiple models where each of which learns to fix the prediction errors of a prior model in the chain. Let’s test one of the most popular boosting machine learning algorithms, the <strong>C5.0</strong> classification based on a set of rules.</p>
<pre class="r"><code># C5.0 classification
set.seed(7)
fitC5UpVar = train(Y~., data = dataTrainVar, 
                   method = &quot;C5.0Rules&quot;, 
                   metric = metric, 
                   trControl = trainControl)</code></pre>
<pre><code>1 package is needed and is not installed. (C50). Would you like to try to install it now?
1: yes
2: no</code></pre>
<pre class="r"><code>print(fitC5UpVar)</code></pre>
<pre><code>Single C5.0 Ruleset 

9514 samples
  28 predictor
   2 classes: &#39;X..50000.&#39;, &#39;X50000..&#39; 

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 2 times) 
Summary of sample sizes: 7611, 7612, 7611, 7612, 7610, 7611, ... 
Addtional sampling using up-sampling

Resampling results:

  Accuracy   Kappa    
  0.9191195  0.3461387</code></pre>
</div>
</div>
</div>
<div id="finalize-model" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Finalize Model</h1>
<div id="prediction" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Predictions on validation dataset</h2>
<p>Here comes the part where I’ve tested the two ensemble models (stacking and boosting) predictions on your validation dataset. But first, I have to pre-process this dataset to make sure both models will correctly run.</p>
<pre class="r"><code># remove missing value
validationNa = na.omit(validation)

# keep the same attributes used in the learning dataset
validationNa = validationNa[, .SD, .SDcols = colnames(dataTrainVar)]

# suitable name for R (i.e. the same as the learning dataset)
validationNa[, Y:=make.names(Y)]

# Yeo-Johnson transform of the attribute AAGE
validationNa$AAGE = predict(preprocessParams, validationNa[, &quot;AAGE&quot;])

# convert characters to factors
ncolCha = names(which(sapply(validationNa, class) == &quot;character&quot;))
validationNa[, (ncolCha) := lapply(.SD, as.factor), .SDcols=ncolCha]</code></pre>
<p>Because I removed a lot of rows during the <a href="#dataCleaning">data cleaning</a> step, it removed some levels of nominal attributes in the learning dataset. This implies that neither model can make prediction for this levels. To avoid errors when predicting on the validation dataset, I’ve removed any rows that do not match levels of the training dataset.</p>
<pre class="r"><code># only keep levels of factors in the validation datatset that match those in the learning one
for (i in which(dataTrainVar[, lapply(.SD, class), .SDcols=-&quot;Y&quot;] == &quot;factor&quot;)){
  x = validationNa[, lapply(.SD, unique), .SDcols = i]
  x = as.character(x[[1]])
  y = dataTrainVar[, lapply(.SD, unique), .SDcols = i]
  y = as.character(y[[1]])
  if (length(setdiff(x, y))&gt;0){
    delNrow = as.numeric()
    for (j in 1:length(setdiff(x, y))){
      delNrow = c(delNrow, which(validationNa[, i, with=F] == setdiff(x, y)[j]))
    }
    validationNa = validationNa[-delNrow, ]
  }
}</code></pre>
<p>Now that we have a validation dataset that matches the same column names and levels of nominal attributes from the learning one, we can make predictions and compare them to the real values.</p>
<pre class="r"><code># make prediction
predictionsStack = predict(multiFits, 
                           validationNa[, .SD, .SDcols = -&quot;Y&quot;], 
                           type=&quot;raw&quot;)

predictionsC5 = predict(fitC5UpVar, 
                        validationNa[, .SD, .SDcols = -&quot;Y&quot;], 
                        type=&quot;raw&quot;)

# confusion matrix calculation
confStack = confusionMatrix(predictionsStack, validationNa[, Y])
confC5 = confusionMatrix(predictionsC5, validationNa[, Y])
confStack</code></pre>
<pre><code>Confusion Matrix and Statistics

           Reference
Prediction  X..50000. X50000..
  X..50000.     44700     2683
  X50000..          0        0
                                          
               Accuracy : 0.9434          
                 95% CI : (0.9413, 0.9454)
    No Information Rate : 0.9434          
    P-Value [Acc &gt; NIR] : 0.5051          
                                          
                  Kappa : 0               
                                          
 Mcnemar&#39;s Test P-Value : &lt;2e-16          
                                          
            Sensitivity : 1.0000          
            Specificity : 0.0000          
         Pos Pred Value : 0.9434          
         Neg Pred Value :    NaN          
             Prevalence : 0.9434          
         Detection Rate : 0.9434          
   Detection Prevalence : 1.0000          
      Balanced Accuracy : 0.5000          
                                          
       &#39;Positive&#39; Class : X..50000.       
                                          </code></pre>
<pre class="r"><code>confC5</code></pre>
<pre><code>Confusion Matrix and Statistics

           Reference
Prediction  X..50000. X50000..
  X..50000.     42426     1441
  X50000..       2274     1242
                                         
               Accuracy : 0.9216         
                 95% CI : (0.9191, 0.924)
    No Information Rate : 0.9434         
    P-Value [Acc &gt; NIR] : 1              
                                         
                  Kappa : 0.3596         
                                         
 Mcnemar&#39;s Test P-Value : &lt;2e-16         
                                         
            Sensitivity : 0.9491         
            Specificity : 0.4629         
         Pos Pred Value : 0.9672         
         Neg Pred Value : 0.3532         
             Prevalence : 0.9434         
         Detection Rate : 0.8954         
   Detection Prevalence : 0.9258         
      Balanced Accuracy : 0.7060         
                                         
       &#39;Positive&#39; Class : X..50000.      
                                         </code></pre>
<p>We can see that the stacking model (<em>i.e</em> <code>Log</code> + <code>kNN</code>) has, indeed, an accuracy of 0.94 which is higher than the boosting model (<em>i.e</em> <code>C5.0</code>) with “only” 0.92. However, the confusion matrix shows the stacking model simply predicts the same value (the one from the prevalent class), which results in a misleading accuracy (<em>i.e</em> accuracy = proportion of the majority class).</p>
<pre class="r"><code># accuracy &amp; proportion of the majority class
accuracyStack = round(confStack$overall[1], 2)                     # accuracy of stack model
propMajorClass = round(max(prop.table(table(validationNa$Y))), 2)  # prop of prevalent class

# accuracy = proportion of the majority class?
accuracyStack == propMajorClass</code></pre>
<pre><code>Accuracy 
    TRUE </code></pre>
</div>
<div id="conclusion" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Conclusion</h2>
<div id="some-insights" class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Some insights</h3>
<p>Based on the calculation of variable importance in the <code>fitC5UpVar</code> model, the three variables that, in order of importance, are the most important when estimating if the income level will be more than 50 000$ per annum seems to be:</p>
<ul>
<li>the tax filer status (levels: Nonfiler)</li>
<li>the age</li>
<li>the number of persons that worked for employer</li>
</ul>
<pre class="r"><code># variable importance for the fitC5UpVar model
varImp(fitC5UpVar, scale = 100)</code></pre>
<pre><code>C5.0Rules variable importance

  only 20 most important variables shown (out of 307)

                                       Overall
FILESTATNonfiler                        100.00
MARSUPWT                                 41.39
NOEMP                                    40.76
AAGE                                     39.66
ASEXMale                                 39.10
ADTOCC                                   26.51
ACLSWKRSelf-employed-incorporated        25.32
AMJINDMedicalexcepthospital              23.53
AHGAMastersdegree(MAMSMEngMEdMSWMBA)     23.32
AHGASomecollegebutnodegree               20.93
AHGAHighschoolgraduate                   20.49
AMJOCCSales                              19.13
HHDFMXSpouseofhouseholder                16.87
HHDFMXChild18+nevermarrNotinasubfamily   16.74
ACLSWKRNotinuniverse                     14.07
AMJOCCProtectiveservices                 13.38
AMARITLSeparated                         13.29
AHGAAssociatesdegree-occup/vocational    12.90
ADTIND                                   10.77
MIGMTR4DifferentstateinWest              10.52</code></pre>
<pre class="r"><code># some plot to illustrate
grid.arrange(
  ggplot(dataset, aes(x = FILESTAT, fill=Y))+
    geom_bar(position = position_dodge())+
    theme(legend.position = &quot;top&quot;), 
  ggplot(dataset, aes(x = AAGE, fill = Y))+
    geom_bar()+
    theme(legend.position = &quot;none&quot;), 
  ggplot(dataset, aes(x = NOEMP, fill = Y))+
    geom_bar(position = position_dodge())+
    theme(legend.position=&quot;none&quot;), 
  nrow = 3, heights = c(2, 1.5, 1.5))</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varImportance"></span>
<img src="analysis_jjoumaa_files/figure-html/varImportance-1.png" alt="Distribution and barplot of the three most important variables when predicting the income level of the person represented by the record." width="672" />
<p class="caption">
Figure 6.1: Distribution and barplot of the three most important variables when predicting the income level of the person represented by the record.
</p>
</div>
</div>
<div id="go-further" class="section level3" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Go further</h3>
<p>Here are few ideas to improve this analysis:</p>
<ul>
<li>Data Preparation
<ul>
<li>Data cleaning
<ul>
<li>Try to impute missing value using k nearest neighbors or bagged tree algorithm. It should allow the selected model to predict the income level for all people, regardless of the different levels of a factor in any nominal attributes.</li>
<li>Look closer for outliers and maybe try some methods like PCA to remove noises.</li>
</ul></li>
<li>Data transform
<ul>
<li>Try normalized numerical variables.</li>
<li>Consider Box-Cox transform on other numerical attributes.</li>
</ul></li>
<li>Data split
<ul>
<li>Simply use the whole learning dataset and not 10 % of it.</li>
</ul></li>
</ul></li>
<li>Evaluate algorithms
<ul>
<li>Spot Check Algorithm: test more linear and non-linear algorithms.</li>
</ul></li>
<li>Improve accuracy
<ul>
<li>Ensembles
<ul>
<li>For boosting, it’s the same, let’s take a look at several boosting machine learning algorithms which seems to be very promising.</li>
<li>For stacking, same as above, test different combinations of models, and then combine predictions using several different algorithms (<em>i.e</em> not just a simple linear model, like I did here).</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Another method is implemented based on bagged trees but at higher computational costs.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<!–– https://stackoverflow.com/questions/56797898/rmarkdown-html-site-yml-navbar-href-links-open-in-new-tab-with-target-blank ––>

<script type="text/javascript" src="links.js"></script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
