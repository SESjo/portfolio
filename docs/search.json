[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Joffrey Joumaa | portfolio",
    "section": "",
    "text": "“A variety of experiences yields new perspectives”\nHighly data savvy, I bring a diverse set of experiences in both industry and academia, including expertise in machine learning, data visualization, and programming. My ability to manage projects, communicate effectively, and work collaboratively has allowed me to excel in a variety of work environments.\nI am confident that my skills and experience will make me a valuable asset for any relevant data-related position, and I look forward to the opportunity to contribute my expertise to your organization.\nKey Achievements include:\n\n\n\n\na PhD in a quantitative discipline involving wildlife motion sensors, i.e. wearable devices\nthe development of a greenhouse gas emissions forecasting model, reducing computation time from days to minutes\nthe implementation of a self-service business intelligence approach with the Tableau product suite\nthe publication of eight scientific papers in highly ranked journals\nthe use of version control and containerization, to ensure reproducible work and create consistent and portable analyses"
  },
  {
    "objectID": "dataiku_jjoumaa.html",
    "href": "dataiku_jjoumaa.html",
    "title": "Dataiku Interview Project",
    "section": "",
    "text": "Please find below my report about the dataset provided by Dataiku. This analysis has been realized using R (version 3.3.3) with all packages updated on 12/02/2023, under a UNIX/LINUX environment (Debian 9.2). The provided script (code_jjoumaa.R) runs in about an hour with an Intel i7 6600U based computer equipped with 32 Gb RAM (DDR4)."
  },
  {
    "objectID": "dataiku_jjoumaa.html#load-libraries",
    "href": "dataiku_jjoumaa.html#load-libraries",
    "title": "Dataiku Interview Project",
    "section": "\n1.1 Load libraries",
    "text": "1.1 Load libraries\nMost of the data manipulation was done using data.table package due to its ability to handle large datasets. Data visualization was mostly done using ggplot2 package. For the modelling part, I’ve used caret package, which provides a lot of useful tools for data science.\n\nCode# data visualization\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(gridExtra)\n# data manipulation\nlibrary(data.table)\nlibrary(stringr)\nlibrary(magrittr)\n# data modeling\nlibrary(caret)\nlibrary(caretEnsemble)\nlibrary(RANN)\n# markdown table\nlibrary(knitr)\nlibrary(DT)\nlibrary(pander)\n# (optional) multithreading\nlibrary(doMC)\nregisterDoMC(cores = 4)"
  },
  {
    "objectID": "dataiku_jjoumaa.html#load-dataset",
    "href": "dataiku_jjoumaa.html#load-dataset",
    "title": "Dataiku Interview Project",
    "section": "\n1.2 Load dataset",
    "text": "1.2 Load dataset\nAs we’ll see in the next steps, both datasets (learning and validation one) present a lot of missing values in the form of ? in raw files, converted then in NA values in R.\n\nCode# learning dataset\ndataset <- fread(\"./dataiku_data/census_income_learn.csv\",\n  na.strings = \"?\"\n)\n# validation dataset\nvalidation <- fread(\"./dataiku_data/census_income_test.csv\",\n  na.strings = \"?\"\n)\n\n\nSince neither dataset has column names, I’ve loaded the description file census_income_metadata.txt, and extracted rows mentioning column names.\n\nCode# selection of the right rows containing colnames\ndatasetNames <- fread(\"./dataiku_data/census_income_metadata.txt\",\n  nrows = (68 - 22),\n  skip = 22,\n  drop = \"V1\"\n)\n# extraction of word in capital\ncolInter <- datasetNames[, unlist(str_extract_all(V2, \"\\\\b[A-Z]+[A-Z0-9]\\\\b\"))]\ncolInter\n\n [1] \"AAGE\"     \"ACLSWKR\"  \"ADTIND\"   \"ADTOCC\"   \"AGI\"      \"AHGA\"    \n [7] \"AHRSPAY\"  \"AHSCOL\"   \"AMARITL\"  \"AMJIND\"   \"AMJOCC\"   \"ARACE\"   \n[13] \"AREORGN\"  \"ASEX\"     \"AUNMEM\"   \"AUNTYPE\"  \"AWKSTAT\"  \"CAPGAIN\" \n[19] \"CAPLOSS\"  \"DIVVAL\"   \"FEDTAX\"   \"FILESTAT\" \"GRINREG\"  \"GRINST\"  \n[25] \"HHDFMX\"   \"HHDREL\"   \"MARSUPWT\" \"MIGMTR1\"  \"MIGMTR3\"  \"MIGMTR4\" \n[31] \"MIGSAME\"  \"MIGSUN\"   \"NOEMP\"    \"PARENT\"   \"PEARNVAL\" \"PEFNTVTY\"\n[37] \"PEMNTVTY\" \"PENATVTY\" \"PRCITSHP\" \"PTOTVAL\"  \"SEOTR\"    \"TAXINC\"  \n[43] \"VETQVA\"   \"VETYN\"    \"WKSWORK\" \n\n\nThe problem was that 45 columns were mentioned in the description files (colInter), whereas both datasets only have 42 columns. To solve this, I’ve matched each column with their respective names based on the number of unique “value” by columns and the one exposed in the description file. Using this method, I’ve found that AGI, FEDTAX, PEARNVAL, PTOTVAL and TAXINC attributes could be removed from the initial column names, and that the attributes YEARS and Y (the attributes to model, i.e the income level) should be added.\n\nCode# number of instances for each attributes\ndataset[, sapply(.SD, function(x) {\n  length(unique(x))\n})]\n\n   V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12   V13 \n   91     9    52    47    17  1240     3     7    24    15     5    10     2 \n  V14   V15   V16   V17   V18   V19   V20   V21   V22   V23   V24   V25   V26 \n    3     6     8   132   113  1478     6     6    51    38     8 99800    10 \n  V27   V28   V29   V30   V31   V32   V33   V34   V35   V36   V37   V38   V39 \n    9    10     3     4     7     5    43    43    43     5     3     3     3 \n  V40   V41   V42 \n   53     2     2 \n\nCode# remove unexpected column names\ncolInter <- colInter[-c(which(colInter == \"AGI\"))]\ncolInter <- colInter[-c(which(colInter == \"FEDTAX\"))]\ncolInter <- colInter[-c(which(colInter == \"PEARNVAL\"))]\ncolInter <- colInter[-c(which(colInter == \"PTOTVAL\"))]\ncolInter <- colInter[-c(which(colInter == \"TAXINC\"))]\n\n# add two more column names\ncolInter <- c(colInter, c(\"YEAR\", \"Y\"))\ncolnames(dataset) <- colInter[1:42]\ncolnames(validation) <- colInter[1:42]\n\n# dataset preview\ndatatable(head(validation, 5),\n  extensions = \"FixedColumns\",\n  options = list(\n    dom = \"t\",\n    scrollX = TRUE,\n    fixedColumns = FALSE\n  )\n)"
  },
  {
    "objectID": "dataiku_jjoumaa.html#descriptive-statistics",
    "href": "dataiku_jjoumaa.html#descriptive-statistics",
    "title": "Dataiku Interview Project",
    "section": "\n2.1 Descriptive statistics",
    "text": "2.1 Descriptive statistics\nIt is difficult to present all attributes, but a quick summary provides a good overview of the structure.\n\nCode# index of column with characters\nncolCha <- which(sapply(dataset, class) == \"character\")\n\n# conversion of characters in factors\ndataset[, (ncolCha) := lapply(.SD, as.factor), .SDcols = ncolCha]\n\n# structure of the dataset\nkable(data.frame(\n  variable = names(dataset),\n  classe = sapply(dataset, class),\n  first_values = sapply(dataset, function(x) {\n    paste0(head(x), collapse = \", \")\n  }),\n  row.names = NULL\n))\n\n\n\nTable 1: Structure of the dataset\n\n\n\n\n\n\nvariable\nclasse\nfirst_values\n\n\n\nAAGE\ninteger\n73, 58, 18, 9, 10, 48\n\n\nACLSWKR\nfactor\nNot in universe, Self-employed-not incorporated, Not in universe, Not in universe, Not in universe, Private\n\n\nADTIND\ninteger\n0, 4, 0, 0, 0, 40\n\n\nADTOCC\ninteger\n0, 34, 0, 0, 0, 10\n\n\nAHGA\nfactor\nHigh school graduate, Some college but no degree, 10th grade, Children, Children, Some college but no degree\n\n\nAHRSPAY\ninteger\n0, 0, 0, 0, 0, 1200\n\n\nAHSCOL\nfactor\nNot in universe, Not in universe, High school, Not in universe, Not in universe, Not in universe\n\n\nAMARITL\nfactor\nWidowed, Divorced, Never married, Never married, Never married, Married-civilian spouse present\n\n\nAMJIND\nfactor\nNot in universe or children, Construction, Not in universe or children, Not in universe or children, Not in universe or children, Entertainment\n\n\nAMJOCC\nfactor\nNot in universe, Precision production craft & repair, Not in universe, Not in universe, Not in universe, Professional specialty\n\n\nARACE\nfactor\nWhite, White, Asian or Pacific Islander, White, White, Amer Indian Aleut or Eskimo\n\n\nAREORGN\nfactor\nAll other, All other, All other, All other, All other, All other\n\n\nASEX\nfactor\nFemale, Male, Female, Female, Female, Female\n\n\nAUNMEM\nfactor\nNot in universe, Not in universe, Not in universe, Not in universe, Not in universe, No\n\n\nAUNTYPE\nfactor\nNot in universe, Not in universe, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nAWKSTAT\nfactor\nNot in labor force, Children or Armed Forces, Not in labor force, Children or Armed Forces, Children or Armed Forces, Full-time schedules\n\n\nCAPGAIN\ninteger\n0, 0, 0, 0, 0, 0\n\n\nCAPLOSS\ninteger\n0, 0, 0, 0, 0, 0\n\n\nDIVVAL\ninteger\n0, 0, 0, 0, 0, 0\n\n\nFILESTAT\nfactor\nNonfiler, Head of household, Nonfiler, Nonfiler, Nonfiler, Joint both under 65\n\n\nGRINREG\nfactor\nNot in universe, South, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nGRINST\nfactor\nNot in universe, Arkansas, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nHHDFMX\nfactor\nOther Rel 18+ ever marr not in subfamily, Householder, Child 18+ never marr Not in a subfamily, Child <18 never marr not in subfamily, Child <18 never marr not in subfamily, Spouse of householder\n\n\nHHDREL\nfactor\nOther relative of householder, Householder, Child 18 or older, Child under 18 never married, Child under 18 never married, Spouse of householder\n\n\nMARSUPWT\nnumeric\n1700.09, 1053.55, 991.95, 1758.14, 1069.16, 162.61\n\n\nMIGMTR1\nfactor\nNA, MSA to MSA, NA, Nonmover, Nonmover, NA\n\n\nMIGMTR3\nfactor\nNA, Same county, NA, Nonmover, Nonmover, NA\n\n\nMIGMTR4\nfactor\nNA, Same county, NA, Nonmover, Nonmover, NA\n\n\nMIGSAME\nfactor\nNot in universe under 1 year old, No, Not in universe under 1 year old, Yes, Yes, Not in universe under 1 year old\n\n\nMIGSUN\nfactor\nNA, Yes, NA, Not in universe, Not in universe, NA\n\n\nNOEMP\ninteger\n0, 1, 0, 0, 0, 1\n\n\nPARENT\nfactor\nNot in universe, Not in universe, Not in universe, Both parents present, Both parents present, Not in universe\n\n\nPEFNTVTY\nfactor\nUnited-States, United-States, Vietnam, United-States, United-States, Philippines\n\n\nPEMNTVTY\nfactor\nUnited-States, United-States, Vietnam, United-States, United-States, United-States\n\n\nPENATVTY\nfactor\nUnited-States, United-States, Vietnam, United-States, United-States, United-States\n\n\nPRCITSHP\nfactor\nNative- Born in the United States, Native- Born in the United States, Foreign born- Not a citizen of U S, Native- Born in the United States, Native- Born in the United States, Native- Born in the United States\n\n\nSEOTR\ninteger\n0, 0, 0, 0, 0, 2\n\n\nVETQVA\nfactor\nNot in universe, Not in universe, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nVETYN\ninteger\n2, 2, 2, 0, 0, 2\n\n\nWKSWORK\ninteger\n0, 52, 0, 0, 0, 52\n\n\nYEAR\ninteger\n95, 94, 95, 94, 94, 95\n\n\nY\nfactor\n- 50000., - 50000., - 50000., - 50000., - 50000., - 50000.\n\n\n\n\n\n\n\nCode# summary of the dataset\npander(summary(dataset))\n\n\nTable 2: Summary of the dataset\n\n\n\n\n(a) Table continues below\n\n\n\n\n\n\n\nAAGE\nACLSWKR\nADTIND\nADTOCC\n\n\n\nMin. : 0.00\nNot in universe :100245\nMin. : 0.00\nMin. : 0.00\n\n\n1st Qu.:15.00\nPrivate : 72028\n1st Qu.: 0.00\n1st Qu.: 0.00\n\n\nMedian :33.00\nSelf-employed-not incorporated: 8445\nMedian : 0.00\nMedian : 0.00\n\n\nMean :34.49\nLocal government : 7784\nMean :15.35\nMean :11.31\n\n\n3rd Qu.:50.00\nState government : 4227\n3rd Qu.:33.00\n3rd Qu.:26.00\n\n\nMax. :90.00\nSelf-employed-incorporated : 3265\nMax. :51.00\nMax. :46.00\n\n\nNA\n(Other) : 3529\nNA\nNA\n\n\n\n\n\n\n\n\n(b) Table continues below\n\n\n\n\n\n\nAHGA\nAHRSPAY\nAHSCOL\n\n\n\nHigh school graduate :48407\nMin. : 0.00\nCollege or university: 5688\n\n\nChildren :47422\n1st Qu.: 0.00\nHigh school : 6892\n\n\nSome college but no degree:27820\nMedian : 0.00\nNot in universe :186943\n\n\nBachelors degree(BA AB BS):19865\nMean : 55.43\nNA\n\n\n7th and 8th grade : 8007\n3rd Qu.: 0.00\nNA\n\n\n10th grade : 7557\nMax. :9999.00\nNA\n\n\n(Other) :40445\nNA\nNA\n\n\n\n\n\n\n\n\n(c) Table continues below\n\n\n\n\n\nAMARITL\nAMJIND\n\n\n\nDivorced :12710\nNot in universe or children :100684\n\n\nMarried-A F spouse present : 665\nRetail trade : 17070\n\n\nMarried-civilian spouse present:84222\nManufacturing-durable goods : 9015\n\n\nMarried-spouse absent : 1518\nEducation : 8283\n\n\nNever married :86485\nManufacturing-nondurable goods : 6897\n\n\nSeparated : 3460\nFinance insurance and real estate: 6145\n\n\nWidowed :10463\n(Other) : 51429\n\n\n\n\n\n\n\n\n(d) Table continues below\n\n\n\n\n\nAMJOCC\nARACE\n\n\n\nNot in universe :100684\nAmer Indian Aleut or Eskimo: 2251\n\n\nAdm support including clerical: 14837\nAsian or Pacific Islander : 5835\n\n\nProfessional specialty : 13940\nBlack : 20415\n\n\nExecutive admin and managerial: 12495\nOther : 3657\n\n\nOther service : 12099\nWhite :167365\n\n\nSales : 11783\nNA\n\n\n(Other) : 33685\nNA\n\n\n\n\n\n\n\n\n(e) Table continues below\n\n\n\n\n\n\nAREORGN\nASEX\nAUNMEM\n\n\n\nAll other :171907\nFemale:103984\nNo : 16034\n\n\nMexican-American : 8079\nMale : 95539\nNot in universe:180459\n\n\nMexican (Mexicano) : 7234\nNA\nYes : 3030\n\n\nCentral or South American: 3895\nNA\nNA\n\n\nPuerto Rican : 3313\nNA\nNA\n\n\nOther Spanish : 2485\nNA\nNA\n\n\n(Other) : 2610\nNA\nNA\n\n\n\n\n\n\n\n\n(f) Table continues below\n\n\n\n\n\n\nAUNTYPE\nAWKSTAT\nCAPGAIN\n\n\n\nJob leaver : 598\nChildren or Armed Forces :123769\nMin. : 0.0\n\n\nJob loser - on layoff: 976\nFull-time schedules : 40736\n1st Qu.: 0.0\n\n\nNew entrant : 439\nNot in labor force : 26808\nMedian : 0.0\n\n\nNot in universe :193453\nPT for non-econ reasons usually FT: 3322\nMean : 434.7\n\n\nOther job loser : 2038\nUnemployed full-time : 2311\n3rd Qu.: 0.0\n\n\nRe-entrant : 2019\nPT for econ reasons usually PT : 1209\nMax. :99999.0\n\n\nNA\n(Other) : 1368\nNA\n\n\n\n\n\n\n\n\n(g) Table continues below\n\n\n\n\n\n\nCAPLOSS\nDIVVAL\nFILESTAT\n\n\n\nMin. : 0.00\nMin. : 0.0\nHead of household : 7426\n\n\n1st Qu.: 0.00\n1st Qu.: 0.0\nJoint both 65+ : 8332\n\n\nMedian : 0.00\nMedian : 0.0\nJoint both under 65 :67383\n\n\nMean : 37.31\nMean : 197.5\nJoint one under 65 & one 65+: 3867\n\n\n3rd Qu.: 0.00\n3rd Qu.: 0.0\nNonfiler :75094\n\n\nMax. :4608.00\nMax. :99999.0\nSingle :37421\n\n\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n(h) Table continues below\n\n\n\n\n\nGRINREG\nGRINST\n\n\n\nAbroad : 530\nNot in universe:183750\n\n\nMidwest : 3575\nCalifornia : 1714\n\n\nNortheast : 2705\nUtah : 1063\n\n\nNot in universe:183750\nFlorida : 849\n\n\nSouth : 4889\nNorth Carolina : 812\n\n\nWest : 4074\n(Other) : 10627\n\n\nNA\nNA’s : 708\n\n\n\n\n\n\n\n\n(i) Table continues below\n\n\n\n\n\nHHDFMX\nHHDREL\n\n\n\nHouseholder :53248\nHouseholder :75475\n\n\nChild <18 never marr not in subfamily :50326\nChild under 18 never married :50426\n\n\nSpouse of householder :41695\nSpouse of householder :41709\n\n\nNonfamily householder :22213\nChild 18 or older :14430\n\n\nChild 18+ never marr Not in a subfamily:12030\nOther relative of householder: 9703\n\n\nSecondary individual : 6122\nNonrelative of householder : 7601\n\n\n(Other) :13889\n(Other) : 179\n\n\n\n\n\n\n\n\n(j) Table continues below\n\n\n\n\n\n\nMARSUPWT\nMIGMTR1\nMIGMTR3\n\n\n\nMin. : 37.87\nNonmover :82538\nNonmover :82538\n\n\n1st Qu.: 1061.62\nMSA to MSA :10601\nSame county : 9812\n\n\nMedian : 1618.31\nNonMSA to nonMSA: 2811\nDifferent county same state: 2797\n\n\nMean : 1740.38\nNot in universe : 1516\nNot in universe : 1516\n\n\n3rd Qu.: 2188.61\nMSA to nonMSA : 790\nDifferent region : 1178\n\n\nMax. :18656.30\n(Other) : 1571\n(Other) : 1986\n\n\nNA\nNA’s :99696\nNA’s :99696\n\n\n\n\n\n\n\n\n(k) Table continues below\n\n\n\n\n\nMIGMTR4\nMIGSAME\n\n\n\nNonmover :82538\nNo : 15773\n\n\nSame county : 9812\nNot in universe under 1 year old:101212\n\n\nDifferent county same state: 2797\nYes : 82538\n\n\nNot in universe : 1516\nNA\n\n\nDifferent state in South : 973\nNA\n\n\n(Other) : 2191\nNA\n\n\nNA’s :99696\nNA\n\n\n\n\n\n\n\n\n(l) Table continues below\n\n\n\n\n\n\nMIGSUN\nNOEMP\nPARENT\n\n\n\nNo : 9987\nMin. :0.000\nBoth parents present : 38983\n\n\nNot in universe:84054\n1st Qu.:0.000\nFather only present : 1883\n\n\nYes : 5786\nMedian :1.000\nMother only present : 12772\n\n\nNA’s :99696\nMean :1.956\nNeither parent present: 1653\n\n\nNA\n3rd Qu.:4.000\nNot in universe :144232\n\n\nNA\nMax. :6.000\nNA\n\n\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n(m) Table continues below\n\n\n\n\n\n\nPEFNTVTY\nPEMNTVTY\nPENATVTY\n\n\n\nUnited-States:159163\nUnited-States:160479\nUnited-States:176989\n\n\nMexico : 10008\nMexico : 9781\nMexico : 5767\n\n\nPuerto-Rico : 2680\nPuerto-Rico : 2473\nPuerto-Rico : 1400\n\n\nItaly : 2212\nItaly : 1844\nGermany : 851\n\n\nCanada : 1380\nCanada : 1451\nPhilippines : 845\n\n\n(Other) : 17367\n(Other) : 17376\n(Other) : 10278\n\n\nNA’s : 6713\nNA’s : 6119\nNA’s : 3393\n\n\n\n\n\n\n\n\n(n) Table continues below\n\n\n\n\n\n\nPRCITSHP\nSEOTR\nVETQVA\n\n\n\nForeign born- Not a citizen of U S : 13401\nMin. :0.0000\nNo : 1593\n\n\nForeign born- U S citizen by naturalization: 5855\n1st Qu.:0.0000\nNot in universe:197539\n\n\nNative- Born abroad of American Parent(s) : 1756\nMedian :0.0000\nYes : 391\n\n\nNative- Born in Puerto Rico or U S Outlying: 1519\nMean :0.1754\nNA\n\n\nNative- Born in the United States :176992\n3rd Qu.:0.0000\nNA\n\n\nNA\nMax. :2.0000\nNA\n\n\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVETYN\nWKSWORK\nYEAR\nY\n\n\n\nMin. :0.000\nMin. : 0.00\nMin. :94.0\n- 50000.:187141\n\n\n1st Qu.:2.000\n1st Qu.: 0.00\n1st Qu.:94.0\n50000+. : 12382\n\n\nMedian :2.000\nMedian : 8.00\nMedian :94.0\nNA\n\n\nMean :1.515\nMean :23.17\nMean :94.5\nNA\n\n\n3rd Qu.:2.000\n3rd Qu.:52.00\n3rd Qu.:95.0\nNA\n\n\nMax. :2.000\nMax. :52.00\nMax. :95.0\nNA\n\n\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAs mentioned above, a lot of missing values occurred in this dataset, especially for the attributes MIGMTR1, MIGMTR3, MIGMTR4 and MIGSUN."
  },
  {
    "objectID": "dataiku_jjoumaa.html#data-visualizations",
    "href": "dataiku_jjoumaa.html#data-visualizations",
    "title": "Dataiku Interview Project",
    "section": "\n2.2 Data visualizations",
    "text": "2.2 Data visualizations\nAnother way to have an idea of the occurrence of missing values is to used data visualization.\n\nCode# build dataPlot\ndataPlot <- setDT(melt(is.na(dataset)))\n\n# subsample\ndataPlot <- dataPlot[, .SD[sample(nrow(dataset), 1000)], by = Var2] %>%\n  .[, `:=`(id_row, c(1:.N)), by = c(\"Var2\")]\n\n# missing values\nggplot(dataPlot, aes(x = Var2, y = id_row)) +\n  geom_tile(aes(fill = value)) +\n  labs(x = \"Attributes\", y = \"Rows\") +\n  scale_fill_manual(\n    values = c(\"white\", \"black\"),\n    labels = c(\"Real\", \"Missing\")\n  ) +\n  theme(\n    legend.position = \"top\",\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.key = element_rect(colour = \"black\")\n  )\n\n\n\nFigure 1: Representation of missing values within the learning dataset\n\n\n\n\nIt also helps for the representation of numerical attributes.\n\nCode# numerical attributes index\nncolInt <- which(sapply(dataset, class) == \"integer\")\n\n# plot for each numerical attributes\nggplot(\n  melt(dataset[, .SD, .SDcols = c(ncolInt, ncol(dataset))], id.vars = \"Y\"),\n  aes(x = value, col = Y, fill = Y)\n) +\n  geom_histogram(aes(y = ..density..),\n    bins = 10,\n    fill = \"white\"\n  ) +\n  geom_density(alpha = .2) +\n  theme(legend.position = \"top\") +\n  facet_wrap(~variable, scales = \"free\")\n\n\n\nFigure 2: Histogram and density for each of the numerical attributes.\n\n\n\n\nNothing particularly clear here, except that some attributes do not seem to contain a lot of information (AHRSPAY, CAPGAIN, CAPLOSS, DIVVAL, SEOTR, VETYN, WKSWORK or YEAR). We can also note the attribute “AAGE” has a Gaussian-like distribution a bit shifted, especially for Y = \"- 50000., which may require to be transformed. Let’s have a look at another representation of this data.\n\nCodeggplot(\n  melt(dataset[, .SD, .SDcols = c(ncolInt, ncol(dataset))], id.vars = \"Y\"),\n  aes(x = \"\", y = value, col = Y, fill = Y)\n) +\n  geom_boxplot(alpha = .2) +\n  theme(\n    legend.position = \"top\",\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  ) +\n  facet_wrap(~variable, scales = \"free\")\n\n\n\nFigure 3: Boxplot for each of the numerical attributes.\n\n\n\n\n\nCode# nominal attributes names\nncolFac <- names(which(sapply(dataset, class) == \"factor\"))\n\n# plot for each nominal attributes\ndataPlot <- lapply(ncolFac[-c(length(ncolFac))], function(x) {\n  A <- dataset[, levels(get(x)), by = Y]\n  B <- dataset[, table(get(x)), by = Y] %>%\n    .[, `:=`(\n      V1 = as.integer(V1),\n      levels = as.factor(A$V1)\n    )]\n  ggplot(B, aes(x = as.factor(levels), y = V1, fill = Y)) +\n    geom_bar(\n      stat = \"identity\",\n      position = position_dodge()\n    ) +\n    labs(y = \"Count\", x = \"\", title = x) +\n    theme(\n      legend.justification = c(0, 1),\n      legend.position = c(0, 1),\n      axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n})\n\n# print of the two first plot\ngrid.arrange(grobs = list(dataPlot[[1]], dataPlot[[2]]), ncol = 2)\n\n\n\n\n\n\nFigure 4: Barplot for each of the nominal attributes.\n\n\n\n\n\nCode# another way to it, which appears not working using Rmarkdown document\n# grid.arrange(grobs=dataPlot, ncol=2)"
  },
  {
    "objectID": "dataiku_jjoumaa.html#dataCleaning",
    "href": "dataiku_jjoumaa.html#dataCleaning",
    "title": "Dataiku Interview Project",
    "section": "\n3.1 Data Cleaning",
    "text": "3.1 Data Cleaning\nWhen looking at Figure 1, it appears that a lot of missing values occurred within the learning dataset. Here, I’ve taken the liberty to “simply” removed these values. Another way to deal with missing values would have been to use an imputation method, such as the K-nearest neighbors method 1 provided by the package caret (code commented below). Because this method requires high computational costs, I’ve preferred to stick to the first method, even if, as we’ll see in the next steps, it means the final model won’t be able to predict Y in some cases.\n\nCode# removal of missing values\ndatasetNa <- na.omit(dataset)\n\n## or we could have done something like this:\n# preProcValues = preProcess(dataset, method = c(\"knnImpute\"))\n# dataset.imp = predict(preProcValues, dataset)"
  },
  {
    "objectID": "dataiku_jjoumaa.html#feature-selection",
    "href": "dataiku_jjoumaa.html#feature-selection",
    "title": "Dataiku Interview Project",
    "section": "\n3.2 Feature Selection",
    "text": "3.2 Feature Selection\nAs a first step to feature selection, I’ve decided to remove any attributes with a zero or near zero variance. For many models, this may cause the model to crash or the fit to be unstable.\n\nCode# identification of near zero variance attributes\nnzv <- nearZeroVar(datasetNa, saveMetrics = TRUE)\ndatatable(nzv[nzv$nzv, ],\n  caption = \"Information relative to zero and near zero variance predictors.\",\n  option = list(dom = \"t\")\n)\n\n\n\n\n\nCode# removal of these predictors\ndatasetNaNzv <- datasetNa[, .SD, .SDcols = !nzv$nzv]\n\n\nThe second step was to remove any attributes with a correlation higher than 0.75. This is a basic method, which could be improved by combining correlogram with significance test.\n\nCode# matrix correlation\nncolInt <- which(sapply(datasetNaNzv, class) == \"integer\")\ndescrCor <- cor(datasetNaNzv[, .SD, .SDcols = ncolInt])\n\n# which attributes will be removed based on matrix correlation\ncorrplot(descrCor, method = \"pie\")\nhighlyCorDescr <- findCorrelation(descrCor, cutoff = .75)\nnames(ncolInt[highlyCorDescr])\n\n[1] \"WKSWORK\"\n\nCode# attribute \"WKSWORK\" removed\ndatasetNaNzv[, (names(ncolInt[highlyCorDescr])) := NULL]\n\n\n\nFigure 5: Correlogram of dataset.\n\n\n\n\nHere, I’ve only removed the attribute “WKSWORK”. At the end of the “Feature selection” step, the current dataset includes 30 attributes instead of 42 in the original one."
  },
  {
    "objectID": "dataiku_jjoumaa.html#data-transforms",
    "href": "dataiku_jjoumaa.html#data-transforms",
    "title": "Dataiku Interview Project",
    "section": "\n3.3 Data Transforms",
    "text": "3.3 Data Transforms\n\n3.3.1 Yeo-Johnson transform\nI’ve used the Yeo-Johnson transform, since the attributes “AAGE”, i.e the age of the people in this dataset, has a Gaussian-like distribution with a skew (Figure 2). To make it “more Gaussian”, I’ve simply performed a YeoJohnson transform; I would have normally performed a BoxCox transform, but it does not support raw values that are equal to zero.\n\nCode# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(datasetNaNzv[, \"AAGE\"], method = c(\"YeoJohnson\"))\n\n# transform the dataset using the parameters\ndatasetNaNzv$AAGE <- predict(preprocessParams, datasetNaNzv[, \"AAGE\"])\n\n\n\n3.3.2 One-Hot encoding\nAs we’ll see in the next steps, one-hot encoding was not really necessary for both algorithms tested hereafter. However, it may be a good way to improve computational costs for many algorithms that do not perform well, when dealing with nominal attributes.\n\nCode# one-hot encoding\ndummies <- dummyVars(Y ~ ., data = datasetNaNzv)\nmatDummies <- predict(dummies, newdata = datasetNaNzv)\n\n\nAs a consequence, some columns might appear to be a linear combination of others. To ensure the non-redundancy of information, I’ve removed these rows.\n\nCode# remove linear combination\ncomboInfo <- findLinearCombos(matDummies)\ndatasetNaNzvQr <- as.data.table(matDummies[, -comboInfo$remove])\ndatasetNaNzvQr[, Y := datasetNaNzv[, Y]]"
  },
  {
    "objectID": "dataiku_jjoumaa.html#split-out-validation-dataset",
    "href": "dataiku_jjoumaa.html#split-out-validation-dataset",
    "title": "Dataiku Interview Project",
    "section": "\n3.4 Split-out validation dataset",
    "text": "3.4 Split-out validation dataset\nI know you provided a “test file”, but the learning dataset is large enough to be split in a learning dataset and a validation dataset by itself. Moreover, my computer is not powerful enough to run models on the whole learning dataset, so I’ve split the learning dataset with 10% of the data for the training process and the other 90% for testing (with a powerful enough computers, I would split the dataset using 80% of the data for the learning step and the other 20% for the evaluation). In the last step, I’ll evaluate the chosen model on the provided “test file”.\n\nCode# identification of indexes for splitting\nset.seed(7)\ntrainIndex <- createDataPartition(datasetNaNzvQr$Y,\n  p = .1,\n  list = FALSE,\n  times = 1\n)\n\n# split\ndatasetNaNzvQr[, Y := as.factor(make.names(Y))]\ndataTrain <- datasetNaNzvQr[trainIndex, ]\ndataTest <- datasetNaNzvQr[-trainIndex, ]"
  },
  {
    "objectID": "dataiku_jjoumaa.html#test-options-and-evaluation-metric",
    "href": "dataiku_jjoumaa.html#test-options-and-evaluation-metric",
    "title": "Dataiku Interview Project",
    "section": "\n4.1 Test options and evaluation metric",
    "text": "4.1 Test options and evaluation metric\nHere I’ve chosen to cross-validate the model using 5 folds repeated twice. As explained before, with a powerful enough computer I would encourage increasing these values. Because the problem to solve is a classification problem, the metric used to select the optimal model is the “Accuracy”, except for the “Stacking” part, where the area under the ROC curve was used.\n\nCode# cross-validation\nset.seed(7)\ntrainControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  repeats = 2,\n  savePredictions = \"final\", # required for stacking\n  classProbs = TRUE\n) # required for stacking\n\n# metric evaluation\nmetric <- \"Accuracy\""
  },
  {
    "objectID": "dataiku_jjoumaa.html#spot-check-algorithms",
    "href": "dataiku_jjoumaa.html#spot-check-algorithms",
    "title": "Dataiku Interview Project",
    "section": "\n4.2 Spot Check Algorithms",
    "text": "4.2 Spot Check Algorithms\nBecause the dataset is strongly imbalance…\n\nCode# proportion of each levels of Y\ndatatable(\n  data.table(\n    freq = table(datasetNaNzvQr$Y),\n    percentage = round(prop.table(table(datasetNaNzvQr$Y)) * 100, 2)\n  ),\n  option = list(dom = \"t\")\n)\n\n\n\n\n\n\n… it can have significant negative impact on model fitting (by the way, it seems consistent that a large proportion of these people earn less than 50 000$ pear annum). Here I’ve explored two different ways to deal with it:\n\n\ndown-sampling: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class.\n\nup-sampling: randomly sample (with replacement) the minority class to be the same size as the majority class.\n\nIn addition, I’ve tested two different algorithms, the Logistic Regression, and k-Nearest Neighbors.\n\nCode# down-sampling\ntrainControl$sampling <- \"down\"\n\n# logistic regression\nset.seed(7)\nfitLogDown <- train(Y ~ .,\n  data = dataTrain,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = metric,\n  trControl = trainControl\n)\n\n# KNN\nset.seed(7)\nfitKnnDown <- train(Y ~ .,\n  data = dataTrain,\n  method = \"knn\",\n  metric = metric,\n  trControl = trainControl\n)\n\n# up-sampling\ntrainControl$sampling <- \"up\"\n\n# logistic regression\nset.seed(7)\nfitLogUp <- train(Y ~ .,\n  data = dataTrain,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = metric,\n  trControl = trainControl\n)\n\n# KNN\nset.seed(7)\nfitKnnUp <- train(Y ~ .,\n  data = dataTrain,\n  method = \"knn\",\n  metric = metric,\n  trControl = trainControl\n)"
  },
  {
    "objectID": "dataiku_jjoumaa.html#CompareAlgorithm",
    "href": "dataiku_jjoumaa.html#CompareAlgorithm",
    "title": "Dataiku Interview Project",
    "section": "\n4.3 Compare Algorithms",
    "text": "4.3 Compare Algorithms\n\nCode# resample\nresults <- resamples(list(\n  KNN_UP = fitKnnUp,\n  LOG_UP = fitLogUp,\n  KNN_DOWN = fitKnnDown,\n  LOG_DOWN = fitLogDown\n))\n\n# summary\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: KNN_UP, LOG_UP, KNN_DOWN, LOG_DOWN \nNumber of resamples: 10 \n\nAccuracy \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nKNN_UP   0.7861272 0.7933030 0.8021026 0.7996639 0.8063584 0.8113505    0\nLOG_UP   0.3608193 0.8272871 0.8399895 0.7953308 0.8436472 0.8949028    0\nKNN_DOWN 0.6006306 0.6115127 0.6203388 0.6213483 0.6330793 0.6405675    0\nLOG_DOWN 0.6263794 0.7792086 0.7866527 0.7728101 0.7977917 0.8128286    0\n\nKappa \n               Min.    1st Qu.     Median       Mean   3rd Qu.       Max. NA's\nKNN_UP   0.12070882 0.13707161 0.14558179 0.14449985 0.1528234 0.16022824    0\nLOG_UP   0.04003155 0.26565309 0.29853090 0.27619801 0.3223886 0.34684118    0\nKNN_DOWN 0.06257470 0.06846597 0.07426959 0.07606744 0.0841786 0.09147037    0\nLOG_DOWN 0.08160797 0.21929935 0.22963676 0.22159830 0.2422348 0.27955925    0\n\n\nIt appears the up-sampling method performs better than the down-sampling. In addition, the k-Nearest Neighbors algorithm provides better results than the Logistic Regression one. Let’s now compare both algorithms using the up-sampling method with a dataset including nominal attributes (i.e. not using one-hot encoding).\n\nCode# new data split based on the dataset without hot encoding\ndatasetNaNzv[, Y := as.factor(make.names(Y))]\ndataTrainVar <- datasetNaNzv[trainIndex, ]\ndataTestVar <- datasetNaNzv[-trainIndex, ]\n\n# logistic regression\nset.seed(7)\nfitLogUpVar <- train(Y ~ .,\n  data = dataTrainVar,\n  method = \"glm\",\n  family = \"binomial\",\n  metric = metric,\n  trControl = trainControl\n)\n\n# KNN\nset.seed(7)\nfitKnnUpVar <- train(Y ~ .,\n  data = dataTrainVar,\n  method = \"knn\",\n  metric = metric,\n  trControl = trainControl\n)\n\n# resample\nresults <- resamples(list(\n  KNN_UP = fitKnnUp,\n  LOG_UP = fitLogUp,\n  KNN_DOWN = fitKnnDown,\n  LOG_DOWN = fitLogDown,\n  KNN_UP_VAR = fitKnnUpVar,\n  LOG_UP_VAR = fitLogUpVar\n))\n\n# summary\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: KNN_UP, LOG_UP, KNN_DOWN, LOG_DOWN, KNN_UP_VAR, LOG_UP_VAR \nNumber of resamples: 10 \n\nAccuracy \n                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nKNN_UP     0.7861272 0.7933030 0.8021026 0.7996639 0.8063584 0.8113505    0\nLOG_UP     0.3608193 0.8272871 0.8399895 0.7953308 0.8436472 0.8949028    0\nKNN_DOWN   0.6006306 0.6115127 0.6203388 0.6213483 0.6330793 0.6405675    0\nLOG_DOWN   0.6263794 0.7792086 0.7866527 0.7728101 0.7977917 0.8128286    0\nKNN_UP_VAR 0.7854890 0.7935655 0.8015769 0.7989281 0.8043878 0.8108250    0\nLOG_UP_VAR 0.1514196 0.7926957 0.8392013 0.7488078 0.8494482 0.8533123    0\n\nKappa \n                  Min.    1st Qu.     Median       Mean   3rd Qu.       Max.\nKNN_UP      0.12070882 0.13707161 0.14558179 0.14449985 0.1528234 0.16022824\nLOG_UP      0.04003155 0.26565309 0.29853090 0.27619801 0.3223886 0.34684118\nKNN_DOWN    0.06257470 0.06846597 0.07426959 0.07606744 0.0841786 0.09147037\nLOG_DOWN    0.08160797 0.21929935 0.22963676 0.22159830 0.2422348 0.27955925\nKNN_UP_VAR  0.12022934 0.13740228 0.14592863 0.14439255 0.1535522 0.15962758\nLOG_UP_VAR -0.05243764 0.24262988 0.27824470 0.22688123 0.3000925 0.33037603\n           NA's\nKNN_UP        0\nLOG_UP        0\nKNN_DOWN      0\nLOG_DOWN      0\nKNN_UP_VAR    0\nLOG_UP_VAR    0\n\n\n\nCodedotplot(results)\n\n\n\nFigure 6: Models comparison.\n\n\n\n\nUsing direct nominal attributes instead of one-hot encoding provides better results (i.e. higher accuracy) for the Logistic Regression, which is not the case for k-Nearest Neighbors algorithm, which provides the same accuracy."
  },
  {
    "objectID": "dataiku_jjoumaa.html#algorithm-tuning",
    "href": "dataiku_jjoumaa.html#algorithm-tuning",
    "title": "Dataiku Interview Project",
    "section": "\n5.1 Algorithm Tuning",
    "text": "5.1 Algorithm Tuning\nDepending on the algorithm to chose, we can improve the accuracy by choosing the appropriate set of parameters. Below, you’ll find a recipe to find the optimal number of neighbors to set when using a k-Nearest Neighbors algorithm. Due to high computational cost, I did not run this code.\n\nCode# search for the optimal number of neighbors K (did not run)\nset.seed(7)\ngrid <- expand.grid(.k = seq(1, 20, by = 1))\nfitKnnUpVar <- train(Y ~ .,\n  data = dataTrainVar,\n  method = \"knn\",\n  metric = metric,\n  tuneGrid = grid,\n  trControl = trainControl\n)\n\n# tuning kNN parameter\nplot(fitKnnUpVar)"
  },
  {
    "objectID": "dataiku_jjoumaa.html#ensembles",
    "href": "dataiku_jjoumaa.html#ensembles",
    "title": "Dataiku Interview Project",
    "section": "\n5.2 Ensembles",
    "text": "5.2 Ensembles\n\n5.2.1 Stacking\nAnother way to improve the accuracy is to combine the predictions of several models into ensemble predictions. To do this, we first have to make sure predictions from sub-models have a low correlation.\n\nCode# correlation of models pairs of predictions\ncorrplot(modelCor(results), method = \"pie\")\n\n\n\nFigure 7: Correlogram of the predictions by models.\n\n\n\n\nHere, that seems to be the case, since no one has a high correlation (i.e > 0.75). I’ve decided to use both algorithms using nominal attributes and the up-sampling method to build a meta-model (multiFit).\n\nCode# cross-validation\nstackControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  repeats = 2,\n  savePredictions = \"final\",\n  classProbs = TRUE\n)\n\n# list of models\nset.seed(7)\nmultiModels <- caretList(Y ~ .,\n  data = dataTrainVar,\n  methodList = c(\"glm\", \"knn\"),\n  tuneList = list(\n    glm = caretModelSpec(\n      method = \"glm\",\n      family = \"binomial\"\n    ),\n    knn = caretModelSpec(method = \"knn\")\n  ),\n  trControl = trainControl\n)\n\n# stacking\nset.seed(7)\nmultiFits <- caretEnsemble(multiModels,\n  trControl = stackControl\n)\nprint(multiFits)\n\nA glm ensemble of 4 base models: glm, knn, glm.1, knn.1\n\nEnsemble results:\nGeneralized Linear Model \n\n19028 samples\n    4 predictor\n    2 classes: 'X..50000.', 'X50000..' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 2 times) \nSummary of sample sizes: 15223, 15221, 15223, 15223, 15222, 15223, ... \nResampling results:\n\n  Accuracy   Kappa\n  0.9424007  0    \n\n\nThe accuracy seems to be better than both algorithms alone (Log: 0.75 and kNN: 0.8, 0.75, 0.71, whereas stacking: 0.94). However, the Kappa value of 0 is symptomatic of a problem that we’re going to clarify when looking at the predictions.\n\n5.2.2 Boosting\nAnother way to used ensemble predictions is to use an algorithm based on boosting method. The idea here is to build multiple models where each of which learns to fix the prediction errors of a prior model in the chain. Let’s test one of the most popular boosting machine learning algorithms, the C5.0 classification based on a set of rules.\n\nCode# C5.0 classification\nset.seed(7)\nfitC5UpVar <- train(Y ~ .,\n  data = dataTrainVar,\n  method = \"C5.0Rules\",\n  metric = metric,\n  trControl = trainControl\n)\nprint(fitC5UpVar)\n\nSingle C5.0 Ruleset \n\n9514 samples\n  28 predictor\n   2 classes: 'X..50000.', 'X50000..' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 2 times) \nSummary of sample sizes: 7611, 7612, 7611, 7612, 7610, 7611, ... \nAddtional sampling using up-sampling\n\nResampling results:\n\n  Accuracy   Kappa    \n  0.9191195  0.3461387"
  },
  {
    "objectID": "dataiku_jjoumaa.html#prediction",
    "href": "dataiku_jjoumaa.html#prediction",
    "title": "Dataiku Interview Project",
    "section": "\n6.1 Predictions on validation dataset",
    "text": "6.1 Predictions on validation dataset\nHere comes the part where I’ve tested the two ensemble models (stacking and boosting) predictions on your validation dataset. But first, I have to pre-process this dataset to make sure both models will correctly run.\n\nCode# remove missing value\nvalidationNa <- na.omit(validation)\n\n# keep the same attributes used in the learning dataset\nvalidationNa <- validationNa[, .SD, .SDcols = colnames(dataTrainVar)]\n\n# suitable name for R (i.e. the same as the learning dataset)\nvalidationNa[, Y := make.names(Y)]\n\n# Yeo-Johnson transform of the attribute AAGE\nvalidationNa$AAGE <- predict(preprocessParams, validationNa[, \"AAGE\"])\n\n# convert characters to factors\nncolCha <- names(which(sapply(validationNa, class) == \"character\"))\nvalidationNa[, (ncolCha) := lapply(.SD, as.factor), .SDcols = ncolCha]\n\n\nBecause I removed a lot of rows during the data cleaning step, it removed some levels of nominal attributes in the learning dataset. This implies that neither model can make prediction for this levels. To avoid errors when predicting on the validation dataset, I’ve removed any rows that do not match levels of the training dataset.\n\nCode# only keep levels of factors in the validation datatset that match those in the learning one\nfor (i in which(dataTrainVar[, lapply(.SD, class), .SDcols = -\"Y\"] == \"factor\")) {\n  x <- validationNa[, lapply(.SD, unique), .SDcols = i]\n  x <- as.character(x[[1]])\n  y <- dataTrainVar[, lapply(.SD, unique), .SDcols = i]\n  y <- as.character(y[[1]])\n  if (length(setdiff(x, y)) > 0) {\n    delNrow <- as.numeric()\n    for (j in 1:length(setdiff(x, y))) {\n      delNrow <- c(delNrow, which(validationNa[, i, with = F] == setdiff(x, y)[j]))\n    }\n    validationNa <- validationNa[-delNrow, ]\n  }\n}\n\n\nNow that we have a validation dataset that matches the same column names and levels of nominal attributes from the learning one, we can make predictions and compare them to the real values.\n\nCode# make prediction\npredictionsStack <- predict(multiFits,\n  validationNa[, .SD, .SDcols = -\"Y\"],\n  type = \"raw\"\n)\n\npredictionsC5 <- predict(fitC5UpVar,\n  validationNa[, .SD, .SDcols = -\"Y\"],\n  type = \"raw\"\n)\n\n# confusion matrix calculation\nconfStack <- confusionMatrix(predictionsStack, validationNa[, Y])\nconfC5 <- confusionMatrix(predictionsC5, validationNa[, Y])\nconfStack\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  X..50000. X50000..\n  X..50000.     44700     2683\n  X50000..          0        0\n                                          \n               Accuracy : 0.9434          \n                 95% CI : (0.9413, 0.9454)\n    No Information Rate : 0.9434          \n    P-Value [Acc > NIR] : 0.5051          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9434          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9434          \n         Detection Rate : 0.9434          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : X..50000.       \n                                          \n\nCodeconfC5\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  X..50000. X50000..\n  X..50000.     42426     1441\n  X50000..       2274     1242\n                                         \n               Accuracy : 0.9216         \n                 95% CI : (0.9191, 0.924)\n    No Information Rate : 0.9434         \n    P-Value [Acc > NIR] : 1              \n                                         \n                  Kappa : 0.3596         \n                                         \n Mcnemar's Test P-Value : <2e-16         \n                                         \n            Sensitivity : 0.9491         \n            Specificity : 0.4629         \n         Pos Pred Value : 0.9672         \n         Neg Pred Value : 0.3532         \n             Prevalence : 0.9434         \n         Detection Rate : 0.8954         \n   Detection Prevalence : 0.9258         \n      Balanced Accuracy : 0.7060         \n                                         \n       'Positive' Class : X..50000.      \n                                         \n\n\nWe can see that the stacking model (i.e Log + kNN) has, indeed, an accuracy of 0.94 which is higher than the boosting model (i.e C5.0) with “only” 0.92. However, the confusion matrix shows the stacking model simply predicts the same value (the one from the prevalent class), which results in a misleading accuracy (i.e accuracy = proportion of the majority class).\n\nCode# accuracy & proportion of the majority class\naccuracyStack <- round(confStack$overall[1], 2) # accuracy of stack model\npropMajorClass <- round(max(prop.table(table(validationNa$Y))), 2) # prop of prevalent class\n\n# accuracy = proportion of the majority class?\naccuracyStack == propMajorClass\n\nAccuracy \n    TRUE"
  },
  {
    "objectID": "dataiku_jjoumaa.html#conclusion",
    "href": "dataiku_jjoumaa.html#conclusion",
    "title": "Dataiku Interview Project",
    "section": "\n6.2 Conclusion",
    "text": "6.2 Conclusion\n\n6.2.1 Some insights\nBased on the calculation of variable importance in the fitC5UpVar model, the three variables that, in order of importance, are the most important when estimating if the income level will be more than 50 000$ per annum seems to be:\n\nthe tax filer status (levels: Nonfiler)\nthe age\nthe number of persons that worked for employer\n\n\nCode# variable importance for the fitC5UpVar model\nvarImp(fitC5UpVar, scale = 100)\n\nC5.0Rules variable importance\n\n  only 20 most important variables shown (out of 307)\n\n                                       Overall\nFILESTATNonfiler                        100.00\nMARSUPWT                                 41.39\nNOEMP                                    40.76\nAAGE                                     39.66\nASEXMale                                 39.10\nADTOCC                                   26.51\nACLSWKRSelf-employed-incorporated        25.32\nAMJINDMedicalexcepthospital              23.53\nAHGAMastersdegree(MAMSMEngMEdMSWMBA)     23.32\nAHGASomecollegebutnodegree               20.93\nAHGAHighschoolgraduate                   20.49\nAMJOCCSales                              19.13\nHHDFMXSpouseofhouseholder                16.87\nHHDFMXChild18+nevermarrNotinasubfamily   16.74\nACLSWKRNotinuniverse                     14.07\nAMJOCCProtectiveservices                 13.38\nAMARITLSeparated                         13.29\nAHGAAssociatesdegree-occup/vocational    12.90\nADTIND                                   10.77\nMIGMTR4DifferentstateinWest              10.52\n\nCode# some plot to illustrate\ngrid.arrange(\n  ggplot(dataset, aes(x = FILESTAT, fill = Y)) +\n    geom_bar(position = position_dodge()) +\n    theme(legend.position = \"top\"),\n  ggplot(dataset, aes(x = AAGE, fill = Y)) +\n    geom_bar() +\n    theme(legend.position = \"none\"),\n  ggplot(dataset, aes(x = NOEMP, fill = Y)) +\n    geom_bar(position = position_dodge()) +\n    theme(legend.position = \"none\"),\n  nrow = 3, heights = c(2, 1.5, 1.5)\n)\n\n\n\nFigure 8: Distribution and barplot of the three most important variables when predicting the income level of the person represented by the record.\n\n\n\n\n\n6.2.2 Go further\nHere are few ideas to improve this analysis:\n\nData Preparation\n\nData cleaning\n\nTry to impute missing value using k nearest neighbors or bagged tree algorithm. It should allow the selected model to predict the income level for all people, regardless of the different levels of a factor in any nominal attributes.\nLook closer for outliers and maybe try some methods like PCA to remove noises.\n\n\nData transform\n\nTry normalized numerical variables.\nConsider Box-Cox transform on other numerical attributes.\n\n\nData split\n\nSimply use the whole learning dataset and not 10 % of it.\n\n\n\n\nEvaluate algorithms\n\nSpot Check Algorithm: test more linear and non-linear algorithms.\n\n\nImprove accuracy\n\nEnsembles\n\nFor boosting, it’s the same, let’s take a look at several boosting machine learning algorithms which seems to be very promising.\nFor stacking, same as above, test different combinations of models, and then combine predictions using several different algorithms (i.e not just a simple linear model, like I did here)."
  },
  {
    "objectID": "test_jjoumaa.html",
    "href": "test_jjoumaa.html",
    "title": "Dataiku Interview Project",
    "section": "",
    "text": "Please find below my report about the dataset provided by Dataiku. This analysis has been realized using R (version 3.3.3) with all packages updated on 12/02/2023, under a UNIX/LINUX environment (Debian 9.2). The provided script (code_jjoumaa.R) runs in about an hour with an Intel i7 6600U based computer equipped with 32 Gb RAM (DDR4)."
  },
  {
    "objectID": "test_jjoumaa.html#load-libraries",
    "href": "test_jjoumaa.html#load-libraries",
    "title": "Dataiku Interview Project",
    "section": "\n1.1 Load libraries",
    "text": "1.1 Load libraries\nMost of the data manipulation was done using data.table package due to its ability to handle large datasets. Data visualization was mostly done using ggplot2 package. For the modelling part, I’ve used caret package, which provides a lot of useful tools for data science.\n\nCode# data visualization\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(gridExtra)\n# data manipulation\nlibrary(data.table)\nlibrary(stringr)\n# data modeling\nlibrary(caret)\nlibrary(caretEnsemble)\nlibrary(RANN)\n# markdown table\nlibrary(knitr)\nlibrary(DT)\nlibrary(pander)\n# (optional) multithreading\nlibrary(doMC)\nregisterDoMC(cores = 4)"
  },
  {
    "objectID": "test_jjoumaa.html#load-dataset",
    "href": "test_jjoumaa.html#load-dataset",
    "title": "Dataiku Interview Project",
    "section": "\n1.2 Load dataset",
    "text": "1.2 Load dataset\nAs we’ll see in the next steps, both datasets (learning and validation one) present a lot of missing values in the form of ? in raw files, converted then in NA values in R.\n\nCode# learning dataset\ndataset = fread(\"census_income_learn.csv\", \n                na.strings = \"?\")\n# validation dataset\nvalidation = fread(\"census_income_test.csv\", \n                   na.strings = \"?\")\n\n\nSince neither dataset has column names, I’ve loaded the description file census_income_metadata.txt, and extracted rows mentioning column names.\n\nCode# selection of the right rows containing colnames\ndatasetNames = fread(\"census_income_metadata.txt\", \n                     nrows = (68-22), \n                     skip = 22, \n                     drop = \"V1\")\n# extraction of word in capital\ncolInter = datasetNames[, unlist(str_extract_all(V2, '\\\\b[A-Z]+[A-Z0-9]\\\\b'))]\ncolInter\n\n [1] \"AAGE\"     \"ACLSWKR\"  \"ADTIND\"   \"ADTOCC\"   \"AGI\"      \"AHGA\"    \n [7] \"AHRSPAY\"  \"AHSCOL\"   \"AMARITL\"  \"AMJIND\"   \"AMJOCC\"   \"ARACE\"   \n[13] \"AREORGN\"  \"ASEX\"     \"AUNMEM\"   \"AUNTYPE\"  \"AWKSTAT\"  \"CAPGAIN\" \n[19] \"CAPLOSS\"  \"DIVVAL\"   \"FEDTAX\"   \"FILESTAT\" \"GRINREG\"  \"GRINST\"  \n[25] \"HHDFMX\"   \"HHDREL\"   \"MARSUPWT\" \"MIGMTR1\"  \"MIGMTR3\"  \"MIGMTR4\" \n[31] \"MIGSAME\"  \"MIGSUN\"   \"NOEMP\"    \"PARENT\"   \"PEARNVAL\" \"PEFNTVTY\"\n[37] \"PEMNTVTY\" \"PENATVTY\" \"PRCITSHP\" \"PTOTVAL\"  \"SEOTR\"    \"TAXINC\"  \n[43] \"VETQVA\"   \"VETYN\"    \"WKSWORK\" \n\n\nThe problem was that 45 columns were mentioned in the description files (colInter), whereas both datasets only have 42 columns. To solve this, I’ve matched each column with their respective names based on the number of unique “value” by columns and the one exposed in the description file. Using this method, I’ve found that AGI, FEDTAX, PEARNVAL, PTOTVAL and TAXINC attributes could be removed from the initial column names, and that the attributes YEARS and Y (the attributes to model, i.e the income level) should be added.\n\nCode# number of instances for each attributes\ndataset[, sapply(.SD, function(x){length(unique(x))})]\n\n   V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12   V13 \n   91     9    52    47    17  1240     3     7    24    15     5    10     2 \n  V14   V15   V16   V17   V18   V19   V20   V21   V22   V23   V24   V25   V26 \n    3     6     8   132   113  1478     6     6    51    38     8 99800    10 \n  V27   V28   V29   V30   V31   V32   V33   V34   V35   V36   V37   V38   V39 \n    9    10     3     4     7     5    43    43    43     5     3     3     3 \n  V40   V41   V42 \n   53     2     2 \n\nCode# remove unexpected column names\ncolInter = colInter[-c(which(colInter == \"AGI\"))]\ncolInter = colInter[-c(which(colInter == \"FEDTAX\"))]\ncolInter = colInter[-c(which(colInter == \"PEARNVAL\"))]\ncolInter = colInter[-c(which(colInter == \"PTOTVAL\"))]\ncolInter = colInter[-c(which(colInter == \"TAXINC\"))]\n\n# add two more column names\ncolInter = c(colInter, c(\"YEAR\", \"Y\"))\ncolnames(dataset) = colInter[1:42]\ncolnames(validation) = colInter[1:42]\n\n# dataset preview\ndatatable(head(validation, 5), \n          caption = \"Preview of the dataset\", \n          extensions = 'FixedColumns', \n          options = list(\n            dom = \"t\", \n            scrollX = TRUE, \n            fixedColumns = FALSE))"
  },
  {
    "objectID": "test_jjoumaa.html#descriptive-statistics",
    "href": "test_jjoumaa.html#descriptive-statistics",
    "title": "Dataiku Interview Project",
    "section": "\n2.1 Descriptive statistics",
    "text": "2.1 Descriptive statistics\nIt is difficult to present all attributes, but a quick summary provides a good overview of the structure.\n\nCode# index of column with characters\nncolCha = which(sapply(dataset, class) == \"character\")\n\n# conversion of characters in factors\ndataset[, (ncolCha):=lapply(.SD, as.factor), .SDcols = ncolCha]\n\n# structure of the dataset\nkable(data.frame(variable = names(dataset), \n                 classe = sapply(dataset, class), \n                 first_values = sapply(dataset, function(x) {\n                   paste0(head(x), collapse = \", \")}), \n                 row.names = NULL))\n\n\n\n\n\n\n\n\nvariable\nclasse\nfirst_values\n\n\n\nAAGE\ninteger\n73, 58, 18, 9, 10, 48\n\n\nACLSWKR\nfactor\nNot in universe, Self-employed-not incorporated, Not in universe, Not in universe, Not in universe, Private\n\n\nADTIND\ninteger\n0, 4, 0, 0, 0, 40\n\n\nADTOCC\ninteger\n0, 34, 0, 0, 0, 10\n\n\nAHGA\nfactor\nHigh school graduate, Some college but no degree, 10th grade, Children, Children, Some college but no degree\n\n\nAHRSPAY\ninteger\n0, 0, 0, 0, 0, 1200\n\n\nAHSCOL\nfactor\nNot in universe, Not in universe, High school, Not in universe, Not in universe, Not in universe\n\n\nAMARITL\nfactor\nWidowed, Divorced, Never married, Never married, Never married, Married-civilian spouse present\n\n\nAMJIND\nfactor\nNot in universe or children, Construction, Not in universe or children, Not in universe or children, Not in universe or children, Entertainment\n\n\nAMJOCC\nfactor\nNot in universe, Precision production craft & repair, Not in universe, Not in universe, Not in universe, Professional specialty\n\n\nARACE\nfactor\nWhite, White, Asian or Pacific Islander, White, White, Amer Indian Aleut or Eskimo\n\n\nAREORGN\nfactor\nAll other, All other, All other, All other, All other, All other\n\n\nASEX\nfactor\nFemale, Male, Female, Female, Female, Female\n\n\nAUNMEM\nfactor\nNot in universe, Not in universe, Not in universe, Not in universe, Not in universe, No\n\n\nAUNTYPE\nfactor\nNot in universe, Not in universe, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nAWKSTAT\nfactor\nNot in labor force, Children or Armed Forces, Not in labor force, Children or Armed Forces, Children or Armed Forces, Full-time schedules\n\n\nCAPGAIN\ninteger\n0, 0, 0, 0, 0, 0\n\n\nCAPLOSS\ninteger\n0, 0, 0, 0, 0, 0\n\n\nDIVVAL\ninteger\n0, 0, 0, 0, 0, 0\n\n\nFILESTAT\nfactor\nNonfiler, Head of household, Nonfiler, Nonfiler, Nonfiler, Joint both under 65\n\n\nGRINREG\nfactor\nNot in universe, South, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nGRINST\nfactor\nNot in universe, Arkansas, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nHHDFMX\nfactor\nOther Rel 18+ ever marr not in subfamily, Householder, Child 18+ never marr Not in a subfamily, Child <18 never marr not in subfamily, Child <18 never marr not in subfamily, Spouse of householder\n\n\nHHDREL\nfactor\nOther relative of householder, Householder, Child 18 or older, Child under 18 never married, Child under 18 never married, Spouse of householder\n\n\nMARSUPWT\nnumeric\n1700.09, 1053.55, 991.95, 1758.14, 1069.16, 162.61\n\n\nMIGMTR1\nfactor\nNA, MSA to MSA, NA, Nonmover, Nonmover, NA\n\n\nMIGMTR3\nfactor\nNA, Same county, NA, Nonmover, Nonmover, NA\n\n\nMIGMTR4\nfactor\nNA, Same county, NA, Nonmover, Nonmover, NA\n\n\nMIGSAME\nfactor\nNot in universe under 1 year old, No, Not in universe under 1 year old, Yes, Yes, Not in universe under 1 year old\n\n\nMIGSUN\nfactor\nNA, Yes, NA, Not in universe, Not in universe, NA\n\n\nNOEMP\ninteger\n0, 1, 0, 0, 0, 1\n\n\nPARENT\nfactor\nNot in universe, Not in universe, Not in universe, Both parents present, Both parents present, Not in universe\n\n\nPEFNTVTY\nfactor\nUnited-States, United-States, Vietnam, United-States, United-States, Philippines\n\n\nPEMNTVTY\nfactor\nUnited-States, United-States, Vietnam, United-States, United-States, United-States\n\n\nPENATVTY\nfactor\nUnited-States, United-States, Vietnam, United-States, United-States, United-States\n\n\nPRCITSHP\nfactor\nNative- Born in the United States, Native- Born in the United States, Foreign born- Not a citizen of U S, Native- Born in the United States, Native- Born in the United States, Native- Born in the United States\n\n\nSEOTR\ninteger\n0, 0, 0, 0, 0, 2\n\n\nVETQVA\nfactor\nNot in universe, Not in universe, Not in universe, Not in universe, Not in universe, Not in universe\n\n\nVETYN\ninteger\n2, 2, 2, 0, 0, 2\n\n\nWKSWORK\ninteger\n0, 52, 0, 0, 0, 52\n\n\nYEAR\ninteger\n95, 94, 95, 94, 94, 95\n\n\nY\nfactor\n- 50000., - 50000., - 50000., - 50000., - 50000., - 50000.\n\n\n\n\nCode# summary of the dataset\npander(summary(dataset))\n\n\nTable continues below\n\n\n\n\n\n\n\nAAGE\nACLSWKR\nADTIND\nADTOCC\n\n\n\nMin. : 0.00\nNot in universe :100245\nMin. : 0.00\nMin. : 0.00\n\n\n1st Qu.:15.00\nPrivate : 72028\n1st Qu.: 0.00\n1st Qu.: 0.00\n\n\nMedian :33.00\nSelf-employed-not incorporated: 8445\nMedian : 0.00\nMedian : 0.00\n\n\nMean :34.49\nLocal government : 7784\nMean :15.35\nMean :11.31\n\n\n3rd Qu.:50.00\nState government : 4227\n3rd Qu.:33.00\n3rd Qu.:26.00\n\n\nMax. :90.00\nSelf-employed-incorporated : 3265\nMax. :51.00\nMax. :46.00\n\n\nNA\n(Other) : 3529\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\nAHGA\nAHRSPAY\nAHSCOL\n\n\n\nHigh school graduate :48407\nMin. : 0.00\nCollege or university: 5688\n\n\nChildren :47422\n1st Qu.: 0.00\nHigh school : 6892\n\n\nSome college but no degree:27820\nMedian : 0.00\nNot in universe :186943\n\n\nBachelors degree(BA AB BS):19865\nMean : 55.43\nNA\n\n\n7th and 8th grade : 8007\n3rd Qu.: 0.00\nNA\n\n\n10th grade : 7557\nMax. :9999.00\nNA\n\n\n(Other) :40445\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\nAMARITL\nAMJIND\n\n\n\nDivorced :12710\nNot in universe or children :100684\n\n\nMarried-A F spouse present : 665\nRetail trade : 17070\n\n\nMarried-civilian spouse present:84222\nManufacturing-durable goods : 9015\n\n\nMarried-spouse absent : 1518\nEducation : 8283\n\n\nNever married :86485\nManufacturing-nondurable goods : 6897\n\n\nSeparated : 3460\nFinance insurance and real estate: 6145\n\n\nWidowed :10463\n(Other) : 51429\n\n\n\n\nTable continues below\n\n\n\n\n\nAMJOCC\nARACE\n\n\n\nNot in universe :100684\nAmer Indian Aleut or Eskimo: 2251\n\n\nAdm support including clerical: 14837\nAsian or Pacific Islander : 5835\n\n\nProfessional specialty : 13940\nBlack : 20415\n\n\nExecutive admin and managerial: 12495\nOther : 3657\n\n\nOther service : 12099\nWhite :167365\n\n\nSales : 11783\nNA\n\n\n(Other) : 33685\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\nAREORGN\nASEX\nAUNMEM\n\n\n\nAll other :171907\nFemale:103984\nNo : 16034\n\n\nMexican-American : 8079\nMale : 95539\nNot in universe:180459\n\n\nMexican (Mexicano) : 7234\nNA\nYes : 3030\n\n\nCentral or South American: 3895\nNA\nNA\n\n\nPuerto Rican : 3313\nNA\nNA\n\n\nOther Spanish : 2485\nNA\nNA\n\n\n(Other) : 2610\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\nAUNTYPE\nAWKSTAT\nCAPGAIN\n\n\n\nJob leaver : 598\nChildren or Armed Forces :123769\nMin. : 0.0\n\n\nJob loser - on layoff: 976\nFull-time schedules : 40736\n1st Qu.: 0.0\n\n\nNew entrant : 439\nNot in labor force : 26808\nMedian : 0.0\n\n\nNot in universe :193453\nPT for non-econ reasons usually FT: 3322\nMean : 434.7\n\n\nOther job loser : 2038\nUnemployed full-time : 2311\n3rd Qu.: 0.0\n\n\nRe-entrant : 2019\nPT for econ reasons usually PT : 1209\nMax. :99999.0\n\n\nNA\n(Other) : 1368\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\nCAPLOSS\nDIVVAL\nFILESTAT\n\n\n\nMin. : 0.00\nMin. : 0.0\nHead of household : 7426\n\n\n1st Qu.: 0.00\n1st Qu.: 0.0\nJoint both 65+ : 8332\n\n\nMedian : 0.00\nMedian : 0.0\nJoint both under 65 :67383\n\n\nMean : 37.31\nMean : 197.5\nJoint one under 65 & one 65+: 3867\n\n\n3rd Qu.: 0.00\n3rd Qu.: 0.0\nNonfiler :75094\n\n\nMax. :4608.00\nMax. :99999.0\nSingle :37421\n\n\nNA\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\nGRINREG\nGRINST\n\n\n\nAbroad : 530\nNot in universe:183750\n\n\nMidwest : 3575\nCalifornia : 1714\n\n\nNortheast : 2705\nUtah : 1063\n\n\nNot in universe:183750\nFlorida : 849\n\n\nSouth : 4889\nNorth Carolina : 812\n\n\nWest : 4074\n(Other) : 10627\n\n\nNA\nNA’s : 708\n\n\n\n\nTable continues below\n\n\n\n\n\nHHDFMX\nHHDREL\n\n\n\nHouseholder :53248\nHouseholder :75475\n\n\nChild <18 never marr not in subfamily :50326\nChild under 18 never married :50426\n\n\nSpouse of householder :41695\nSpouse of householder :41709\n\n\nNonfamily householder :22213\nChild 18 or older :14430\n\n\nChild 18+ never marr Not in a subfamily:12030\nOther relative of householder: 9703\n\n\nSecondary individual : 6122\nNonrelative of householder : 7601\n\n\n(Other) :13889\n(Other) : 179\n\n\n\n\nTable continues below\n\n\n\n\n\n\nMARSUPWT\nMIGMTR1\nMIGMTR3\n\n\n\nMin. : 37.87\nNonmover :82538\nNonmover :82538\n\n\n1st Qu.: 1061.62\nMSA to MSA :10601\nSame county : 9812\n\n\nMedian : 1618.31\nNonMSA to nonMSA: 2811\nDifferent county same state: 2797\n\n\nMean : 1740.38\nNot in universe : 1516\nNot in universe : 1516\n\n\n3rd Qu.: 2188.61\nMSA to nonMSA : 790\nDifferent region : 1178\n\n\nMax. :18656.30\n(Other) : 1571\n(Other) : 1986\n\n\nNA\nNA’s :99696\nNA’s :99696\n\n\n\n\nTable continues below\n\n\n\n\n\nMIGMTR4\nMIGSAME\n\n\n\nNonmover :82538\nNo : 15773\n\n\nSame county : 9812\nNot in universe under 1 year old:101212\n\n\nDifferent county same state: 2797\nYes : 82538\n\n\nNot in universe : 1516\nNA\n\n\nDifferent state in South : 973\nNA\n\n\n(Other) : 2191\nNA\n\n\nNA’s :99696\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\nMIGSUN\nNOEMP\nPARENT\n\n\n\nNo : 9987\nMin. :0.000\nBoth parents present : 38983\n\n\nNot in universe:84054\n1st Qu.:0.000\nFather only present : 1883\n\n\nYes : 5786\nMedian :1.000\nMother only present : 12772\n\n\nNA’s :99696\nMean :1.956\nNeither parent present: 1653\n\n\nNA\n3rd Qu.:4.000\nNot in universe :144232\n\n\nNA\nMax. :6.000\nNA\n\n\nNA\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\nPEFNTVTY\nPEMNTVTY\nPENATVTY\n\n\n\nUnited-States:159163\nUnited-States:160479\nUnited-States:176989\n\n\nMexico : 10008\nMexico : 9781\nMexico : 5767\n\n\nPuerto-Rico : 2680\nPuerto-Rico : 2473\nPuerto-Rico : 1400\n\n\nItaly : 2212\nItaly : 1844\nGermany : 851\n\n\nCanada : 1380\nCanada : 1451\nPhilippines : 845\n\n\n(Other) : 17367\n(Other) : 17376\n(Other) : 10278\n\n\nNA’s : 6713\nNA’s : 6119\nNA’s : 3393\n\n\n\n\nTable continues below\n\n\n\n\n\n\nPRCITSHP\nSEOTR\nVETQVA\n\n\n\nForeign born- Not a citizen of U S : 13401\nMin. :0.0000\nNo : 1593\n\n\nForeign born- U S citizen by naturalization: 5855\n1st Qu.:0.0000\nNot in universe:197539\n\n\nNative- Born abroad of American Parent(s) : 1756\nMedian :0.0000\nYes : 391\n\n\nNative- Born in Puerto Rico or U S Outlying: 1519\nMean :0.1754\nNA\n\n\nNative- Born in the United States :176992\n3rd Qu.:0.0000\nNA\n\n\nNA\nMax. :2.0000\nNA\n\n\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nVETYN\nWKSWORK\nYEAR\nY\n\n\n\nMin. :0.000\nMin. : 0.00\nMin. :94.0\n- 50000.:187141\n\n\n1st Qu.:2.000\n1st Qu.: 0.00\n1st Qu.:94.0\n50000+. : 12382\n\n\nMedian :2.000\nMedian : 8.00\nMedian :94.0\nNA\n\n\nMean :1.515\nMean :23.17\nMean :94.5\nNA\n\n\n3rd Qu.:2.000\n3rd Qu.:52.00\n3rd Qu.:95.0\nNA\n\n\nMax. :2.000\nMax. :52.00\nMax. :95.0\nNA\n\n\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "saegus_jjoumaa.html",
    "href": "saegus_jjoumaa.html",
    "title": "Saegus Interview Project",
    "section": "",
    "text": "Many American cities have communal bike sharing stations where you can rent bicycles by the hour or day. Washington, D.C. is one of these cities. The District collects detailed data on the number of bicycles people rent by the hour and day.\nHadi Fanaee-T at the University of Porto compiled this data into a CSV file, which you’ll be working with in this project. The file contains 17380 rows, with each row representing the number of bike rentals for a single hour of a single day. You can download the data from the University of California, Irvine’s website.\nHere’s what the first five rows look like:\n\nCode# packages\nlibrary(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(doMC)\nlibrary(randomForest)\nlibrary(corrplot)\nlibrary(plotly)\n\n# loading file\ndataset <- fread(\"./saegus_data/bike_rental_hour.csv\")\n\n# print first rows\nhead(dataset)\n\n\n\n  \n\n\n\nHere are the descriptions for the relevant columns:\n\n\ninstant - A unique sequential ID number for each row\n\ndteday - The date of the rentals\n\nseason - The season in which the rentals occurred\n\nyr - The year the rentals occurred\n\nmnth - The month the rentals occurred\n\nhr - The hour the rentals occurred\n\nholiday - Whether or not the day was a holiday\n\nweekday - The day of the week (as a number, 0 to 7)\n\nworkingday - Whether or not the day was a working day\n\nweathersit - The weather (as a categorical variable)\n\ntemp - The temperature, on a 0-1 scale\n\natemp - The adjusted temperature\n\nhum - The humidity, on a 0-1 scale\n\nwindspeed - The wind speed, on a 0-1 scale\n\ncasual - The number of casual riders (people who hadn’t previously signed up with the bike sharing program)\n\nregistered - The number of registered riders (people who had already signed up)\n\ncnt - The total number of bike rentals (casual + registered)\n\nLet’s say your customer want to predict the total number of bikes people rented in a given hour (cnt column ).\nIn this project you’ll have to provide a clear and meaningfull data analysis using data story tellings approach and statistical technics.\nHave Fun!"
  },
  {
    "objectID": "saegus_jjoumaa.html#a-simple-one",
    "href": "saegus_jjoumaa.html#a-simple-one",
    "title": "Saegus Interview Project",
    "section": "\n3.1 A Simple One",
    "text": "3.1 A Simple One\nIn this scenario, I’ve only got few minutes to answer the question. So I provide a simple graphical way to answer the question.\n\nCode# a simple answer\nggplotly(ggplot(dataset, aes(x = hr, y = cnt)) +\n  geom_boxplot() +\n  geom_smooth(\n    data = dataset[, .(cnt = median(cnt)), by = hr],\n    aes(y = cnt, x = as.numeric(hr)),\n    se = TRUE,\n    span = 0.3\n  ))\n\n\nFigure 2: Evolution of rental bike count accross an average day\n\n\n\nThis graph is interesting, since it highlights the distribution of the rental bike count accross an “average” day. The problem here, is that we simply aggregate the data, without considering other information, such as the variation induced by weather condition or type of day (working day or holiday) on the rental bike count. That’s the reason why, if I’ve got enough time, I’ll definitely support the second scenario."
  },
  {
    "objectID": "saegus_jjoumaa.html#without-weather-conditions",
    "href": "saegus_jjoumaa.html#without-weather-conditions",
    "title": "Saegus Interview Project",
    "section": "\n3.2 Without Weather Conditions",
    "text": "3.2 Without Weather Conditions\nThis scenario requires that I have enough time to perform modelling using all information provided in the dataset, except those related to weather conditions (the numerical ones). Why? Simply because in the real world, if my customer wants to know the rental bike count at noon tomorrow, he also has to have relevant information about the weather conditions tomorrow at noon which, as everyone knows, might be quite tricky.\nThat is the reason why for this part, I’ve focused on building a predictive model of the rental bike count, only based on non-numerical-weather-condition variables.\n\n3.2.1 Overview of Selected Variables\nHere, for each variable I’ve performed a non-parametric statistical test, to highlight if there is difference in rental bike count between the modalities of the considered categorial variable:\n\n\nWilcox Test: to compare two modalities\n\nkruskal-Wallis Test: to compare more than two modalities\n\n\nCode# but first, build plots\npairsVarCat <- lapply(colnames(dataset)[3:10], function(x) {\n  ggpairs(dataset, columns = c(\"cnt\", x), cardinality_threshold = 24)\n})\n\n\n\n\nSeason\nYear\nMonth\nHour\nHoliday\nWeekday\nWorking Day\nWeathersit\n\n\n\n\nCode# statistical test\nkruskal.test(cnt ~ season, data = dataset)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by season\nKruskal-Wallis chi-squared = 1190.3, df = 3, p-value < 2.2e-16\n\n\nThe result is strongly significant, which means there is at least one season for which the median of the rental bike count is different from the others.\n\nCode# draw plot\npairsVarCat[[1]]\n\n\n\nFigure 3: Season\n\n\n\n\n\n\n\nCode# statistical test\nwilcox.test(cnt ~ yr, data = dataset)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  cnt by yr\nW = 28707154, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe result is strongly significant, which means there is a difference in the distribution of rental bike count between both years.\n\nCode# draw plot\npairsVarCat[[2]]\n\n\n\nFigure 4: Year\n\n\n\n\n\n\n\nCode# statistical test\nkruskal.test(cnt ~ mnth, data = dataset)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by mnth\nKruskal-Wallis chi-squared = 1290.5, df = 11, p-value < 2.2e-16\n\n\nThe result is strongly significant, which means there is at least one month for which the median of the rental bike count is different from the others.\n\nCode# draw plot\npairsVarCat[[3]]\n\n\n\nFigure 5: Month\n\n\n\n\n\n\n\nCode# statistical test\nkruskal.test(cnt ~ hr, data = dataset)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by hr\nKruskal-Wallis chi-squared = 10973, df = 23, p-value < 2.2e-16\n\n\nThe result is strongly significant, which means there is at least one hour for which the median of the rental bike count is different from the others.\n\nCode# draw plot\npairsVarCat[[4]]\n\n\n\nFigure 6: Hour\n\n\n\n\n\n\n\nCode# statistical test\nwilcox.test(cnt ~ holiday, data = dataset)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  cnt by holiday\nW = 4650072, p-value = 9.93e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe result is strongly significant, which means there is a difference in the distribution of rental bike count whether the customer is in holiday or not.\n\nCode# draw plot\npairsVarCat[[5]]\n\n\n\nFigure 7: Holiday\n\n\n\n\n\n\n\nCode# statistical test\nkruskal.test(cnt ~ weekday, data = dataset)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by weekday\nKruskal-Wallis chi-squared = 24.584, df = 6, p-value = 0.0004076\n\n\nThe result is still significant (i.e. p-value < 0.05), but less than other variables. That means that it seems there is at least one weekday for which the median of the rental bike count is different from the others.\n\nCode# draw plot\npairsVarCat[[6]]\n\n\n\nFigure 8: Weekday\n\n\n\n\n\n\n\nCode# statistical test\nwilcox.test(cnt ~ workingday, data = dataset)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  cnt by workingday\nW = 31858297, p-value = 0.005558\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe result is still significant (i.e. p-value < 0.05), but less than other variables. That means that it seems there is a difference in the distribution of the rental bike count between a working day or not.\n\nCode# draw plot\npairsVarCat[[7]]\n\n\n\nFigure 9: Working day\n\n\n\n\n\n\n\nCode# statistical test\nkruskal.test(cnt ~ weathersit, data = dataset)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by weathersit\nKruskal-Wallis chi-squared = 397.03, df = 3, p-value < 2.2e-16\n\n\nThe result is strongly significant, which means there is at least one weather condition for which the median of the rental bike count is different from the others.\n\nCode# draw plot\npairsVarCat[[8]]\n\n\n\nFigure 10: Weathersit\n\n\n\n\n\n\n\n\n3.2.2 Machine Learning Part\nIn this part, I’ve only chosen to present a random forest, but it is advised to test several other algorithms. To build this model, I’ve used the caret package which provides relevant tools when building a whole pipeline.\n\nCode# format data\ndataset[, \":=\"(season = as.numeric(season),\n  holiday = as.numeric(holiday),\n  mnth = as.numeric(mnth),\n  hr = as.numeric(hr),\n  yr = as.numeric(yr),\n  weekday = as.numeric(weekday),\n  workingday = as.numeric(workingday),\n  weathersit = as.numeric(weathersit),\n  cnt = as.numeric(cnt))]\n\n# train set - repeated cross validation 5-fold\ncontrol <- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  repeats = 2,\n  savePredictions = \"final\"\n)\n\n# to speed-up calculations\nregisterDoMC(cores = 4)\n\n# performance metric\nmetrics <- \"RMSE\"\n\n# fix the seed for reproducible results\nset.seed(123)\n\n# 10% random indices due to calculatio, but I would advice 80%\ntrainIndex <- createDataPartition(dataset$cnt,\n  p = 0.1,\n  list = FALSE\n)\n\n# training dataset\ndataTrain1 <- dataset[\n  trainIndex,\n  -c(\"instant\", \"dteday\", \"casual\", \"registered\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n]\n\n# test dataset\ndataTest1 <- dataset[\n  -trainIndex,\n  -c(\"instant\", \"dteday\", \"casual\", \"registered\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n]\n\n# model\nmdl.rf1 <- train(cnt ~ .,\n  data = dataTrain1,\n  method = \"rf\",\n  metric = metrics,\n  trControl = control,\n  importance = TRUE\n)\n\n\nOne way to evaluate the model is to look at the plot Predictions vs Real Values:\n\nCode# make prediction\npredictionsRf1 <- predict(mdl.rf1,\n  dataTest1,\n  type = \"raw\"\n)\n\n# plot against real value\nggplot(\n  data.table(\n    Predictions = predictionsRf1,\n    `Real values` = dataTest1$cnt\n  ),\n  aes(x = `Real values`, y = Predictions)\n) +\n  geom_point() +\n  geom_path(\n    data = data.table(\n      x = c(0, max(dataTest1$cnt)),\n      y = c(0, max(dataTest1$cnt))\n    ),\n    aes(x = x, y = y), col = \"blue\"\n  )\n\n\n\nFigure 11: Model evaluation\n\n\n\n\nThe results are not so bad. This model tends to over-estimate the rental bike count for small values and under-estimate for high values."
  },
  {
    "objectID": "saegus_jjoumaa.html#with-weather-conditions",
    "href": "saegus_jjoumaa.html#with-weather-conditions",
    "title": "Saegus Interview Project",
    "section": "\n3.3 With Weather Conditions",
    "text": "3.3 With Weather Conditions\nIn this scenario, I’ve assumed the customer has an absolute knowledge of everything (especially the weather condition for the day and the hour he wants to predict the rental bike count). So, I’ve developed a model that includes variables related to weather conditions.\nAs information on weather conditions is represented by numerical variables, I’ve drawn a correlogram to identify if there are variables correlated with one another.\n\nCode# identification weather conditions variables\ncolWea <- which(colnames(dataset) %in% c(\"temp\", \"atemp\", \"hum\", \"windspeed\"))\n\n# correlation calculation\ndescrCor <- cor(dataset[, .SD, .SDcols = colWea])\n\n# correlogram\ncorrplot(descrCor, method = \"pie\")\n\n\n\nFigure 12: Correlogram\n\n\n\n\nHere, we can see that temp and atemp are two strongly correlated variables. That means they carry almost the same information, which is why, to avoid redundant information for the Machine Learning Part, we must remove one of both variables. Here I choose to remove the temp variable since it is not adjusted. It’s a subjective choice, but I’ve supposed if atemp is the adjusted temperature, then it must have more information than simply the temperature alone. One way to choose more objectively between both variables, would be to compare the predictions of two models, one with temp and the other with atemp, and to keep the variable associated with the model having the best predictions.\nNow I’ll present the numerical variables that we’re going to add to the previous model.\n\n3.3.1 Overview of Selected Weather-Conditions Variables\nHere, for each weather condition variable, I’ve drawn a multi plot that allows to investigate the correlation associated with the cnt variable.\n\nCode# but first, build plots\npairsVarNum <- lapply(c(\"atemp\", \"hum\", \"windspeed\"), function(x) {\n  ggpairs(dataset,\n    columns = c(\"cnt\", x),\n    lower = list(\n      continuous = wrap(\"smooth\",\n        colour = \"grey50\",\n        size = 0.1\n      ),\n      combo = \"facetdensity\"\n    ),\n    upper = list(combo = wrap(\"box_no_facet\",\n      outlier.shape = NA\n    ))\n  )\n})\n\n\n\n\nAdjusted Temperature\nHumidity\nWindspeed\n\n\n\n\nCodepairsVarNum[[1]]\n\n\n\nFigure 13: Adjusted temperature\n\n\n\n\nThis suggests an increase of the rental bike count with the increasing of the adjusted temperature.\n\n\n\nCodepairsVarNum[[2]]\n\n\n\nFigure 14: Humidity\n\n\n\n\nThis suggests a decrease of the rental bike count with the increasing of the humidity.\n\n\n\nCodepairsVarNum[[3]]\n\n\n\nFigure 15: Windspeed\n\n\n\n\nThis suggests there is almost to linear relationship between rental bike count and windspeed.\n\n\n\n\n3.3.2 Machine Learning Part\nIt’s almost the same process seen previously, except than we added atemp, hum and windspeed to the variables of the model.\n\nCode# training dataset\ndataTrain2 <- dataset[\n  trainIndex,\n  -c(\"instant\", \"dteday\", \"casual\", \"registered\", \"temp\")\n]\n\n# test dataset\ndataTest2 <- dataset[\n  -trainIndex,\n  -c(\"instant\", \"dteday\", \"casual\", \"registered\", \"temp\")\n]\n\n# model\nmdl.rf2 <- train(cnt ~ .,\n  data = dataTrain2,\n  method = \"rf\",\n  metric = metrics,\n  trControl = control,\n  importance = TRUE\n)\n\n\nThe same way we did previously, we can check the plot Predictions vs Real Values.\n\nCode# make prediction\npredictionsRf2 <- predict(mdl.rf2,\n  dataTest2,\n  type = \"raw\"\n)\n\n# plot against real value\nggplot(\n  data.table(\n    Predictions = predictionsRf2,\n    `Real values` = dataTest2$cnt\n  ),\n  aes(x = `Real values`, y = Predictions)\n) +\n  geom_point() +\n  geom_path(\n    data = data.table(\n      x = c(0, max(dataTest2$cnt)),\n      y = c(0, max(dataTest2$cnt))\n    ),\n    aes(x = x, y = y), col = \"blue\"\n  )\n\n\n\nFigure 16: Prediction vs. Real Values\n\n\n\n\nThe result is quite the same as shown previously. Predictions are not so bad, but the model tends to over-estimate the rental bike count for small values and under-estimate for high values."
  },
  {
    "objectID": "saegus_jjoumaa.html#model-selection",
    "href": "saegus_jjoumaa.html#model-selection",
    "title": "Saegus Interview Project",
    "section": "\n3.4 Model Selection",
    "text": "3.4 Model Selection\nSince we build two random forest models, we then need to choose which one to use to make prediction on the rental bike counts. First, we can check the performance metric (i.e. RMSE, for Root Mean Square Error, here) during the cross validation process.\n\nCode# resample results\nresults <- resamples(list(\n  RF1 = mdl.rf1,\n  RF2 = mdl.rf2\n))\n\n# data wrangling\nresults <- data.table(melt(results$values, id.vars = \"Resample\"))\nresults[, c(\"mdl\", \"crit\") := tstrsplit(variable,\n  \"~\",\n  fixed = T,\n  type.convert = T\n)]\n\n# plot\nggplot(results[, \":=\"(mdl = as.factor(mdl),\n  crit = as.factor(crit))]) +\n  geom_boxplot(aes(y = value, x = reorder(mdl, value))) +\n  coord_flip() +\n  facet_wrap(~crit, scales = \"free\") +\n  labs(y = \"Valeurs\", x = \"Modèles\")\n\n\n\nFigure 17: Resample\n\n\n\n\nHere, the difference between both models doesn’t seem to help choosing one model over the other. Since we build both models using a training dataset, we can use the test dataset which have not been used during the pipeline, to evaluate the predictions of each model, and finally choose the best one.\n\nCode# RF1\nprint(\"Results for the model with only categorial variables\")\n\n[1] \"Results for the model with only categorial variables\"\n\nCodepostResample(pred = predictionsRf1, obs = dataTest1$cnt)\n\n      RMSE   Rsquared        MAE \n60.3190376  0.8897067 38.0028393 \n\nCode# RF2\nprint(\"Results for the model with almost all variables\")\n\n[1] \"Results for the model with almost all variables\"\n\nCodepostResample(pred = predictionsRf2, obs = dataTest2$cnt)\n\n      RMSE   Rsquared        MAE \n60.4817774  0.8896874 38.9893355 \n\n\nThese results suggest the Random Forest based only on categorial variables performed better than the other model including weather condition information."
  },
  {
    "objectID": "saegus_jjoumaa.html#go-further",
    "href": "saegus_jjoumaa.html#go-further",
    "title": "Saegus Interview Project",
    "section": "\n3.5 Go Further",
    "text": "3.5 Go Further\n\n\nData Preparation\n\nData Transform: Try normalized numerical variables\nData Split: Increase the proportion of dataset used during the training part\n\n\n\nData Modelling\n\nModel Selection: I would definitely encourage to test several algorithms\nStacking Method: Combine predictions from several model\n\n\n\nModel Deployment: If I had more time, I would have provided a small shiny application, allowing the customer to have prediction for a given set of inputs"
  },
  {
    "objectID": "booking_jjoumaa.html",
    "href": "booking_jjoumaa.html",
    "title": "Booking Interview Project",
    "section": "",
    "text": "Codelibrary(data.table)\nlibrary(ggplot2)\nlibrary(nlme)\nlibrary(knitr)\nlibrary(magrittr)\nlibrary(pander)\nlibrary(itsadug)\nlibrary(mgcv)\nlibrary(gridExtra)\n# avoid scientific notation\noptions(scipen = 999)"
  },
  {
    "objectID": "booking_jjoumaa.html#statement",
    "href": "booking_jjoumaa.html#statement",
    "title": "Booking Interview Project",
    "section": "\n3.1 Statement",
    "text": "3.1 Statement\n“Dear Reporting Team,\nI am account manager for France. I would like to get some insights in our supply in regards to the football championship that will be held in France this year. I am especially interested to see if we are opening enough new properties. Can you please share your findings on our supply and highlight your outtakes?\nThanks in advance.”"
  },
  {
    "objectID": "booking_jjoumaa.html#data-mining",
    "href": "booking_jjoumaa.html#data-mining",
    "title": "Booking Interview Project",
    "section": "\n3.2 Data mining",
    "text": "3.2 Data mining\n\n3.2.1 Few comments\n\n\n\nThe dataset (data) is considered as a snapshot of 10.000 properties randomly selected in 10 cities: Paris, Napoli, Florence, Cannes, Roma, Nice, Palermo, Milano, Venice, Lyon, during April 2016. That means we do not know what proportion these 10.000 properties represent in each cities. In addition, this dataset do not include any information on the estimation of the number of people that will attend the football championship this summer, neither the proportion of these people that is expected to book a reservation via Booking.com, nor the number of properties already booked through Booking.com. Considering these, we need to:\n\nHave an idea of the proportion these 10.000 properties represent of the Booking.com’s offer in each city;\nEstimate the number of people that will attend the football championship.\n\nWe also need to make several asumption:\n\nConsidering the football championship will be held in France, we will only keep french cities in our analysis. In addition, in the present dataset, only Paris, Lyon and Nice have a stadium that will be used for the competition (UEFA2016 - Wikipedia), so we will focus on these cities thereafter;\nSomeone that will attend the football championship is defined here as someone with a ticket for a match (and not someone that will only go to the Fan zones);\n\n3.2.2 Analysis & Results\n\n3.2.2.1 Number of rooms available\nBased on a snapshot of the website Booking.com dated of April 1st, 2016 (thanks to web.archive.org), we know that Paris has 3981 properties, Nice 856, and Lyon 396 (Table 2).\n\nCode# subset with the relevant cities\ndata.uefa <- data[city %in% c(\"Paris\", \"Nice\", \"Lyon\"), ]\n# nb of properties in dataset per city\ndata.uefa.cities <- data.uefa[, .(\n  nb_rooms_dataset = sum(nr_rooms),\n  nb_properties_dataset = .N\n), by = city]\n# add nb of properties available\ndata.uefa.cities[, nb_properties_available := c(3981, 856, 396)]\n# proportion of properties in the dataset\ndata.uefa.cities[, prop_properties_dataset := round((100 / nb_properties_available) * nb_properties_dataset, 2)]\n# nb of rooms available based on the proportion of properties in the dataset\ndata.uefa.cities[, nb_rooms_available := floor((nb_rooms_dataset * 100) / prop_properties_dataset)]\nkable(data.uefa.cities)\n\n\n\nTable 2: Summary of properties available based on the provided dataset.\n\n\n\n\n\n\n\n\n\ncity\nnb_rooms_dataset\nnb_properties_dataset\nnb_properties_available\nprop_properties_dataset\nnb_rooms_available\n\n\n\nParis\n7411\n2009\n3981\n50.46\n14686\n\n\nNice\n869\n580\n856\n67.76\n1282\n\n\nLyon\n553\n188\n396\n47.47\n1164\n\n\n\n\n\n\n\n\n\nThis means the dataset data represents 50.46 % of the properties in Paris, 67.76 % of the properties in Nice and 47.47 % of the properties in Lyon (considering these proportions, the 10.000 properties within this dataset were probably not randomly selected after all…). Based on the number of rooms available in the dataset and these proportions, we estimated the total number of rooms available for:\n\nParis of 14,686;\nNice of 1,282;\nLyon of 1,164.\n\n3.2.2.2 Number of attendees per city\nI based my estimation of the number of attendees (Figure 1) on the capacity of each stadium in each cities, multiplied by the number of matches, based on the Wikipedia page (UEFA-2016).\n\nParis with 5 matches at the Parc des Princes (capacity: 48712) and 7 matches at the Stade de France (capacity: 81338) => \\(5 \\times 48712 + 7 \\times 81338 = 812926\\);\nNice with 4 matches at the Stade de Nice (capacity: 35624) => \\(4 \\times 35624 = 142496\\);\nLyon with 3 matches at the Parc Olympique Lyonnais (capacity: 59186) => \\(3 \\times 59186 = 177558\\).\n\nFor Paris, I also considered to include the Stade de France which is located in Saint-Denis, closed to Paris. As specified in the Few comments section, this basically means that I made the following assumption:\n\n1 ticket = 1 attendee\n\n\nCodeggplot(data.table(\n  City = c(\"Paris\", \"Nice\", \"Lyon\"),\n  Attendees = c(812926, 142496, 177558)\n), aes(City, Attendees)) +\n  geom_col() +\n  labs(y = \"Expected number of attendees\")\n\n\n\nFigure 1: Expected number of attendees per city based on the capacity of each stadium.\n\n\n\n\n\n3.2.2.3 Number of rooms vs. number of attendees\nConsidering that Booking.com is able to offer a maximum of 14,686, 1,282 and 1,164 rooms, for respectively Paris, Nice and Lyon, a simple difference (Figure 2) shows that Booking.com needs to increase significantly the number of rooms available, and so of properties, to reach the demand and welcome all the attendees of the football championship.\n\nCodeggplot(\n  data.table(\n    City = rep(c(\"Paris\", \"Nice\", \"Lyon\"), 2),\n    Attendees = c(\n      812926 - data.uefa.cities[city == \"Paris\", nb_rooms_available],\n      142496 - data.uefa.cities[city == \"Nice\", nb_rooms_available],\n      177558 - data.uefa.cities[city == \"Lyon\", nb_rooms_available],\n      data.uefa.cities[city == \"Paris\", nb_rooms_available],\n      data.uefa.cities[city == \"Nice\", nb_rooms_available],\n      data.uefa.cities[city == \"Lyon\", nb_rooms_available]\n    ),\n    Group = c(\"Other\", \"Other\", \"Other\", \"Booking\", \"Booking\", \"Booking\")\n  ),\n  aes(City, Attendees)\n) +\n  geom_col(aes(fill = Group)) +\n  guides(fill = guide_legend(\n    title.position = \"top\",\n    title = \"accommodation\"\n  )) +\n  labs(y = \"Expected number of attendees\") +\n  theme(legend.position = \"top\")\n\n\n\nFigure 2: Histogram of the number of attendees per city with a colour code for the capacity of Booking.com to provide them a room.\n\n\n\n\nFortunately for Booking.com, this graph does not take into consideration two features:\n\nThe real proportion of attendees behind our estimation of the number of attendees, e.g an attendee could buy two tickets, meaning that our estimation of the number of attendees is over-estimated;\nNot all the estimated attendees will book a room though Booking.com, in other words what is the market share of Booking.com among those who will book a room through internet.\n\nConsidering this, I propose an image plot (Figure 3) where we can easily read if Booking.com has enough rooms available considering (i) the real proportion of attendees based on our estimation, and (ii) the proportion of this people that will book a room through Booking.com.\n\nCode# a function to create a 100x100 matrix of the number of rooms available in function of the proportion of real attendees and the proportion of these people that will book a room through Booking.com\nimg.fct <- function(nb_attendees, nb_rooms, city) {\n  nb_rooms <- nb_rooms\n  A <- matrix(seq(1, nb_attendees, length = 100), nrow = 100, ncol = 100)\n  B <- t(matrix(seq(1 / 100, 100 / 100, length = 100), nrow = 100, ncol = 100))\n  C <- (A * B) - nb_rooms\n  D <- melt(C)\n  D$City <- city\n  D\n}\n# calculation of this matrix for each city\nimg.data <- rbind(\n  img.fct(812926, data.uefa.cities[city == \"Paris\", nb_rooms_available], \"Paris\"),\n  img.fct(142496, data.uefa.cities[city == \"Nice\", nb_rooms_available], \"Nice\"),\n  img.fct(177558, data.uefa.cities[city == \"Lyon\", nb_rooms_available], \"Lyon\")\n)\n# plot\nggplot(img.data, aes(Var1, Var2)) +\n  geom_raster(aes(fill = log(value)), interpolate = TRUE) +\n  labs(\n    x = \"Proportion of attendees that will book reservation through Booking.com (%)\",\n    y = \"Proportion of real attendees based on my estimation (%)\"\n  ) +\n  scale_fill_gradientn(\n    colours = rev(rainbow(7)),\n    na.value = \"grey\",\n    breaks = c(log(2), log(60), log(3000), log(160000)),\n    labels = c(2, 60, 3000, 160000)\n  ) +\n  guides(fill = guide_colorbar(\n    title.position = \"top\",\n    barwidth = 11,\n    title = \"Additional nb of rooms to open\"\n  )) +\n  facet_grid(\"~ City\") +\n  theme(legend.position = \"top\")\n\n\n\nFigure 3: Image plot for the three cities of the number of rooms vs. the real proportion of attendees based on our estimation and the proportion of this people that will book a room through Booking.com.\n\n\n\n\nIn Figure 3, the grey area represents the number of rooms Booking.com is able to offer, considering a certain proportion of real attendees as well as a proportion of these people that will book a room through Booking.com."
  },
  {
    "objectID": "booking_jjoumaa.html#conclusions",
    "href": "booking_jjoumaa.html#conclusions",
    "title": "Booking Interview Project",
    "section": "\n3.3 Conclusions",
    "text": "3.3 Conclusions\n\n\n\nConsidering all the assumptions and results, Paris seems to have the best capacity to welcome attendees, compared to Nice or Lyon (in Figure 3), the grey area is bigger for Paris than Nice and Lyon). Based on this results, I would recommend to increase the number of properties in both cities (Nice and Lyon) to make sure it reach the same level of capacity than Paris. In order to reach this same level of capacity, Nice has to open 1,293 rooms and Lyon 2,044. In addition, assuming that one ticket in four (the choice is totally arbitrary) will be sell to someone looking for an accommodation, i.e. 25 % of the proportion of real attendees and that 20 % (also an arbitrary choice) of these people will look for an accommodation through Booking.com, it appears that 24,728, 5,627 and 7,445 rooms needs to be available in Paris, Nice and Lyon, that is to say respectively 6,704, 3,756 and 2,532 new properties to meet the demand."
  },
  {
    "objectID": "booking_jjoumaa.html#statement-1",
    "href": "booking_jjoumaa.html#statement-1",
    "title": "Booking Interview Project",
    "section": "\n4.1 Statement",
    "text": "4.1 Statement\n“Dear Reporting Team,\nLately I get the feeling that the properties we are opening in Southern Europe are getting more and more expensive, while our customers demand lower priced accommodations. I would like to present some data in our next account managers meetup, but of course I would like to have some validated insights to back this up. Can you help me out on this?\nRegards.”"
  },
  {
    "objectID": "booking_jjoumaa.html#data-mining-1",
    "href": "booking_jjoumaa.html#data-mining-1",
    "title": "Booking Interview Project",
    "section": "\n4.2 Data mining",
    "text": "4.2 Data mining\n\n4.2.1 Few comments\n\n\n\nAs I only get a snapshot of the situation for 10.000 properties randomly selected from April 2016, we will focus on the relationship between the minimal room rate for a single-night (minrate) and the date that the property was available at Booking.com for the first time (first_opened). I will thus test the hypothesis of an increase in minrate with first_opened. As the question is focused on the Southern Europe, I only considered cities in Italy (Napoli, Florence, Roma, Palermo, Milano, Venice) in the further analysis.\n\n4.2.2 Analysis & Results\n\nCode# new dataset for Italia\ndata.it <- data[cc1 == \"it\", ]\n# some exploratory graphs\ngrid.arrange(\n  ggplot(data.it, aes(minrate)) +\n    geom_histogram(aes(y = ..density..)) +\n    geom_density() +\n    labs(\n      x = \"Minimal room rate for a single night\",\n      y = \"Density\"\n    ),\n  ggplot(data.it, aes(y = minrate, x = city)) +\n    geom_boxplot() +\n    labs(\n      y = \"Minimal room rate for a single night\",\n      x = \"City\"\n    ),\n  ncol = 2\n)\n\n\n\nFigure 4: Histogram and boxplot of the minimal room rate for a single night in several italian cities.\n\n\n\n\nIt appears there are a lot of outliers and zeros in the dataset (Figure 4). I decided then to remove any rows with a minimal room rate for a single night of zero and other outliers using the IQR rule.\n\nCode# we removed zeros\ndata.it <- data.it[minrate != 0, ]\n# and other outliers\noutliers <- function(out) {\n  q1 <- quantile(out, 0.25)\n  q3 <- quantile(out, 0.75)\n  iqr <- q3 - q1\n  which(out > q1 - 1.5 * iqr & out < q3 + 1.5 * iqr)\n}\ndata.it.out <- rbindlist(lapply(split(data.it, by = \"city\"), function(x) {\n  x <- x[outliers(x$minrate), ]\n}))\n# new plot\ngrid.arrange(\n  ggplot(data.it.out, aes(minrate)) +\n    geom_histogram(aes(y = ..density..)) +\n    geom_density() +\n    labs(\n      x = \"Minimal room rate for a single night\",\n      y = \"Density\"\n    ),\n  ggplot(data.it.out, aes(y = minrate, x = city)) +\n    geom_boxplot() +\n    labs(\n      y = \"Minimal room rate for a single night\",\n      x = \"Cities\"\n    ),\n  ncol = 2\n)\n\n\n\nFigure 5: Histogram and boxplot of the minimal room rate in several italian cities, without considering outliers.\n\n\n\n\nThis is much better, now we are going to look at the trend over time (Figure 6), by first using a generalized additive model.\n\nCodeggplot(data.it.out, aes(x = first_opened, y = minrate)) +\n  geom_point() +\n  geom_smooth() +\n  labs(\n    y = \"Minimal room rate for a single night\",\n    x = \"Date that the property was available at Booking.com for the first time\"\n  )\n\n\n\nFigure 6: Distribution of minimal room rates for a single night with the date that the property was available at Booking.com for the first time. The blue line is for a generalized additive model automatically adjusted with data.\n\n\n\n\nNothing really clear here, neither using a simple linear regression (Figure 7).\n\nCodeggplot(data.it.out, aes(x = first_opened, y = minrate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    y = \"Minimal room rate for a single night\",\n    x = \"Date that the property was available at Booking.com for the first time\"\n  )\n\n\n\nFigure 7: Distribution of minimal room rates for a single night with the date that the property was available at Booking.com for the first time. The blue line is for a linear regression automatically adjusted with data.\n\n\n\n\nAs the dataset is structured by city, it is better to include a random effect in our models, using a linear mixed effects models.\n\nCode# na.action = na.omit)\"}\n# building LME\nlme.1 <- lme(minrate ~ first_opened, data = data.it.out, random = ~ 1 | city, na.action = na.omit)\n# results\npander(summary(lme.1))\n\n\nTable 3: Output for the linear mixed effect model: lme(minrate~first_opened,data=data.it.out,random=~1|city)\n\n\n\n\n(a) Fixed effects: minrate ~ first_opened\n\n\n\n\n\n\n\n\n\n \nValue\nStd.Error\nDF\nt-value\np-value\n\n\n\n(Intercept)\n144\n54.1\n4132\n2.661\n0.007811\n\n\nfirst_opened\n-0.00000003283\n0.00000003598\n4132\n-0.9123\n0.3617\n\n\n\n\n\n\n\n\n(b) Standardized Within-Group Residuals\n\n\n\n\n\n\n\n\nMin\nQ1\nMed\nQ3\nMax\n\n\n-3.364\n-0.6873\n-0.1539\n0.5618\n3.79\n\n\n\n\n\n\n\n(c) Linear mixed-effects model fit by REML : minrate ~ first_opened\n\n\n\n\n\n\n\n \nObservations\nGroups\nLog-restricted-likelihood\n\n\ncity\n4139\n6\n-21373\n\n\n\n\n\n\nThe ouput reveals there is no significant (p-value for first_opened > 0.05) linear trend over time (defined as the date the property was available for the first time) of the minimal room rate for a single. Now, I take a look at the residual to know how trustful this result is.\n\nCodepar(mfrow = c(2, 2))\n# diagnostic plots\n# plot(lme.1)\nacf(residuals(lme.1, normalized = T))\nhist(residuals(lme.1, normalized = T))\nplot(residuals(lme.1, normalized = T))\n\n\n\nFigure 8: Some diagnostic plot of the linear mixed effect model lme.1.\n\n\n\n\nThe diagnostic plots (Figure 8) realized on the residuals of the linear mixed effect model lme.1 do not reveal any patterns, meaning the model is not missing any relevant variable. This also means that we can be confident in its results, regardless the city: there are no trend towards the minimal room rate for a single night with the date the property was available at Booking.com for the first time in this dataset."
  },
  {
    "objectID": "booking_jjoumaa.html#conclusions-1",
    "href": "booking_jjoumaa.html#conclusions-1",
    "title": "Booking Interview Project",
    "section": "\n4.3 Conclusions",
    "text": "4.3 Conclusions\nNo matter the city considered, there is no trend towards the minimal room rate for a single night with the date the property was available at Booking.com for the first time. Rather, there is a relative stability of the room rate with the date the property was available for the first time. While this analysis is interesting, it would probably be even more interesting to look at the relationship of the minimal room rate with time, and not only during the month of April 2016. This would may be highlight trends in agreement with your feeling."
  },
  {
    "objectID": "booking_jjoumaa.html#few-comments-1",
    "href": "booking_jjoumaa.html#few-comments-1",
    "title": "Booking Interview Project",
    "section": "\n5.1 Few comments",
    "text": "5.1 Few comments\nGiven this dataset, there are a lot of questions that could be addressed, such as:\n\nWhich city has the center the most covered by properties?\nIs there any differences among cities and countries in terms of properties offering (hotel_type, nr_rooms, payment_method or star_rating)?\nCan we relate the type of properties to the distance from the city center?\n\nBut here, I decided to focus on the minimal room rate for a single night, and I tried to model this variable considering other information present within the dataset."
  },
  {
    "objectID": "booking_jjoumaa.html#analysis-results-2",
    "href": "booking_jjoumaa.html#analysis-results-2",
    "title": "Booking Interview Project",
    "section": "\n5.2 Analysis & Results",
    "text": "5.2 Analysis & Results\nI basically used the same approach than before, I removed zeros and outliers:\n\nCode# I removed zeros\ndata.inter <- data[minrate != 0, ]\n# and other outliers\ndata.inter <- rbindlist(lapply(split(data.inter, by = \"city\"), function(x) {\n  x <- x[outliers(x$minrate), ]\n}))\n\n\nThen I converted every qualitative variables as factor:\n\nCodedata.inter$city <- as.factor(data.inter$city)\ndata.inter$cc1 <- as.factor(data.inter$cc1)\ndata.inter$star_rating <- as.factor(data.inter$star_rating)\ndata.inter$hotel_type <- as.factor(data.inter$hotel_type)\ndata.inter$payment_method <- as.factor(data.inter$payment_method)\n\n\nFinally I ran a set of generalized additive models using all the relevant information found in data.inter.\n\nCodebam.list <- list()\nbam.list$bam.1 <- bam(\n  minrate ~ s(nr_rooms) +\n    s(km_from_city_center) +\n    s(nr_rooms, km_from_city_center, city, bs = \"fs\", m = 1) +\n    s(nr_rooms, km_from_city_center, payment_method, bs = \"fs\", m = 1) +\n    s(nr_rooms, km_from_city_center, star_rating, bs = \"fs\", m = 1) +\n    s(nr_rooms, km_from_city_center, hotel_type, bs = \"fs\", m = 1),\n  data = data.inter,\n  na.action = na.omit\n)\n\nbam.list$bam.2 <- bam(\n  minrate ~ s(nr_rooms) +\n    s(km_from_city_center) +\n    s(nr_rooms, km_from_city_center, city, bs = \"fs\", m = 1) +\n    # s(nr_rooms, km_from_city_center,payment_method, bs=\"fs\",m=1)+\n    s(nr_rooms, km_from_city_center, star_rating, bs = \"fs\", m = 1) +\n    s(nr_rooms, km_from_city_center, hotel_type, bs = \"fs\", m = 1),\n  data = data.inter,\n  na.action = na.omit\n)\n\nbam.list$bam.3 <- bam(\n  minrate ~ s(nr_rooms) +\n    s(km_from_city_center) +\n    s(nr_rooms, km_from_city_center, city, bs = \"fs\", m = 1) +\n    # s(nr_rooms, km_from_city_center,payment_method, bs=\"fs\",m=1)+\n    # s(nr_rooms, km_from_city_center,star_rating, bs=\"fs\",m=1)+\n    s(nr_rooms, km_from_city_center, hotel_type, bs = \"fs\", m = 1),\n  data = data.inter,\n  na.action = na.omit\n)\n\nbam.list$bam.4 <- bam(\n  minrate ~ s(nr_rooms) +\n    # s(km_from_city_center)+\n    s(nr_rooms, km_from_city_center, city, bs = \"fs\", m = 1) +\n    # s(nr_rooms, km_from_city_center,payment_method, bs=\"fs\",m=1)+\n    # s(nr_rooms, km_from_city_center,star_rating, bs=\"fs\",m=1)+\n    s(nr_rooms, km_from_city_center, hotel_type, bs = \"fs\", m = 1),\n  data = data.inter,\n  na.action = na.omit\n)\n\nbam.list$bam.5 <- bam(\n  minrate ~ # s(nr_rooms)+\n    # s(km_from_city_center)+\n    s(nr_rooms, km_from_city_center, city, bs = \"fs\", m = 1) +\n    # s(nr_rooms, km_from_city_center,payment_method, bs=\"fs\",m=1)+\n    # s(nr_rooms, km_from_city_center,star_rating, bs=\"fs\",m=1)+\n    s(nr_rooms, km_from_city_center, hotel_type, bs = \"fs\", m = 1),\n  data = data.inter,\n  na.action = na.omit\n)\n\nbam.list$bam.6 <- bam(\n  minrate ~ # s(nr_rooms)+\n    # s(km_from_city_center)+\n    s(city, bs = \"re\") +\n    # s(nr_rooms, km_from_city_center,payment_method, bs=\"fs\",m=1)+\n    # s(nr_rooms, km_from_city_center,star_rating, bs=\"fs\",m=1)+\n    s(hotel_type, bs = \"re\"),\n  data = data.inter,\n  na.action = na.omit\n)\n\n\nI based the model selection looking at the p-value and the deviance explained. You can find in the Appendix section a summary of each model. Basically, I removed any non-significant part of the equation that was due to a random effect, i.e:\n\ns(nr_rooms, km_from_city_center,payment_method, bs=\"fs\",m=1)\ns(nr_rooms, km_from_city_center,star_rating, bs=\"fs\",m=1)\n\nThen I removed any non-significant part of the equation that was due to a fixed effect, i.e:\n\ns(nr_rooms)\ns(km_from_city_center)\n\nAs it remained two random smooth (a random effect on the intercept and the slope) and no fixed terms, I transformed these random smooth effects by random intercept effects, i.e:\n\ns(city, bs=\"re\")\ns(hotel_type, bs=\"re\"\n\n\n\n\nThe deviance, that is to say the proportion of variation in minrate, explained by the first full model bam.1 is nearly the same as the much more simplify model (bam.6), around 36 %.\n\nCodeggplot(data.table(\n  deviance = sapply(bam.list, function(x) {\n    round(summary(x)$dev.expl * 100, 2)\n  }),\n  model = names(bam.list)\n), aes(x = model, y = deviance)) +\n  geom_point() +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(\n    x = \"Generalized additive models\",\n    y = \"Deviance explained\"\n  )\n\n\n\nFigure 9: Deviance explained of the minimal room rate for a single night by each model.\n\n\n\n\nBased on this analysis and using the principle of parsimony, the best model to explain variation of the minimal room rate for a single night is composed of two random intercept effects, the city and the type of properties."
  },
  {
    "objectID": "booking_jjoumaa.html#conclusions-2",
    "href": "booking_jjoumaa.html#conclusions-2",
    "title": "Booking Interview Project",
    "section": "\n5.3 Conclusions",
    "text": "5.3 Conclusions\n\n\n\nWithout any fixed term in the model bam.6, this analysis showed that minimal room rate for a single night is nearly constant no matter the number of rooms or the distance from the city center. It also showed that the only qualitative variables that matters when setting a price is the location and the type of property proposed. To go further in the analysis I could have look at the effect of each mode, i.e. Paris, Napoli, Florence, Cannes, Roma, Nice, Milano, Venice, Palermo, Lyon for the cities, and Apartment, Hotel, Bed and Breakfast, ApartHotel, Holiday home, Hostel, Guest house, Homestay, Villa, Farm stay, Country house, Boat for type of properties, within the model bam.1, but a simple boxplot will higlight these differences in a more visual way Figure 10 and Figure 11.\n\nCodeggplot(data.inter, aes(x = city, y = minrate)) +\n  geom_boxplot() +\n  labs(x = \"City\", y = \"Minimal room rate for a single night\")\n\n\n\nFigure 10: Minimal room rate for a single night related to cities\n\n\n\n\n\nCodeggplot(data.inter, aes(x = hotel_type, y = minrate)) +\n  geom_boxplot() +\n  labs(x = \"Type of property\", y = \"Minimal rate for a single room\")\n\n\n\nFigure 11: Minimal room rate for a single night related to type of properties"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Some Achievements:",
    "section": "",
    "text": "Dataiku Interview: Test job interview for Dataiku\nSægus Interview: Test job interview for Sægus\nBooking Interview: Test job interview for Booking"
  },
  {
    "objectID": "index.html#webapps",
    "href": "index.html#webapps",
    "title": "Some Achievements:",
    "section": "Webapps",
    "text": "Webapps\n\n\n\n\n\n\n\n\nDash (Python)\nDash App A webapp used as a proof of concept for risk assessment in wheat trading\n\n\n\nShiny (R)\nShiny App same application but using R and Shiny (it was one of the first version before switching to Python and Dash)"
  },
  {
    "objectID": "index.html#package",
    "href": "index.html#package",
    "title": "Some Achievements:",
    "section": "Package",
    "text": "Package\n\n\n\n\n\n\n\n\nweanlingNES\nR Package The goal of this package is to provide in one place the analyses and functions developed during my postdoc on the ontogeny of diving behavior in elephant seals."
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Some Achievements:",
    "section": "Publications",
    "text": "Publications\n\nJouma’a, Joffrey Phd Thesis - 2016: Resource Acquisition Strategies and Cost of Transport in Southern Elephant Seal (http://www.theses.fr/2016LAROS014)\nJouma’a, Joffrey, Yves, Le Bras, Picard, Baptiste & Guinet, Christophe. (2017) Three-dimensional assessment of hunting strategies in a deep diving predator. Marine Ecology Progress Series (10.3354/meps12191)\nJouma’a, Joffrey, Yves, Le Bras, Richard, Gaëtan, Vacquié-Garcia, Jade, El Ksabi, Nory, Picard, Baptiste & Guinet Christophe. (2016) Adjustment of diving behaviour with prey encounters and body condition in a deep diving predator: the Southern Elephant Seal. Functional Ecology, 30, (4), 636-648 (10.1111/1365-2435.12514)\nCazau, Dorian, Bonnel, Julien, Joffrey, Jouma’a, Yves, Le Bras & Guinet Christophe. (2017) Measuring the marine soundscape of the Indian Ocean with Southern Elephant Seals used as acoustic gliders of opportunity. Journal of Atmospheric and Oceanic Technology (10.1175/JTECH-D-16-0124.1)\nDay, Louise, Jouma’a, Joffrey, Bonnel, Julien & Guinet Christophe. (2017) Acoustic measurements of post-dive cardiac responses in southern elephant seals (Mirounga leonina) during surfacing at sea. Journal of Experimental Biology (10.1242/jeb.146928)\nYves, Le Bras, Jouma’a, Joffrey, Picard, Baptiste & Guinet Christophe. (2016) How elephant seals (Mirounga leonina) adjust their fine scale horizontal movement and diving behaviour in relation to local prey encounter rate. PLoS One (10.1371/journal.pone.0167226)\nGénin, Alexandre, Richard, Gaëtan, Jouma’a, Joffrey, Picard, Baptiste, El Ksabi, Nory, Vacquié-Garcia, Jade & Guinet Christophe. (2015) Characterization of postdive recovery using sound recordings and its relationship to dive duration, exertion and foraging effort of southern elephant seals (Mirounga leonina). Marine Mammal Science, 1748-7692 (10.1111/mms.12235)\nRichard, Gaëtan, Vacquié-Garcia, Jade, Jouma’a, Joffrey, Picard, Baptiste, Génin, Alexandre, Arnould, John, Bailleul, Frédéric & Guinet, Christophe, (2014) Variation in body condition during the post-moult foraging trip of southern elephant seals and its consequences on diving behaviour. Journal of Experimental Biology, 217, 2609-2619 (10.1242/jeb.088542)"
  },
  {
    "objectID": "resume_jjoumaa.html",
    "href": "resume_jjoumaa.html",
    "title": "Resume",
    "section": "",
    "text": "PhD in Quantitative Ecology\nUniversity of La Rochelle, Chizé, France\n2013 - 2016\nMSc in Biodiversity, Ecology and Evolution\nUniversity of Paul Sabatier, Toulouse, France\n2012 - 2013\nUniversity degree “Discovery of the business world”\nUniversity of Bretagne Occidentale, Brest, France\n2011 - 2012\nUniversity of Bretagne Occidentale, Brest, France\nMaster’s degree in Marine Biological Sciences\n2010 - 2011\nBSc in Biology - Biochemistry\nUniversity of La Rochelle, La Rochelle, France\n2009 - 2010"
  },
  {
    "objectID": "resume_jjoumaa.html#experience",
    "href": "resume_jjoumaa.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\nData Scientist\nUniversity of California, Santa Cruz\nOct 2021 - present\nData Scientist\nThegreendata\nApr 2020 - Aug 2021\nData Consultant\nSaegus\nNov 2018 - Mar 2020\nBiostatistician Consultant\nIT&M STATS\nJul 2018 - Oct 2018\nData Analyst\nMalakoff Médéric\nDec 2017 - Jun 2018\nPhD\nCNRS\nOct 2013 - Nov 2016"
  }
]